[
  {
    "objectID": "posts/ST2024/Intro.html",
    "href": "posts/ST2024/Intro.html",
    "title": "00. Intro",
    "section": "",
    "text": "- 해당 수업에서는 확률에 관한 이론과 회귀분석에 대해 소개합니다.\n- 목표 1 : 궁극적으로 어떠한 자료가 주어졌을 때, 해당 자료가 어떤 분포를 따르고, 주어진 값에서 어떠한 확률값을 가지는 지를 학습한다!\n- 목표 2 : 다양한 회귀분석 이론을 학습하고, 적용해보며 실생활에 활용할 수 있도록 한다!",
    "crumbs": [
      "Posts",
      "ST2024",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/ST2024/Intro.html#예제-1.-동전-1회-던지는-실험",
    "href": "posts/ST2024/Intro.html#예제-1.-동전-1회-던지는-실험",
    "title": "00. Intro",
    "section": "예제 1. 동전 1회 던지는 실험",
    "text": "예제 1. 동전 1회 던지는 실험\n1 표본공간 : $X = [0, 1] $\n2 앞면이 나올 확률은? \\(\\to P(X = 1) = 1/2\\)",
    "crumbs": [
      "Posts",
      "ST2024",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/ST2024/Intro.html#예제-2.-동전을-2회-던지는-실험",
    "href": "posts/ST2024/Intro.html#예제-2.-동전을-2회-던지는-실험",
    "title": "00. Intro",
    "section": "예제 2. 동전을 2회 던지는 실험",
    "text": "예제 2. 동전을 2회 던지는 실험\n\nQ1.앞면이 두번 나올 확률\n1 첫 번째 시행 : \\(X_1 \\in \\text{[앞, 뒤]} = \\,\\in [0,1]\\)\n2 두 번째 시행 : \\(X_2 \\in \\text{[앞, 뒤]} = \\,\\in [0,1]\\)\n3 그렇다면 앞면이 두번 나올 확률은?\n\\[\\begin{align*}P(X_1 = 1, X_2 = 1) &= P(X_1=1)P(X_2=1) \\\\ \\\\\n                                    &= \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{4}   \\end{align*}\\]\n\n\nQ2. 앞면이 한번 나올 확률\ncase1\n\\[\\begin{align*}P(X_1 = 0, X_2 = 1) &= P(X_1=0)P(X_2=1) \\\\ \\\\\n                                    &= \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{4}   \\end{align*}\\]\ncase2\n\\[\\begin{align*}P(X_1 = 1, X_2 = 0) &= P(X_1=0)P(X_2=1) \\\\ \\\\\n                                    &= \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{4}   \\end{align*}\\]\n\\[\\text{case1} + \\text{case2} = \\frac{1}{2}\\]",
    "crumbs": [
      "Posts",
      "ST2024",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/ST2024/Intro.html#예제3.-동전을-3회-던지는-실험",
    "href": "posts/ST2024/Intro.html#예제3.-동전을-3회-던지는-실험",
    "title": "00. Intro",
    "section": "예제3. 동전을 3회 던지는 실험",
    "text": "예제3. 동전을 3회 던지는 실험\n\nQ1. 모두 앞면이 나올 확률\n\\[\\begin{align*}P(X_1 = 1, X_2 = 1, X_3 = 1) &= P(X_1=1)P(X_2=1)P(X_3=1) \\\\ \\\\\n                                    &= \\frac{1}{2}\\times \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{8}   \\end{align*}\\]\n\n\nQ2. 앞면이 두번 나올 확률\ncase1\n\\[\\begin{align*}P(X_1 = 1, X_2 = 1, X_3 = 0) &= P(X_1=1)P(X_2=1)P(X_3=0) \\\\ \\\\\n                                    &= \\frac{1}{2}\\times \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{8}   \\end{align*}\\]\ncase2\n\\[\\begin{align*}P(X_1 = 1, X_2 = 0, X_3 = 1) &= P(X_1=1)P(X_2=0)P(X_3=0) \\\\ \\\\\n                                    &= \\frac{1}{2}\\times \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{8}   \\end{align*}\\]\ncase3\n\\[\\begin{align*}P(X_1 = 0, X_2 = 1, X_3 = 1) &= P(X_1=1)P(X_2=0)P(X_3=0) \\\\ \\\\\n                                    &= \\frac{1}{2}\\times \\frac{1}{2}\\times \\frac{1}{2} = \\frac{1}{8}   \\end{align*}\\]\n결국 case1,2,3을 모두 더하면\n동전을 3회 던졌을 때 앞면이 두 번 나올 확률은? \\(\\to \\frac{3}{8}\\)\n의문 1 : 뭔가 규칙성이 있어 보이기 때문에, 저거를 한번에 계산하는 어떤 공식이 있을 것 같에요.\n\n이러한 것을 빠르게 계산하기 위해서는, 주어진 자료에 대한 확률분포를 알고 있어야 한다.\n다시 목표 1 : 궁극적으로 어떠한 자료가 주어졌을 때, 해당 자료가 어떤 분포를 따르고, 주어진 값에서 어떠한 확률값을 가지는 지를 학습한다!",
    "crumbs": [
      "Posts",
      "ST2024",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/ST2024/Intro.html#베르누이-분포",
    "href": "posts/ST2024/Intro.html#베르누이-분포",
    "title": "00. Intro",
    "section": "베르누이 분포",
    "text": "베르누이 분포\n- 성공과 실패와 같이 두 가지 결과만 나타나는 실험에 대한 확률분포\n\n관심있는 사건이 등장할 확률이 \\(p\\)인 실험\n\n\\[X \\sim Ber(p)\\]\n\\[P(X)=f(x)= p^{x}(1-p)^{1-x}\\]\n- 다시 동전던지기를 생각해보면? 위 수식 \\(x\\)에 1을 넣거나 0을 넣으면, 주어진 값에 대한 확률을 쉽게 구할 수 있겠죠?",
    "crumbs": [
      "Posts",
      "ST2024",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/ST2024/Intro.html#이항분포",
    "href": "posts/ST2024/Intro.html#이항분포",
    "title": "00. Intro",
    "section": "이항분포",
    "text": "이항분포\n- 베르누이 분포를 따르는 \\(n\\)개의 확률변수들이 특정 사건을 \\(x\\)번 일으키는지에 대한 확률분포\n- 다시 예제 : 동전을 3회 던졌을 떄 앞면이 2번 나올 확률은?\n\n접근 : 이항분포를 이용해서 앞면이 2번 나올 확률을 계산하라는 거네?\n\n- 잠깐 중학교 수학 : combination\n\\[_{n}\\mathrm{C}_{x} = \\frac{n!}{x!(n-x)!}\\]\n\n동전을 3번 던졌을 때 앞면이 2번 나오는 경우의 수는?\n\n동전을 3번 던졌을 때 뒷면이 1번 나오는 경우의 수를 구하는 것과 같습니당\n[1,1,0], [1,0,1], [0,1,1]\n\n\n\\[_{3}\\mathrm{C}_{2} \\,= \\,_{3}\\mathrm{C}_{1} =  3\\]\n- 다시 예제를 생각해보면, 우리는 모든 case를 구하고, 그 case 수와 확률값을 곱했다.\n\n근데 우리는 combination을 배웠으니까 위처럼 경우의 수를 쉽게 구할 수 있자나?\n즉, 3번 던져서 앞면이 2번 나올 확률은 1/8이고 이러한 경우가 3개니까. 답은 3/8이야\n\n- 이를 이항분포로 나타내면?\n\\[X \\sim B(n,p)\\]\n\\[ f(x) \\,=\\, _{n}\\mathrm{C}_{x}\\,p^{x}(1-p)^{n-x}\\]\n\\[f(x = 2) \\,= \\,_{3}\\mathrm{C}_{2}\\,\\times p^{2}(1-p)^{1} = 3\\times \\frac 18 = \\frac 38 \\]\n- 다시 목표 : 궁극적으로 어떠한 자료가 주어졌을 때, 해당 자료가 어떤 분포를 따르고, 주어진 값에서 어떠한 확률값을 가지는 지를 학습한다!",
    "crumbs": [
      "Posts",
      "ST2024",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/ST2024/Intro.html#intro-1",
    "href": "posts/ST2024/Intro.html#intro-1",
    "title": "00. Intro",
    "section": "intro",
    "text": "intro\n1 변수들간의 관계를 모형화하고 조사하는 통계적인 기법\n2 가장 널리 활용되는 통계 기법 중 하나이며 머신러닝, 딥러닝을 학습하기 위해 필수적으로 알아햐하는 방법론이다.\n3 아들의 키는 아버지의 키로 회귀한다는 말에서 유래됨",
    "crumbs": [
      "Posts",
      "ST2024",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/ST2024/Intro.html#목표",
    "href": "posts/ST2024/Intro.html#목표",
    "title": "00. Intro",
    "section": "목표",
    "text": "목표\n1 오차의 존재하에서 변수 간의 관계를 나타내는 수식을 찾아내는 것!\n2 즉, 아래와 같은 실생활에서 발견할 수 있는 현상\n\n날이 더우면, 아이스아메리카노 판매량이 증가한다?\n\n\\[ \\text{아이스아메리카노 판매량}= 1.5\\times{온도} + \\beta_0 + \\varepsilon\\]\n\n우리가 위와 같은 현상을 발견했으면 아래와 같은 수식을 발견하는 것이 회귀분석의 목적이다!\n\n\\[y \\approx 1.5x_1 + \\beta_0 \\]\n\n# \\ code-fold: true\nlibrary(tidyverse)\n\noptions(repr.plot.res = 200, repr.plot.height = 4,repr.plot.width = 6)\nlibrary(tidyverse)\n\nx = seq(-10,40, length.out = 1000)\nerror = rnorm(1000)*10\ny = 1.5*x + error\nhat_y = 1.5*x\n\ndata  = tibble(x = x, \"실제값\" = y, \"예측값\" = hat_y)\n\ndata %&gt;% \n        gather(\"label\",\"value\",-x) %&gt;%\n            ggplot(aes(x = x, y = value, color = label)) +\n                geom_point(alpha = 0.2) +\n                ggtitle(\"아메리카노 판매량 예측\") +\n                xlab(\"온도\") + \n                ylab(\"아이스아메리카노 판매량\") +  \n                theme(plot.title = element_text(hjust = 0.5,))\n\n\n\n\n\n\n\n\n- 다시 목표 2 : 다양한 회귀분석 이론을 학습하고, 적용해보며 실생활에 적용할 수 있도록 한다!\n\n이를 학습하며, 결정계수, MSE 등등 최적의 회귀식을 찾기위한 방법론을 학습할 예정\n또한, 분류문제에서 회귀분석이 어떻게 적용되는지를 학습한다.",
    "crumbs": [
      "Posts",
      "ST2024",
      "00. Intro"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-19-03. MLE.html",
    "href": "posts/PBA2024/2024-05-19-03. MLE.html",
    "title": "03. MLE",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\\[L(\\theta)=f(\\mathbf{x}  | \\theta) =  f(x_1|\\theta)f(x_2|\\theta)\\dots f(x_n|\\theta)\\]\n- 해석 1(일반적인 정의 : 블로그, 위키피디아, 기타 포럼 등등)\n\nLikelihood function\n우도함수는 일반적으로 설명력 함수라고 알려져 있다.\n수식을 보았을 때, 주어진 랜덤샘플 \\(x_1,x_2 \\dots x_n\\)에 대한 결합확률밀도함수(그냥 다 곱한거)를 나타내고 있다.\n즉, 이는 모수(\\(p, \\theta, \\mu, \\sigma...\\))가 주어졌을 때 주어진 데이터(샘플)가 어떤 분포(or 가정, 현상)를 얼만큼 설명하는지 직관적인 형태를 수식으로 나타냈다고 이해하자. (\\(\\star\\star\\star\\))\n\n- 해석 2 (조금 더.. 전문적인 지식이 들어간 해석)\n\n모수 (\\(p, \\theta, \\mu, \\sigma...\\))가 주어졌을 때 \\(\\bf {x}\\)를 관측할 확률을 나타낸다.(\\(\\star\\star\\star\\))\n즉, 특정 모수가 주어졌을 때 어떤 샘플들이 얻어질 확률이지 모수(확률, 평균)가 어떤 상수 값 (ex \\(p = 0.2\\))과 일치할 확률을 의미하지 않는다.(\\(\\star\\star\\star\\))\n\n- 해석 2에 대한 좀더 직관적인 접근으로 아래와 같은 실험에서 우도함수를 최대화하는 \\(\\hat{p}_{mle}\\) 구해보자.\n\n동전 던지기 실험 \\(\\to X_i \\overset{iid}\\sim Ber(p), \\quad f(x) = p^{x}(1-p)^{1-x}, \\quad  x \\in [0,1]\\)\n\n\nx = [0,1,0,1]\nx\n\n[0, 1, 0, 1]\n\n\n- 우도함수 계산\n\\(f(x = 0) = p^{x}(1-p)^{1-x} = \\frac 12\\)\n\\(f(x = 1) = p^{x}(1-p)^{1-x} = \\frac 12\\)\n\n(1/2)**4\n\n0.0625\n\n\n- 일반적으로 우리가 알고 있는 사실 \\(\\to p = 1/2, \\,\\, f(x) = 1/2\\)\n- 우도함수 계산\n- 만약 \\(p = 1/3\\)이라면? \\(\\to f(x) = (1/3)^{x}(2/3)^{1-x}\\)\n- 우도함수 계산\n\n(2/3)*(1/3)*(2/3)*(1/3)\n\n0.04938271604938271\n\n\n- 음? \\(p=1/2\\)일 때가, 더 우도함수 값이 크다.\n\n해석 1의 관점 : 오, \\(p=1/2\\)일 때, 설명력이 더 좋네?\n해석 2의 관점 : 오, \\(p=1/2\\)일 때, 샘플 (0,1,0,1)이 얻어질 확률이 더 크네?\n우리는 앞으로 해석 1의 관점을 일차적으로 말하고, 그 다음에 디테일하게 해석 2를 말할 줄 알아야 한다!\n\n\n\n\n1 가능도함수 = 우도함수 = likelihood function\n2 최대가능도함수 = 최대우도함수 = Maximum likelihood function = 모수 \\(p\\)의 MLE\n\n\n\n(조건은 생략할게여…)\nstep 1. 우도함수 구하기\n\\[L(\\theta)=f(\\mathbf{x}  | \\theta) =  f(x_1|\\theta)f(x_2|\\theta)\\dots f(x_n|\\theta)\\]\nstep 2. \\(\\log L(\\theta) = l(\\theta)\\) 미분\n\\[\\hat {\\theta}_{mle} = \\frac {d}{d\\,\\theta} \\log L(\\theta) = \\frac {d}{d\\,\\theta} l(\\theta)\\]\nwhy? 미분하여 최대값?\n- 중학교 수학 : 로그함수는 일반적으로 증가함수이며, 증가함수는 미분하여 0이되는 지점에서 최대값을 갖는다.\n- 다시 예제\n\n동전 던지기 실험 \\(\\to X_i \\overset{iid}\\sim Ber(p), \\quad f(x) = p^{x}(1-p)^{1-x}, \\quad  x \\in [0,1]\\)\n\n\n일반적으로 알려진 사실 \\(p = 1/2\\)\n\\(L(p) = p^{\\sum x}(1-p)^{n-\\sum x}\\)\n\\(l(p) = \\sum x \\log p + (n-\\sum x)\\log (1-p)\\)\n\n\\[\\begin{align*} \\frac{d}{d\\,p} l(p) &= \\frac{\\sum x}{p} - \\frac{n-\\sum x}{1-p} \\\\ \\\\\n                    &= (1-p)\\sum x - p(n-\\sum x)  \\\\ \\\\\n                    &= \\sum x  - pn = 0\\end{align*}\\]\n\\[\\divideontimes \\,\\,\\hat {p}_{mle} = \\frac {\\sum x}{ n} = \\bar x \\]\n\n모수 \\(\\hat p_{mle}\\)는 표본평균 \\(\\bar x\\)로 수렴한다.\n확인\n\n\nx = [0,1,0,1]\n\nsum(x)/4\n\n0.5\n\n\n\n\n\n\n\n\\[y = \\beta_0 + \\beta_1x + \\varepsilon,\\quad \\varepsilon \\overset{iid}\\sim N(0,\\sigma^2)\\]\n1 복습. OLS(정규방정식)\n\n아래의 식을 만족하는 \\(\\beta_0, \\beta_1\\)은 OLS, 즉 최소제곱법을 이용하여 구한 최적의 해다.\n\n\\[\\sum ({y_i - \\beta_0 -\\beta_1x})^2 = 0\\]\n2 MLE\n\n오차항은 정규성을 가정하였으므로. 다음과 같이 쓸 수 있다.\n\n\\[f(\\varepsilon) = \\frac {1}{\\sqrt {2\\pi}}e^{-\\frac{1}{2}\\varepsilon^2}\\]\n\n가능도 함수 해석\n\n\\(\\to\\) 1. 설명력 함수를 생각하면 오차항에 대한 우도함수를 구하고, 그것을 미분하여 우도함수를 최대화 시키는 \\(\\beta_0, \\beta_1\\)을 구하면 되지 않을까?\n\\(\\to\\) 우도함수 구하기\n\\[l(\\beta_0,\\beta_1) = -\\frac{\\sum \\varepsilon^2}{2\\sqrt{2\\pi}} = -\\frac{\\sum(y -  \\beta_0- \\beta_1x)^2}{2\\sqrt{2\\pi}} \\]\n\\(\\to\\) 우도함수 미분\n\\[\\frac {d}{d (\\beta_0, \\beta_1)}l(\\beta_0,\\beta_1)  = -\\frac {d}{d (\\beta_0, \\beta_1)}\\frac{\\sum(y -  \\beta_0- \\beta_1x)^2}{2\\sqrt{2\\pi}} = 0\\text{ 을 만족하는} (\\beta_1, \\beta_0) \\]\n\\(\\to\\) 차피 상수항은 넘어가니까…\n\\[\\frac {d}{d (\\beta_0, \\beta_1)}l(\\beta_0,\\beta_1)  = \\frac {d}{d (\\beta_0, \\beta_1)}\\sum(y -  \\beta_0- \\beta_1x)^2 = 0\\text{ 을 만족하는} (\\beta_1, \\beta_0) \\]\n\\(\\to\\) 어? 근데 이거 OLS랑 똑같네??\n\nOLS : 아래의 식을 만족하는 \\(\\beta_0, \\beta_1\\)은 OLS, 즉 최소제곱법을 이용하여 구한 최적의 해다.\n\n\\[\\sum ({y_i - \\beta_0 -\\beta_1x})^2 = 0\\]\n\n아하! 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 OLS를 최소화하는 \\(\\beta_0, \\beta_1\\)을 구하면 된다!\\((\\star\\star\\star)\\)\n\n\n\n\n\n1 잠깐 위에 동전던저기 예제\n\n\\(L(p) = p^{\\sum x}(1-p)^{n-\\sum x}\\)\n\\(l(p) = \\sum x \\log p + (n-\\sum x)\\log (1-p)\\)\n\n\\[\\begin{align*} \\frac{d}{d\\,p} l(p) &= \\frac{\\sum x}{p} - \\frac{n-\\sum x}{1-p} \\\\ \\\\\n                    &= (1-p)\\sum x - p(n-\\sum x)  \\\\ \\\\\n                    &= \\sum x  - pn = 0\\end{align*}\\]\n\\[\\divideontimes \\,\\,\\hat {p}_{mle} = \\frac {\\sum x}{ n} = \\bar x\\]\n2 로지스틱 복습\n\n지난 시간 로지스틱은 어떤 범주의 속할 확률(동전이 앞면이 나올지, 내가 여자인지 남자인지)을 모형화하는 것이라고 했다…\n확률을 모형화…모형화..어?? 위에 하고 똑같자나…\n\n\n동전 던지기 실험 \\(\\to Y_i \\overset{iid}\\sim Ber(p), \\quad f(y) = p^{y}(1-p)^{1-y}, \\quad  y \\in [0,1]\\)\n\n\n단순히 \\(p\\)가 \\(\\beta_0 + \\beta_1x\\)로 바뀐거죠?\n다시 쓰면?\n\n\n동전 던지기 실험 \\(\\to Y_i \\overset{iid}\\sim Ber(p), \\quad f(y) = (\\beta_0+\\beta_1x)^{y}(1-(\\beta_0+\\beta_1x))^{1-y}, \\quad  y \\in [0,1]\\)\n\n\n이거에 대한 우도함수는?\n\n\\(\\to\\) \\(L(\\beta_0, \\beta_1) = (\\beta_0+\\beta_1x)^{\\sum y}[1-(\\beta_0+\\beta_1x)]^{n-\\sum y}\\)\n\n이거를 교재 4장, 17페이지에서 아래와 같이 좀 문과생? 들이 보기 어려운 수식을 써서 표현한거에요…\n\n\\[\\prod_{i=1}^{n}\\left (\\beta_0 + \\beta_1x_1^{y_i} \\right )\\left (1-(\\beta_0 + \\beta_1x_1)^{y_i} \\right )\\]\n\\(\\to\\) 다시, \\(l(\\beta_0\\,\\beta_1)\\) 구하기. 이제 그냥 편의상 \\(p\\)라고 쓸게요.\n\\[\\begin{align*} l(p) &= \\sum y \\log p - (n-\\sum y)\\log (1-p)  \\\\ \\\\\n                      &= \\sum y \\log p - (1-y)\\log (1-p)  \\\\ \\\\\n                      &= \\sum y \\log (\\beta_0 + \\beta_1x) - (1-y)\\log (1-(\\beta_0 + \\beta_1x)) \\end{align*}\\]\n\\(\\to \\sum y \\log (\\beta_0 + \\beta_1x) - (1-y)\\log \\left (1-(\\beta_0 + \\beta_1x)\\right )\\) 와 같은 형태를 BCEloss 라고합니다.(뒤에서 배울거에요.)\n\\(\\to\\) 즉, 분류범주가 2개인 경우 BCEloss를 이용하며 모형을 학습해 최적의 모형을 산출하는데, 그게 결국 MLE를 통해 최적의 모수를 구하는 것과 같다!\n\\(\\to\\) 최적의 모형을 적합하기 위해서, OLS, BCELoss를 이용한 최적의 해 산출 방식은 MLE를 이용한 방법과 같다.\n\n\n\n\n\n\n- 정확도 : 예측값이 실제값과 일치할 비율(\\(\\star\\star\\))\n\n(9644 + 81)/10000\n\n0.9725\n\n\n\n\n\n- 재현율(Recall) : 실제 참(거짓)인 것중에 예측된 참(거짓)의 비율(\\(\\star\\star\\star\\))\n\n회사입장에서는 직장상사(True)가 부하(Predict)한테 일을 시키는 거를 상상해보자.\n\n\n81/333\n\n0.24324324324324326\n\n\n\n\n\n- 정밀도(Precision) : 예측된 참(거짓)인 것 중에서 실제 참(거짓)의 비율(\\(\\star\\star\\))\n\n모델(후배)의 건방짐\n\n\n81/104\n\n0.7788461538461539\n\n\n- 회사의 입장 : 사실 다 중요한 지표입니다.\n\n여기서 말하고 싶은 것은 정확도만 보고 어떤 인사이트나 결과 보고서를 작성하면 안된다.\n세부 지표를 함께 확인해야한다!\n저희는 이러한 전체 비율로 어떤 좋아보이지만, 실제 세부 그룹을 나누고 비율이나 평가지표를 계산했을 때 그렇지 않은 경우를 심슨의 역설(\\(\\star\\star\\star\\)) 이라고 한다!\n\n\n\n\n\n\n1 우도함수는 설명력 함수이다.\n2 우도함수는 주어진 모수 (\\(p, \\theta, \\mu, \\sigma...\\))가 주어졌을 때 \\(\\bf {x}\\)를 관측할 확률을 나타낸다.\n3 어떤 분포, 모형에 관한 최적의 MLE를 구하기 위해서는 우도함수의 로그를 취하고 그것을 미분하여 0을 만족하는 경우를 MLE라고 한다.\n4 회귀모형에서 최적의 회귀계수(\\(\\beta_0,\\beta_1\\))을 구하는 방법은 MLE를 이용하여 구하는 방법과 동일하다.",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-19-03. MLE.html#우도함수",
    "href": "posts/PBA2024/2024-05-19-03. MLE.html#우도함수",
    "title": "03. MLE",
    "section": "",
    "text": "\\[L(\\theta)=f(\\mathbf{x}  | \\theta) =  f(x_1|\\theta)f(x_2|\\theta)\\dots f(x_n|\\theta)\\]\n- 해석 1(일반적인 정의 : 블로그, 위키피디아, 기타 포럼 등등)\n\nLikelihood function\n우도함수는 일반적으로 설명력 함수라고 알려져 있다.\n수식을 보았을 때, 주어진 랜덤샘플 \\(x_1,x_2 \\dots x_n\\)에 대한 결합확률밀도함수(그냥 다 곱한거)를 나타내고 있다.\n즉, 이는 모수(\\(p, \\theta, \\mu, \\sigma...\\))가 주어졌을 때 주어진 데이터(샘플)가 어떤 분포(or 가정, 현상)를 얼만큼 설명하는지 직관적인 형태를 수식으로 나타냈다고 이해하자. (\\(\\star\\star\\star\\))\n\n- 해석 2 (조금 더.. 전문적인 지식이 들어간 해석)\n\n모수 (\\(p, \\theta, \\mu, \\sigma...\\))가 주어졌을 때 \\(\\bf {x}\\)를 관측할 확률을 나타낸다.(\\(\\star\\star\\star\\))\n즉, 특정 모수가 주어졌을 때 어떤 샘플들이 얻어질 확률이지 모수(확률, 평균)가 어떤 상수 값 (ex \\(p = 0.2\\))과 일치할 확률을 의미하지 않는다.(\\(\\star\\star\\star\\))\n\n- 해석 2에 대한 좀더 직관적인 접근으로 아래와 같은 실험에서 우도함수를 최대화하는 \\(\\hat{p}_{mle}\\) 구해보자.\n\n동전 던지기 실험 \\(\\to X_i \\overset{iid}\\sim Ber(p), \\quad f(x) = p^{x}(1-p)^{1-x}, \\quad  x \\in [0,1]\\)\n\n\nx = [0,1,0,1]\nx\n\n[0, 1, 0, 1]\n\n\n- 우도함수 계산\n\\(f(x = 0) = p^{x}(1-p)^{1-x} = \\frac 12\\)\n\\(f(x = 1) = p^{x}(1-p)^{1-x} = \\frac 12\\)\n\n(1/2)**4\n\n0.0625\n\n\n- 일반적으로 우리가 알고 있는 사실 \\(\\to p = 1/2, \\,\\, f(x) = 1/2\\)\n- 우도함수 계산\n- 만약 \\(p = 1/3\\)이라면? \\(\\to f(x) = (1/3)^{x}(2/3)^{1-x}\\)\n- 우도함수 계산\n\n(2/3)*(1/3)*(2/3)*(1/3)\n\n0.04938271604938271\n\n\n- 음? \\(p=1/2\\)일 때가, 더 우도함수 값이 크다.\n\n해석 1의 관점 : 오, \\(p=1/2\\)일 때, 설명력이 더 좋네?\n해석 2의 관점 : 오, \\(p=1/2\\)일 때, 샘플 (0,1,0,1)이 얻어질 확률이 더 크네?\n우리는 앞으로 해석 1의 관점을 일차적으로 말하고, 그 다음에 디테일하게 해석 2를 말할 줄 알아야 한다!",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-19-03. MLE.html#용어정리",
    "href": "posts/PBA2024/2024-05-19-03. MLE.html#용어정리",
    "title": "03. MLE",
    "section": "",
    "text": "1 가능도함수 = 우도함수 = likelihood function\n2 최대가능도함수 = 최대우도함수 = Maximum likelihood function = 모수 \\(p\\)의 MLE",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-19-03. MLE.html#mle를-구하는-방법",
    "href": "posts/PBA2024/2024-05-19-03. MLE.html#mle를-구하는-방법",
    "title": "03. MLE",
    "section": "",
    "text": "(조건은 생략할게여…)\nstep 1. 우도함수 구하기\n\\[L(\\theta)=f(\\mathbf{x}  | \\theta) =  f(x_1|\\theta)f(x_2|\\theta)\\dots f(x_n|\\theta)\\]\nstep 2. \\(\\log L(\\theta) = l(\\theta)\\) 미분\n\\[\\hat {\\theta}_{mle} = \\frac {d}{d\\,\\theta} \\log L(\\theta) = \\frac {d}{d\\,\\theta} l(\\theta)\\]\nwhy? 미분하여 최대값?\n- 중학교 수학 : 로그함수는 일반적으로 증가함수이며, 증가함수는 미분하여 0이되는 지점에서 최대값을 갖는다.\n- 다시 예제\n\n동전 던지기 실험 \\(\\to X_i \\overset{iid}\\sim Ber(p), \\quad f(x) = p^{x}(1-p)^{1-x}, \\quad  x \\in [0,1]\\)\n\n\n일반적으로 알려진 사실 \\(p = 1/2\\)\n\\(L(p) = p^{\\sum x}(1-p)^{n-\\sum x}\\)\n\\(l(p) = \\sum x \\log p + (n-\\sum x)\\log (1-p)\\)\n\n\\[\\begin{align*} \\frac{d}{d\\,p} l(p) &= \\frac{\\sum x}{p} - \\frac{n-\\sum x}{1-p} \\\\ \\\\\n                    &= (1-p)\\sum x - p(n-\\sum x)  \\\\ \\\\\n                    &= \\sum x  - pn = 0\\end{align*}\\]\n\\[\\divideontimes \\,\\,\\hat {p}_{mle} = \\frac {\\sum x}{ n} = \\bar x \\]\n\n모수 \\(\\hat p_{mle}\\)는 표본평균 \\(\\bar x\\)로 수렴한다.\n확인\n\n\nx = [0,1,0,1]\n\nsum(x)/4\n\n0.5",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-19-03. MLE.html#회귀모형에서의-mle",
    "href": "posts/PBA2024/2024-05-19-03. MLE.html#회귀모형에서의-mle",
    "title": "03. MLE",
    "section": "",
    "text": "\\[y = \\beta_0 + \\beta_1x + \\varepsilon,\\quad \\varepsilon \\overset{iid}\\sim N(0,\\sigma^2)\\]\n1 복습. OLS(정규방정식)\n\n아래의 식을 만족하는 \\(\\beta_0, \\beta_1\\)은 OLS, 즉 최소제곱법을 이용하여 구한 최적의 해다.\n\n\\[\\sum ({y_i - \\beta_0 -\\beta_1x})^2 = 0\\]\n2 MLE\n\n오차항은 정규성을 가정하였으므로. 다음과 같이 쓸 수 있다.\n\n\\[f(\\varepsilon) = \\frac {1}{\\sqrt {2\\pi}}e^{-\\frac{1}{2}\\varepsilon^2}\\]\n\n가능도 함수 해석\n\n\\(\\to\\) 1. 설명력 함수를 생각하면 오차항에 대한 우도함수를 구하고, 그것을 미분하여 우도함수를 최대화 시키는 \\(\\beta_0, \\beta_1\\)을 구하면 되지 않을까?\n\\(\\to\\) 우도함수 구하기\n\\[l(\\beta_0,\\beta_1) = -\\frac{\\sum \\varepsilon^2}{2\\sqrt{2\\pi}} = -\\frac{\\sum(y -  \\beta_0- \\beta_1x)^2}{2\\sqrt{2\\pi}} \\]\n\\(\\to\\) 우도함수 미분\n\\[\\frac {d}{d (\\beta_0, \\beta_1)}l(\\beta_0,\\beta_1)  = -\\frac {d}{d (\\beta_0, \\beta_1)}\\frac{\\sum(y -  \\beta_0- \\beta_1x)^2}{2\\sqrt{2\\pi}} = 0\\text{ 을 만족하는} (\\beta_1, \\beta_0) \\]\n\\(\\to\\) 차피 상수항은 넘어가니까…\n\\[\\frac {d}{d (\\beta_0, \\beta_1)}l(\\beta_0,\\beta_1)  = \\frac {d}{d (\\beta_0, \\beta_1)}\\sum(y -  \\beta_0- \\beta_1x)^2 = 0\\text{ 을 만족하는} (\\beta_1, \\beta_0) \\]\n\\(\\to\\) 어? 근데 이거 OLS랑 똑같네??\n\nOLS : 아래의 식을 만족하는 \\(\\beta_0, \\beta_1\\)은 OLS, 즉 최소제곱법을 이용하여 구한 최적의 해다.\n\n\\[\\sum ({y_i - \\beta_0 -\\beta_1x})^2 = 0\\]\n\n아하! 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 OLS를 최소화하는 \\(\\beta_0, \\beta_1\\)을 구하면 된다!\\((\\star\\star\\star)\\)\n\n\n\n\n\n1 잠깐 위에 동전던저기 예제\n\n\\(L(p) = p^{\\sum x}(1-p)^{n-\\sum x}\\)\n\\(l(p) = \\sum x \\log p + (n-\\sum x)\\log (1-p)\\)\n\n\\[\\begin{align*} \\frac{d}{d\\,p} l(p) &= \\frac{\\sum x}{p} - \\frac{n-\\sum x}{1-p} \\\\ \\\\\n                    &= (1-p)\\sum x - p(n-\\sum x)  \\\\ \\\\\n                    &= \\sum x  - pn = 0\\end{align*}\\]\n\\[\\divideontimes \\,\\,\\hat {p}_{mle} = \\frac {\\sum x}{ n} = \\bar x\\]\n2 로지스틱 복습\n\n지난 시간 로지스틱은 어떤 범주의 속할 확률(동전이 앞면이 나올지, 내가 여자인지 남자인지)을 모형화하는 것이라고 했다…\n확률을 모형화…모형화..어?? 위에 하고 똑같자나…\n\n\n동전 던지기 실험 \\(\\to Y_i \\overset{iid}\\sim Ber(p), \\quad f(y) = p^{y}(1-p)^{1-y}, \\quad  y \\in [0,1]\\)\n\n\n단순히 \\(p\\)가 \\(\\beta_0 + \\beta_1x\\)로 바뀐거죠?\n다시 쓰면?\n\n\n동전 던지기 실험 \\(\\to Y_i \\overset{iid}\\sim Ber(p), \\quad f(y) = (\\beta_0+\\beta_1x)^{y}(1-(\\beta_0+\\beta_1x))^{1-y}, \\quad  y \\in [0,1]\\)\n\n\n이거에 대한 우도함수는?\n\n\\(\\to\\) \\(L(\\beta_0, \\beta_1) = (\\beta_0+\\beta_1x)^{\\sum y}[1-(\\beta_0+\\beta_1x)]^{n-\\sum y}\\)\n\n이거를 교재 4장, 17페이지에서 아래와 같이 좀 문과생? 들이 보기 어려운 수식을 써서 표현한거에요…\n\n\\[\\prod_{i=1}^{n}\\left (\\beta_0 + \\beta_1x_1^{y_i} \\right )\\left (1-(\\beta_0 + \\beta_1x_1)^{y_i} \\right )\\]\n\\(\\to\\) 다시, \\(l(\\beta_0\\,\\beta_1)\\) 구하기. 이제 그냥 편의상 \\(p\\)라고 쓸게요.\n\\[\\begin{align*} l(p) &= \\sum y \\log p - (n-\\sum y)\\log (1-p)  \\\\ \\\\\n                      &= \\sum y \\log p - (1-y)\\log (1-p)  \\\\ \\\\\n                      &= \\sum y \\log (\\beta_0 + \\beta_1x) - (1-y)\\log (1-(\\beta_0 + \\beta_1x)) \\end{align*}\\]\n\\(\\to \\sum y \\log (\\beta_0 + \\beta_1x) - (1-y)\\log \\left (1-(\\beta_0 + \\beta_1x)\\right )\\) 와 같은 형태를 BCEloss 라고합니다.(뒤에서 배울거에요.)\n\\(\\to\\) 즉, 분류범주가 2개인 경우 BCEloss를 이용하며 모형을 학습해 최적의 모형을 산출하는데, 그게 결국 MLE를 통해 최적의 모수를 구하는 것과 같다!\n\\(\\to\\) 최적의 모형을 적합하기 위해서, OLS, BCELoss를 이용한 최적의 해 산출 방식은 MLE를 이용한 방법과 같다.\n\n\n\n\n\n\n- 정확도 : 예측값이 실제값과 일치할 비율(\\(\\star\\star\\))\n\n(9644 + 81)/10000\n\n0.9725\n\n\n\n\n\n- 재현율(Recall) : 실제 참(거짓)인 것중에 예측된 참(거짓)의 비율(\\(\\star\\star\\star\\))\n\n회사입장에서는 직장상사(True)가 부하(Predict)한테 일을 시키는 거를 상상해보자.\n\n\n81/333\n\n0.24324324324324326\n\n\n\n\n\n- 정밀도(Precision) : 예측된 참(거짓)인 것 중에서 실제 참(거짓)의 비율(\\(\\star\\star\\))\n\n모델(후배)의 건방짐\n\n\n81/104\n\n0.7788461538461539\n\n\n- 회사의 입장 : 사실 다 중요한 지표입니다.\n\n여기서 말하고 싶은 것은 정확도만 보고 어떤 인사이트나 결과 보고서를 작성하면 안된다.\n세부 지표를 함께 확인해야한다!\n저희는 이러한 전체 비율로 어떤 좋아보이지만, 실제 세부 그룹을 나누고 비율이나 평가지표를 계산했을 때 그렇지 않은 경우를 심슨의 역설(\\(\\star\\star\\star\\)) 이라고 한다!",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-19-03. MLE.html#summary",
    "href": "posts/PBA2024/2024-05-19-03. MLE.html#summary",
    "title": "03. MLE",
    "section": "",
    "text": "1 우도함수는 설명력 함수이다.\n2 우도함수는 주어진 모수 (\\(p, \\theta, \\mu, \\sigma...\\))가 주어졌을 때 \\(\\bf {x}\\)를 관측할 확률을 나타낸다.\n3 어떤 분포, 모형에 관한 최적의 MLE를 구하기 위해서는 우도함수의 로그를 취하고 그것을 미분하여 0을 만족하는 경우를 MLE라고 한다.\n4 회귀모형에서 최적의 회귀계수(\\(\\beta_0,\\beta_1\\))을 구하는 방법은 MLE를 이용하여 구하는 방법과 동일하다.",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-05-01. intro.html",
    "href": "posts/PBA2024/2024-05-05-01. intro.html",
    "title": "01. intro",
    "section": "",
    "text": "금일 수업에서는 R, Python을 활용한 데이터 분석을 간략히 소개",
    "crumbs": [
      "Posts",
      "PBA2024",
      "01. intro"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-05-01. intro.html#supervised-learning",
    "href": "posts/PBA2024/2024-05-05-01. intro.html#supervised-learning",
    "title": "01. intro",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n1 설명변수 \\(X = (x_1,x_2,\\dots x_p)\\)로 부터 target변수인 \\(Y\\)를 예측하는 것!\n\\[y \\approx \\beta_1x + \\beta_0\\]\n2 가장 중요한 개념!\n\\[\\text{총 제곱합} = \\text{회귀제곱합} + \\text{잔차제곱합}\\]\n\\[\\sum{(y-\\bar y)}^2 = \\sum{(\\hat y - \\bar y)}^2 + \\sum{( y - \\hat y)}^2 \\]\n\\[\\text{SST} = \\text{SSR} + \\text{SSE}\\]\n3 일반적으로 \\(\\text{SSR}\\) 회귀식으로 설명할 수 있는 변동이라고 하며, 회귀분석 시 가장 관심있는 부분이다.\n4 총 제곱합 \\(\\text{SST}\\)는 변하지 않는 값이므로, SSR이 커지면 SSE즉, 예측값과 실제값의 차이는 줄어들게 된다.\n5 이는, 주어진 데이터로부터 우리가 적합한 회귀직선이 어떤 현상을 잘 설명하고 있다고 볼 수 있다.\n6 그러면, 회귀모형 적합 후, 우리는 무엇을 검토해야 하나?\n\n모형 내의 개별 회귀계수에 대한 검정(t통계량, p-value확인)\n\n\\[H_0 : \\beta_1 =0 \\quad\\text{ vs } \\quad H_1  : \\text{not } H_0\\]\n\n모형에 설명력인 결정계수 값 \\(R^2\\)값 확인, 설명변수의 수가 많아질 경우 \\(adj-R^2\\)값을 확인\n\n\\[R^2 = \\frac{SSR}{SST} = \\frac{1-SSE}{SST}\\]\n\\[R^2_{adj} = \\frac{SSR\\,/\\,(n-(p+1))}{SST\\,/\\,(n-1)} \\]\n\n회귀모형이 통계적을 유의한가?(F-통계량, p-value확인)\n\n\\[H_0 : \\beta_1 = \\beta_2 = \\beta_3 = \\dots 0 \\quad\\text{ vs } \\quad H_1  : \\text{not } H_0\\]\n\n잔차 plot을 통한 모형 진단\n\n\nex1. cars(Simple Linear regression)\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ---------------------------------------------------------------- tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.3     v tidyr     1.3.1\nv purrr     1.0.2     \n-- Conflicts ---------------------------------------------------------------------------------- tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nhead(cars)\n\n\n\nA data.frame: 6 × 2\n\n\n\nspeed\ndist\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n4\n2\n\n\n2\n4\n10\n\n\n3\n7\n4\n\n\n4\n7\n22\n\n\n5\n8\n16\n\n\n6\n9\n10\n\n\n\n\n\n\n- 우리는 주어진 데이터로부터 아래왜 같은 현상을 예측하고 싶음\n\\[\\text{dist} = \\beta_1\\times\\text{speed} + \\beta_0\\]\n- 즉, 속력에 따른 거리를 예측하기 위한 \\(\\beta_1, \\beta_0\\)를 추정! \\(\\to\\) \\((\\hat{\\beta_1}, \\hat{\\beta_0})\\)\n- 일반적으로, 알려진 단순선형회귀분석에서의 \\((\\hat{\\beta_1}, \\hat{\\beta_0})\\)을 구하는 방법!\n\\[\\hat {\\beta_0} = \\bar y - \\hat {\\beta_1}\\bar x\\]\n\\[\\hat {\\beta_1} = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum(x-\\bar x)(y-\\bar y)}{\\sum(x-\\bar x)^2}\\]\n- 공식 유도법(나중에 시간되면 한번 풀어보세요!)\n\n\n\n- 실제로 저렇게 계산한 값과 R에서 회귀분석을 적합한 값이 일치한지 확인해보자\n\nx = cars$speed\ny = cars$dist\n\n\nbar_y = mean(y)\nbar_x = mean(x)\n\nbeta_1 = sum((x-bar_x)*(y-bar_y))/(sum((x-bar_x)^2))\n\nbeta_0 = bar_y - beta_1*bar_x\n\n\nbeta_1\n\n3.93240875912409\n\n\n\nbeta_0\n\n-17.579094890511\n\n\n- 즉, 이론상으로 속도에 따른 거리는 다음과 같이 설명할 수 있다\n\\[\\text{dist} \\approx  3.9324 \\times \\text{spped} -17.579... \\]\n\nlm1 = lm(y~x)\n\n\nlm1\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n    -17.579        3.932  \n\n\n- 결과해석\n\n9.464^2\n\n89.567296\n\n\n\nsummary(lm1)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nx             3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n\nResiduals : 실제값과 예측값의 차이\n\n\\[\\varepsilon_i = \\hat{y_i} - y_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2)\\]\n\n잔차 제곱합은 다음과 같이 표현한다.\n\n\\[SSE =  \\sum_{i=1}^{n} \\varepsilon^2\\]\n\noptions(repr.plot.width = 8, repr.plot.height = 5)\nplot(y - lm1$fitted.values,main = \"잔차 plot\")\n\n\n\n\n\n\n\n\n\nplot(lm1,1)\n\n\n\n\n\n\n\n\n\nextra. subplots 그리는법\n\n\noptions(repr.plot.width = 12, repr.plot.height = 4)\npar(mfrow = c(1,2))\nplot(y - lm1$fitted.values,main = \"잔차 plot\")\nplot(lm1,1)\n\n\n\n\n\n\n\n\n\np-value? 검정통계량의 근거하여 적합한 모형의 유의미성을 검증하는 척도\n\n\nsummary(lm1)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nx             3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n\n결과해석\n1 추정된 회귀계수 \\(\\hat{\\beta_1},\\hat{\\beta_0}\\)는 t-검정통계량의 근거한 p-value값을 보았을 때 0.05보다 작아 통계적으로 유의하다.\n2 결정계수값을 살펴본 결과 적합한 모형은 65% 정도의 설명력을 가지고 있다.\n3 또한, F-통계량의 근거한 p-value값을 보아도 생성된 모델은 통계적으로 유의하다.\n4 잔차 plot을 그려본 결과 오차항의 정규성과 독립성 가정에 위배되지 않았음\n\npar(mfrow=c(1,2))\nplot(lm1,1);plot(lm1,2)\n\n\n\n\n\n\n\n\n5 따라서, 우리가 적합한 모델은 속도에 따른 자동차의 주행거리를 설명하기에 적합한 모형이라고 할 수 있다.\n\n\n\n\nex2. adult(logistic)\n- 로지스틱 모형은 target 변수인 \\(y\\)에 대한 직접적 모형화가 아닌 \\(y\\)가 특정 범주에 포함될 확률을 모형화한다.\n- 임계치(threshole)를 정하고 어떤 범주에 포함될 확률이 임계치보다 높으면 0 or 1로 예측하는 분류 모형이다. (이진분류에서!)\n\n모형 유도\nstep1 : 0과 1사이의 값으로 예측해 주는 모형 설계\n\\[P(X) = \\frac{exp(\\beta_0+\\beta_1X)}{1+exp(\\beta_0+\\beta_1X)}, \\quad \\in (0,1)\\]\nstep2. odds, 배팅을 하는 분야에서 확률 대신 많이 쓰이는 측도\n\\[\\text{odds} = \\frac{P(X)}{1-P(x)}= {exp(\\beta_0 + \\beta_1X)}, \\quad \\in(0, \\infty)\\]\nstep3. logit, 배팅을하는 분야에서 확률 대신 많이 쓰이는 측도\n\\[\\text{logit} = \\log{\\frac{P(X)}{1-P(x)}}  = \\beta_0 + \\beta_1 x, \\quad \\in (-\\infty, \\infty)\\]\n즉, 우리는 로짓에서 보이는 \\(\\beta_0, \\beta_1\\)을 추정하는 것임\n\n\n실습, 주식 데이터\n\n#install.packages(\"ISLR\")\nlibrary(ISLR)\nlibrary(tidyverse)\n#names(Smarket)\n\n\nhead(Smarket)\n\n\n\nA data.frame: 6 × 9\n\n\n\nYear\nLag1\nLag2\nLag3\nLag4\nLag5\nVolume\nToday\nDirection\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n1\n2001\n0.381\n-0.192\n-2.624\n-1.055\n5.010\n1.1913\n0.959\nUp\n\n\n2\n2001\n0.959\n0.381\n-0.192\n-2.624\n-1.055\n1.2965\n1.032\nUp\n\n\n3\n2001\n1.032\n0.959\n0.381\n-0.192\n-2.624\n1.4112\n-0.623\nDown\n\n\n4\n2001\n-0.623\n1.032\n0.959\n0.381\n-0.192\n1.2760\n0.614\nUp\n\n\n5\n2001\n0.614\n-0.623\n1.032\n0.959\n0.381\n1.2057\n0.213\nUp\n\n\n6\n2001\n0.213\n0.614\n-0.623\n1.032\n0.959\n1.3491\n1.392\nUp\n\n\n\n\n\n\n-데이터 설명\n\nlag\\(_i\\) : i번째 전 날의 smarket 주식 종가\ntoday : 오늘 주식 종가\nDirection : 주식이 올라갔는지, 떨어졌는지\nVolume : 거래량\n\n1 데이터셋 분할 7:3으로 분할\n\ntrain &lt;- Smarket %&gt;% sample_frac(0.7)\ntest &lt;- Smarket %&gt;% setdiff(train)\n\n\nglimpse(test)\n\nRows: 375\nColumns: 9\n$ Year      &lt;dbl&gt; 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, ~\n$ Lag1      &lt;dbl&gt; 0.381, 0.959, 1.392, -0.403, 1.303, -0.189, -0.562, 0.359, -~\n$ Lag2      &lt;dbl&gt; -0.192, 0.381, 0.213, 1.392, 0.027, -0.498, 0.701, -1.747, -~\n$ Lag3      &lt;dbl&gt; -2.624, -0.192, 0.614, 0.213, -0.403, 0.287, 0.680, 0.546, 0~\n$ Lag4      &lt;dbl&gt; -1.055, -2.624, -0.623, 0.614, 1.392, 1.303, -0.189, -0.562,~\n$ Lag5      &lt;dbl&gt; 5.010, -1.055, 1.032, -0.623, 0.213, 0.027, -0.498, 0.701, 0~\n$ Volume    &lt;dbl&gt; 1.19130, 1.29650, 1.44500, 1.40780, 1.23260, 1.09800, 1.2953~\n$ Today     &lt;dbl&gt; 0.959, 1.032, -0.403, 0.027, 0.287, 0.680, 0.546, -0.151, -0~\n$ Direction &lt;fct&gt; Up, Up, Down, Up, Up, Up, Up, Down, Down, Down, Down, Down, ~\n\n\n2 모형 적합\n\nlogit_fit &lt;-glm(Direction ~., data = train, \n                family = binomial(link = \"probit\")) ## 이진분류이므로 \"binomial\"이라고 기입\n\nWarning message:\n\"glm.fit: algorithm did not converge\"\nWarning message:\n\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"\n\n\n3 예측\n\nlibrary(caret)\n\n필요한 패키지를 로딩중입니다: lattice\n\n\n다음의 패키지를 부착합니다: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n\n\n\npred &lt;-  ifelse(predict(logit_fit,test, type = \"response\") &gt; 0.5, \"Up\", \"Down\") %&gt;% as_factor()\n\n\nconfusionMatrix(factor(pred), factor(test$Direction))\n\nWarning message in confusionMatrix.default(factor(pred), factor(test$Direction)):\n\"Levels are not in the same order for reference and data. Refactoring data to match.\"\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Down  Up\n      Down  185   0\n      Up      1 189\n                                          \n               Accuracy : 0.9973          \n                 95% CI : (0.9852, 0.9999)\n    No Information Rate : 0.504           \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.9947          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9946          \n            Specificity : 1.0000          \n         Pos Pred Value : 1.0000          \n         Neg Pred Value : 0.9947          \n             Prevalence : 0.4960          \n         Detection Rate : 0.4933          \n   Detection Prevalence : 0.4933          \n      Balanced Accuracy : 0.9973          \n                                          \n       'Positive' Class : Down            \n                                          \n\n\n\n\n정밀도, 재현율, 정확도\n- 위 네 가지는, 분류 모델의 성능 평가시 사용되는 척도이며 처음 접하는 사람들이 많이 헷갈려한다.\n- 위 예제는 4가지 지표에서 뛰어난 성능을 보이나 그렇지 않은 경우가 존재한다면?\n\nas.table(matrix(c(9644,23,252,81), nrow = 2))\n\n     A    B\nA 9644  252\nB   23   81\n\n\n- 정확도(accuracy)를 구해보자\n\n(9644+81)/(9644+23+252+81)\n\n0.9725\n\n\n- 오, 괜찮은 것 같다?\n\n그러나 여기에는 함정이 존재한다.(재현율)\n\n\n(81)/(252+81)\n\n0.243243243243243\n\n\n\n어라? 전체 B중에 예측모델이 B라고 예측한 것은 0.24..밖에 안되네?\n기업의 입장에서 보았을 때, 정확도보단 재현율이 더 중요한 지표임\n즉, 실제 참인 것 중에 예측모델이 참이라고 예측한 비율이 낮은 경우가 발생할 수 있음\n\n- 정밀도 : 모델이 참이라고 예측한 것 중, 실제 참인 것의 비율\n\n81/(23+81)\n\n0.778846153846154\n\n\n- 즉, 우리가 분류모델을 설계하고, 에측 성능을 판단할 때 단순히 정확도만 보고 판단하면 안 된다는 것!",
    "crumbs": [
      "Posts",
      "PBA2024",
      "01. intro"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-05-01. intro.html#unsupervised-learning",
    "href": "posts/PBA2024/2024-05-05-01. intro.html#unsupervised-learning",
    "title": "01. intro",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n- 설명변수 \\(X=(x_1, x_2, \\dots)\\)로 부터 비슷한 성질을 갖는 녀석들끼리 한 집단으로 묶는 것\n- 대표적인 알고리즘 : kmeans, dbscan, hdbscan 등등\n\nk-means\n1주어진 데이터를 k개의 클러스터로 묶는 알고리즘, 각 클러스터와 거리 차이의 분산을 최소화 하는 방식으로 동작\n2 허나 군집의 수, 가중치와 거리 정의가 어려우며, 사전에 주어진 목적이 없으므로 결과 해석이 어려움.\n3 또한 잡음이나 이상값의 영향을 받으며 초기 군집 수를 결정해야 한다는 단점이 있다.\n\n#install.packages(\"patchwork\")\n\n\n\nCode\noptions(repr.plot.res = 200)\n\np1 = ggplot(iris, \n       aes(Petal.Length, Petal.Width, color = Species)) + \n            geom_point()\n\np2 = ggplot(iris, \n       aes(Petal.Length, Petal.Width)) + \n            geom_point()\n\nlibrary(patchwork)\n\np1 + p2\n\n\n\n\n\n\n\n\n\n- 만약, 우리가 생성한 군집이 잘 생성되었다면? 완쪽과 같은 분포로 군집이 형성되어야 한다.\n\n\nCode\nic &lt;- kmeans(iris[,3:4], centers = 3)\n\niris$cluster &lt;- as.factor(ic$cluster)\n\np1 = ggplot(iris, \n       aes(Petal.Length, Petal.Width, color = Species)) + \n            geom_point()\n\np2 = ggplot(iris, \n       aes(Petal.Length, Petal.Width, color = cluster)) + \n            geom_point()\n\np1 + p2",
    "crumbs": [
      "Posts",
      "PBA2024",
      "01. intro"
    ]
  },
  {
    "objectID": "posts/2024-03-26-00. jupyter 단축키 추가법.html",
    "href": "posts/2024-03-26-00. jupyter 단축키 추가법.html",
    "title": "00. jupyter lab 단축키 추가",
    "section": "",
    "text": "intro\n- 예전엔 그냥 주피터에서 R을 쓸 때 %&gt;% 같은 파이프 연산자와 &lt;- 같은 변수 정의를 위한 단축키가 잘 작동되었는데 버전 문제인가? 이제는 적용되지 않더라….\n- 이것 때문에 하루를 날리고 끝내 방법을 찾음\n- 이제 단축키를 내 마음대로 추가할 수 있음 (너무 좋앙)\n\n\n단축키 추가법\n1 settings Editor -&gt; keyboard shortcuts 클릭!\n2 그럼 상단에 Json Setting Editor라고 눈 동그랗게 뜨고 보면 보임\n\n\n\n3 이제 저기로 들어가서 아래와 같은 json 코드를 삽입하자\n\n\n\n4 후. 이것 때문에 하루 다 날린 걸 생각하면 열이 받지만, 다시금 다양한 언어를 잘 활용할 줄 아는 것의 중요성을 느꼈다.\n5 ref\n\n\ntest\n\nlibrary(tidyverse)\n\n\nmpg %&gt;% ggplot(aes(x = hwy, y = cty, color = cyl)) +\n            geom_point(alpha = 0.5, size = 2) +\n            scale_color_viridis_c() +\n            theme_minimal()",
    "crumbs": [
      "Posts",
      "00. jupyter lab 단축키 추가"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "edu1",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 26, 2024\n\n\n04. advanced LM\n\n\nGC \n\n\n\n\nMay 19, 2024\n\n\n03. MLE\n\n\ngc \n\n\n\n\nMay 12, 2024\n\n\n02. stat & ML basic\n\n\nGC \n\n\n\n\nMay 5, 2024\n\n\n01. intro\n\n\nGC \n\n\n\n\nApr 28, 2024\n\n\nExtra 00. tidydata\n\n\nGC \n\n\n\n\nApr 15, 2024\n\n\n00. Intro\n\n\nGC \n\n\n\n\nMar 28, 2024\n\n\n00. jupyter lab 단축키 추가\n\n\nGC \n\n\n\n\nAug 19, 2023\n\n\n00. Plotly test\n\n\nGC \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/2023-08-19-00. Plotly test.html",
    "href": "posts/2023-08-19-00. Plotly test.html",
    "title": "00. Plotly test",
    "section": "",
    "text": "import\n\nimport plotly.express as ex\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/kalilurrahman/datasets/main/mobilephonemktshare2020.csv')\ndf.head()\n\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n31.49\n22.09\n10.02\n7.79\n4.10\n3.15\n2.41\n2.40\n9.51\n0.54\n2.35\n0.95\n0.96\n0.70\n0.84\n0.74\n\n\n1\n2019-11\n31.36\n22.90\n10.18\n8.16\n4.42\n3.41\n2.40\n2.40\n9.10\n0.78\n0.66\n0.97\n0.97\n0.73\n0.83\n0.75\n\n\n2\n2019-12\n31.37\n24.79\n9.95\n7.73\n4.23\n3.19\n2.50\n2.54\n8.13\n0.84\n0.75\n0.90\n0.87\n0.74\n0.77\n0.70\n\n\n3\n2020-01\n31.29\n24.76\n10.61\n8.10\n4.25\n3.02\n2.42\n2.40\n7.55\n0.88\n0.69\n0.88\n0.86\n0.79\n0.80\n0.69\n\n\n4\n2020-02\n30.91\n25.89\n10.98\n7.80\n4.31\n2.89\n2.36\n2.34\n7.06\n0.89\n0.70\n0.81\n0.77\n0.78\n0.80\n0.69\n\n\n\n\n\n\n\n\n\ndf.set_index(\"Date\").diff().\\\n  dropna().boxplot(backend = \"plotly\")",
    "crumbs": [
      "Posts",
      "00. Plotly test"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html",
    "title": "Extra 00. tidydata",
    "section": "",
    "text": "- 제가 자주하는 데이터 분석의 단계\n\n데이터의 기본 정보 (df.info 등등)을 확인\n데이터를 tidy하게 바꾼 후 EDA(탐색적 자료 분석), 통계 분석(t-test) 등등 수행\ntidy데이터를 바탕으로 모델링 수행\n결과 요약 및 해석 -&gt; 결과에 대한 자료 작성 시에도 tidy한 데이터가 사용됨\n\n- 금일 수업 요약\n\ntidy 데이터의 정의\ntidy 데이터를 활용하지 않은 시각화 (matplitlib)\ntidy 데이터를 활용한 시각화 (seaborn, ggplot, plotly 등등…)\n심슨의 역설",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#데이터-생성-tidy-x",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#데이터-생성-tidy-x",
    "title": "Extra 00. tidydata",
    "section": "데이터 생성 (tidy : (x))",
    "text": "데이터 생성 (tidy : (x))\n\n\nCode\nmale_w = np.random.randint(60,100,10000) + np.random.randn(10000)\nfemale_w = np.random.randint(40,80,10000) + np.random.randn(10000)\nmale_h = (np.random.randint(160,200,10000) + np.random.randn(10000))/100\nfemale_h = (np.random.randint(140,180,10000) + np.random.randn(10000))/100\n\n\nbmi_m =  male_w/(male_h**2)\nbmi_f =  female_w/(female_h**2)\n\nbins = [-np.Inf, 18.5,23,25,np.Inf]\nlabels = [\"저체중\",\"정상\",\"과체중\",\"비만\"]\n\ndf = pd.DataFrame([male_w,female_w,male_h,female_h,bmi_m,bmi_f]).T\n\ndf.columns = [\"male_w\",\"female_w\",\"male_h\",\"female_h\",\"bmi_m\",\"bmi_f\"]\n\n\ndf[\"bmi_label_M\"] = pd.cut(df.bmi_m, bins = bins, labels =labels)\ndf[\"bmi_label_F\"] = pd.cut(df.bmi_f, bins = bins, labels =labels)\n\n\nbins_m = [-np.Inf, 1.60,1.70,1.80,1.90,np.Inf]\nlabels = [\"매우 작은키\",\"작은키\",\"보통\",\"큰키\",\"매우 큰키\"]\n\nbins_f = [-np.Inf, 1.40,1.50,1.60,1.70,np.Inf]\n\n\ndf[\"h_label_M\"] = pd.cut(df.male_h, bins = bins_m, labels =labels)\ndf[\"h_label_F\"] = pd.cut(df.female_h, bins = bins_f, labels =labels)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\n\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n95.512228\n77.760589\n1.747844\n1.555205\n31.264669\n32.150229\n비만\n비만\n보통\n보통\n\n\n1\n78.657688\n65.837765\n1.648885\n1.572286\n28.930805\n26.632491\n비만\n비만\n작은키\n보통\n\n\n2\n92.135422\n41.169597\n1.756829\n1.611968\n29.851614\n15.843971\n비만\n저체중\n보통\n큰키\n\n\n3\n85.688058\n55.415655\n1.750162\n1.560285\n27.974579\n22.762752\n비만\n정상\n보통\n보통\n\n\n4\n83.454618\n43.582328\n1.625592\n1.502553\n31.581097\n19.304156\n비만\n정상\n작은키\n보통",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#시각화-1-matplotlib-tidy-x",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#시각화-1-matplotlib-tidy-x",
    "title": "Extra 00. tidydata",
    "section": "시각화 1 (matplotlib, tidy x)",
    "text": "시각화 1 (matplotlib, tidy x)\n- 일단 남자와 여자의 BMI 지수를 비교하고 싶음\n- 데이터 구조\n\ndf.head()\n\n\n\n\n\n\n\n\n\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n69.401280\n59.315255\n1.625395\n1.588538\n26.269360\n23.505598\n비만\n과체중\n작은키\n보통\n\n\n1\n96.582571\n71.060495\n1.754996\n1.704732\n31.357869\n24.452105\n비만\n과체중\n보통\n매우 큰키\n\n\n2\n64.515219\n73.489190\n1.891025\n1.616301\n18.041283\n28.130585\n저체중\n비만\n큰키\n큰키\n\n\n3\n96.048789\n56.877694\n1.831055\n1.723334\n28.647643\n19.151515\n비만\n정상\n큰키\n매우 큰키\n\n\n4\n91.834611\n69.049447\n1.726567\n1.726978\n30.806303\n23.151891\n비만\n과체중\n보통\n매우 큰키\n\n\n\n\n\n\n\n\n- 시각화\n\nplt.figure(figsize=(12,4))\nplt.plot(df.male_w,df.bmi_m,\".\",alpha=0.3)\nplt.plot(df.female_w,df.bmi_f,\".\",alpha=0.3)\nplt.legend([\"남\",\"여\"])\nplt.title(\"남자와 여자 몸무게에 따른 BMI\")\n\nText(0.5, 1.0, '남자와 여자 몸무게에 따른 BMI')",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#시각화-2-matplotlib.pyplot-tidy-x-subplot",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#시각화-2-matplotlib.pyplot-tidy-x-subplot",
    "title": "Extra 00. tidydata",
    "section": "시각화 2 (matplotlib.pyplot, tidy x, subplot)",
    "text": "시각화 2 (matplotlib.pyplot, tidy x, subplot)\n\nfig, axes = plt.subplots(1,2,figsize=(12,4))\n\nax1,ax2 = axes\n\nax1.plot(df.male_w,df.bmi_m,\".b\",alpha=0.3)\nax1.set_title(\"남자 몸무게에 따른 BMI\")\nax2.plot(df.female_w,df.bmi_f,\".r\",alpha=0.3)\nax2.set_title(\"여자 몸무게에 따른 BMI\")\n\nText(0.5, 1.0, '여자 몸무게에 따른 BMI')",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#의문-1",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#의문-1",
    "title": "Extra 00. tidydata",
    "section": "의문 1",
    "text": "의문 1\n- 아래의 데이터 구조를 살펴보자.\n\ndf.head()\n\n\n\n\n\n\n\n\n\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n72.570505\n53.415130\n1.773998\n1.671568\n23.059717\n19.116853\n과체중\n정상\n보통\n큰키\n\n\n1\n93.896500\n43.278004\n1.883459\n1.612035\n26.468980\n16.653995\n비만\n저체중\n큰키\n큰키\n\n\n2\n82.263387\n71.909528\n1.607014\n1.524727\n31.854236\n30.931608\n비만\n비만\n작은키\n보통\n\n\n3\n99.802621\n64.476794\n1.871943\n1.761914\n28.481097\n20.769874\n비만\n정상\n큰키\n매우 큰키\n\n\n4\n78.356406\n42.978916\n1.658058\n1.694691\n28.501984\n14.964927\n비만\n저체중\n작은키\n큰키\n\n\n\n\n\n\n\n\n- 분석가들에게 시각화 및 모델링 시 편한것\n\n한 컬럼에 모든 데이터의 대한 특정 수치(몸무게면 몸무게, 키면 키)가 있으면 좋겠음\n그리고 또 다른 컬럼에는 그것의 범주(성별, BmI수준)을 알려주는 컬럼이 있었으면 좋겠음\n이게 편한 이유? 시각화 수행 시 아래의 긴 코드를 한 줄로 쓸 수 있다!! \\(\\to\\) (지금은 남,녀 두 개의 범주지만, 범주의 수가 늘어나면 코드 라인 수도 늘어난다.)\n\nplt.figure(figsize=(12,4))\nplt.plot(df.male_w,df.bmi_m,\".\",alpha=0.3)\nplt.plot(df.female_w,df.bmi_f,\".\",alpha=0.3)\nplt.legend([\"남\",\"여\"])\nplt.title(\"남자와 여자 몸무게에 따른 BMI\")\n\n또한, 모델링 수행시 입력데이터는 tidy한 형태로 들어가기 때문이다.(이미지, 텍스트 제외)",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#tidydata-만들기",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#tidydata-만들기",
    "title": "Extra 00. tidydata",
    "section": "tidydata 만들기",
    "text": "tidydata 만들기",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#인데스를-id로-변경",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#인데스를-id로-변경",
    "title": "Extra 00. tidydata",
    "section": "1. 인데스를 ID로 변경",
    "text": "1. 인데스를 ID로 변경\n\ndf.head()\n\n\n\n\n\n\n\n\n\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n95.512228\n77.760589\n1.747844\n1.555205\n31.264669\n32.150229\n비만\n비만\n보통\n보통\n\n\n1\n78.657688\n65.837765\n1.648885\n1.572286\n28.930805\n26.632491\n비만\n비만\n작은키\n보통\n\n\n2\n92.135422\n41.169597\n1.756829\n1.611968\n29.851614\n15.843971\n비만\n저체중\n보통\n큰키\n\n\n3\n85.688058\n55.415655\n1.750162\n1.560285\n27.974579\n22.762752\n비만\n정상\n보통\n보통\n\n\n4\n83.454618\n43.582328\n1.625592\n1.502553\n31.581097\n19.304156\n비만\n정상\n작은키\n보통\n\n\n\n\n\n\n\n\n\ndf.reset_index(inplace=True)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nindex\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n0\n95.512228\n77.760589\n1.747844\n1.555205\n31.264669\n32.150229\n비만\n비만\n보통\n보통\n\n\n1\n1\n78.657688\n65.837765\n1.648885\n1.572286\n28.930805\n26.632491\n비만\n비만\n작은키\n보통\n\n\n2\n2\n92.135422\n41.169597\n1.756829\n1.611968\n29.851614\n15.843971\n비만\n저체중\n보통\n큰키\n\n\n3\n3\n85.688058\n55.415655\n1.750162\n1.560285\n27.974579\n22.762752\n비만\n정상\n보통\n보통\n\n\n4\n4\n83.454618\n43.582328\n1.625592\n1.502553\n31.581097\n19.304156\n비만\n정상\n작은키\n보통\n\n\n\n\n\n\n\n\n\ndf.rename(columns={\"index\" : \"ID\" },inplace=True)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nID\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n0\n95.512228\n77.760589\n1.747844\n1.555205\n31.264669\n32.150229\n비만\n비만\n보통\n보통\n\n\n1\n1\n78.657688\n65.837765\n1.648885\n1.572286\n28.930805\n26.632491\n비만\n비만\n작은키\n보통\n\n\n2\n2\n92.135422\n41.169597\n1.756829\n1.611968\n29.851614\n15.843971\n비만\n저체중\n보통\n큰키\n\n\n3\n3\n85.688058\n55.415655\n1.750162\n1.560285\n27.974579\n22.762752\n비만\n정상\n보통\n보통\n\n\n4\n4\n83.454618\n43.582328\n1.625592\n1.502553\n31.581097\n19.304156\n비만\n정상\n작은키\n보통",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#df.meltweight-테이블-생성",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#df.meltweight-테이블-생성",
    "title": "Extra 00. tidydata",
    "section": "2. df.melt(weight) 테이블 생성",
    "text": "2. df.melt(weight) 테이블 생성\n- 한 열에는 셩별(sex), 다른 한 열에는 무게(weight)를 표시\n\ndf.loc[:,[\"ID\",\"male_w\",\"female_w\"]].head(4)\n\n\n\n\n\n\n\n\n\nID\nmale_w\nfemale_w\n\n\n\n\n0\n0\n95.512228\n77.760589\n\n\n1\n1\n78.657688\n65.837765\n\n\n2\n2\n92.135422\n41.169597\n\n\n3\n3\n85.688058\n55.415655\n\n\n\n\n\n\n\n\n\ndf.loc[:,[\"ID\",\"male_w\",\"female_w\"]].\\\n        melt(id_vars=[\"ID\"],\n        value_vars = [\"male_w\",\"female_w\"],\n        var_name = \"sex\",value_name =\"weight\")\n\n\n\n\n\n\n\n\n\nID\nsex\nweight\n\n\n\n\n0\n0\nmale_w\n95.512228\n\n\n1\n1\nmale_w\n78.657688\n\n\n2\n2\nmale_w\n92.135422\n\n\n3\n3\nmale_w\n85.688058\n\n\n4\n4\nmale_w\n83.454618\n\n\n...\n...\n...\n...\n\n\n19995\n9995\nfemale_w\n72.715375\n\n\n19996\n9996\nfemale_w\n78.467711\n\n\n19997\n9997\nfemale_w\n67.947894\n\n\n19998\n9998\nfemale_w\n51.354418\n\n\n19999\n9999\nfemale_w\n48.809833\n\n\n\n\n20000 rows × 3 columns\n\n\n\n\n\nweight = df.melt(id_vars=[\"ID\"],\n            value_vars=[\"male_w\",\"female_w\"],\n            var_name = \"sex\",value_name='weight')\n\nweight.sex = weight.sex.replace(\"male_w\",\"m\").replace(\"female_w\",\"f\")\nweight.head()\n\n\n\n\n\n\n\n\n\nID\nsex\nweight\n\n\n\n\n0\n0\nm\n69.401280\n\n\n1\n1\nm\n96.582571\n\n\n2\n2\nm\n64.515219\n\n\n3\n3\nm\n96.048789\n\n\n4\n4\nm\n91.834611\n\n\n\n\n\n\n\n\n\n생성한 테이블을 다시 돌릴려면?\n\n_t = weight.pivot(index=\"ID\",columns = \"sex\", values = \"weight\").head()\n_t.columns = [\"male_w\",\"male_f\"]\n_t.reset_index()\n\n\n\n\n\n\n\n\n\nID\nmale_w\nmale_f\n\n\n\n\n0\n0\n59.315255\n69.401280\n\n\n1\n1\n71.060495\n96.582571\n\n\n2\n2\n73.489190\n64.515219\n\n\n3\n3\n56.877694\n96.048789\n\n\n4\n4\n69.049447\n91.834611",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#df.meltbmi-테이블-생성",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#df.meltbmi-테이블-생성",
    "title": "Extra 00. tidydata",
    "section": "3. df.melt(BMI) 테이블 생성",
    "text": "3. df.melt(BMI) 테이블 생성\n\nbmi = df.melt(id_vars=[\"ID\"],\n            value_vars=[\"bmi_f\",\"bmi_m\"],\n            var_name = \"sex\",value_name='bmi')\n\nbmi.sex = bmi.sex.replace(\"bmi_m\",\"m\").replace(\"bmi_f\",\"f\")\nbmi.head()\n\n\n\n\n\n\n\n\n\nID\nsex\nbmi\n\n\n\n\n0\n0\nf\n23.505598\n\n\n1\n1\nf\n24.452105\n\n\n2\n2\nf\n28.130585\n\n\n3\n3\nf\n19.151515\n\n\n4\n4\nf\n23.151891",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#weight-테이블과-bmi-테이블-조인",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#weight-테이블과-bmi-테이블-조인",
    "title": "Extra 00. tidydata",
    "section": "4. weight 테이블과 bmi 테이블 조인",
    "text": "4. weight 테이블과 bmi 테이블 조인\n\ndf1 = pd.merge(weight,bmi, \n         left_on=['ID','sex'], right_on=['ID','sex'])\n\n\ndf1.head()\n\n\n\n\n\n\n\n\n\nID\nsex\nweight\nbmi\n\n\n\n\n0\n0\nm\n69.401280\n26.269360\n\n\n1\n1\nm\n96.582571\n31.357869\n\n\n2\n2\nm\n64.515219\n18.041283\n\n\n3\n3\nm\n96.048789\n28.647643\n\n\n4\n4\nm\n91.834611\n30.806303",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#시각화-3plotlytidy-o",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#시각화-3plotlytidy-o",
    "title": "Extra 00. tidydata",
    "section": "5. 시각화 3(plotly,tidy (o))",
    "text": "5. 시각화 3(plotly,tidy (o))\n\ndf1.plot(x=\"weight\", y= \"bmi\",backend=\"plotly\",kind=\"scatter\",\n                color= \"sex\", title= \"남녀 몸무게에 따른 BMI\")",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#시각화-4plotlytidy-o",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#시각화-4plotlytidy-o",
    "title": "Extra 00. tidydata",
    "section": "6. 시각화 4(plotly,tidy (o))",
    "text": "6. 시각화 4(plotly,tidy (o))\n\ndf1.plot(x=\"weight\", y= \"bmi\",backend=\"plotly\",kind=\"scatter\",\n                color= \"sex\", title= \"남녀 몸무게에 따른 BMI\",facet_col=df1.sex)",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#데이터-비교",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#데이터-비교",
    "title": "Extra 00. tidydata",
    "section": "7. 데이터 비교",
    "text": "7. 데이터 비교\n1 tidy (x)\n\ndf.iloc[:,[0,1,2,5,6]].head()\n\n\n\n\n\n\n\n\n\nID\nmale_w\nfemale_w\nbmi_m\nbmi_f\n\n\n\n\n0\n0\n69.401280\n59.315255\n26.269360\n23.505598\n\n\n1\n1\n96.582571\n71.060495\n31.357869\n24.452105\n\n\n2\n2\n64.515219\n73.489190\n18.041283\n28.130585\n\n\n3\n3\n96.048789\n56.877694\n28.647643\n19.151515\n\n\n4\n4\n91.834611\n69.049447\n30.806303\n23.151891\n\n\n\n\n\n\n\n\n2 tidy (o)\n\ndf1.head()\n\n\n\n\n\n\n\n\n\nID\nsex\nweight\nbmi\n\n\n\n\n0\n0\nm\n69.401280\n26.269360\n\n\n1\n1\nm\n96.582571\n31.357869\n\n\n2\n2\nm\n64.515219\n18.041283\n\n\n3\n3\nm\n96.048789\n28.647643\n\n\n4\n4\nm\n91.834611\n30.806303",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#코드-비교",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#코드-비교",
    "title": "Extra 00. tidydata",
    "section": "8. 코드 비교",
    "text": "8. 코드 비교\n1 tidy(x)\nplt.figure(figsize=(12,4))\nplt.plot(df.male_w,df.bmi_m,\".\",alpha=0.3)\nplt.plot(df.female_w,df.bmi_f,\".\",alpha=0.3)\nplt.legend([\"남\",\"여\"])\nplt.title(\"남자와 여자 몸무게에 따른 BMI\")\nfig, axes = plt.subplots(1,2,figsize=(12,4))\nax1,ax2 = axes\nax1.plot(df.male_w,df.bmi_m,\".b\",alpha=0.3)\nax1.set_title(\"남자 몸무게에 따른 BMI\")\nax2.plot(df.female_w,df.bmi_f,\".r\",alpha=0.3)\nax2.set_title(\"여자 몸무게에 따른 BMI\")\n2 tidy (o)\ndf1.plot(x=\"weight\", y= \"bmi\",backend=\"plotly\",kind=\"scatter\",\n                color= \"sex\", title= \"남녀 몸무게에 따른 BMI\")\ndf1.plot(x=\"weight\", y= \"bmi\",backend=\"plotly\",kind=\"scatter\",\n                color= \"sex\", title= \"남녀 몸무게에 따른 BMI\",facet_col=df1.sex)",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#다차원-plot-height-label",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#다차원-plot-height-label",
    "title": "Extra 00. tidydata",
    "section": "9. 다차원 plot (+height label)",
    "text": "9. 다차원 plot (+height label)\n\ndf.head()\n\n\n\n\n\n\n\n\n\nID\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n0\n69.401280\n59.315255\n1.625395\n1.588538\n26.269360\n23.505598\n비만\n과체중\n작은키\n보통\n\n\n1\n1\n96.582571\n71.060495\n1.754996\n1.704732\n31.357869\n24.452105\n비만\n과체중\n보통\n매우 큰키\n\n\n2\n2\n64.515219\n73.489190\n1.891025\n1.616301\n18.041283\n28.130585\n저체중\n비만\n큰키\n큰키\n\n\n3\n3\n96.048789\n56.877694\n1.831055\n1.723334\n28.647643\n19.151515\n비만\n정상\n큰키\n매우 큰키\n\n\n4\n4\n91.834611\n69.049447\n1.726567\n1.726978\n30.806303\n23.151891\n비만\n과체중\n보통\n매우 큰키\n\n\n\n\n\n\n\n\n\nh = df.melt(id_vars=[\"ID\"],\n            value_vars=[\"h_label_M\",\"h_label_F\"],\n            var_name = \"sex\",value_name='h')\n\nh.sex = h.sex.replace(\"h_label_M\",\"m\").replace(\"h_label_F\",\"f\")\nh.head()\n\n\n\n\n\n\n\n\n\nID\nsex\nh\n\n\n\n\n0\n0\nm\n작은키\n\n\n1\n1\nm\n보통\n\n\n2\n2\nm\n큰키\n\n\n3\n3\nm\n큰키\n\n\n4\n4\nm\n보통\n\n\n\n\n\n\n\n\n\ndf2 = pd.merge(df1,h, \n         left_on=['ID','sex'], right_on=['ID','sex'])\n\n\ndf2.head()\n\n\n\n\n\n\n\n\n\nID\nsex\nweight\nbmi\nh\n\n\n\n\n0\n0\nm\n69.401280\n26.269360\n작은키\n\n\n1\n1\nm\n96.582571\n31.357869\n보통\n\n\n2\n2\nm\n64.515219\n18.041283\n큰키\n\n\n3\n3\nm\n96.048789\n28.647643\n큰키\n\n\n4\n4\nm\n91.834611\n30.806303\n보통\n\n\n\n\n\n\n\n\n\nfig = df2.plot(x=\"weight\",y=\"bmi\",color=\"sex\",facet_col = \"h\",\n             facet_row=\"sex\",kind= \"scatter\",backend = \"plotly\",height=400, width=1000,opacity=0.2)\n\nfig.update_yaxes(matches=None)\nfig.show()",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#summary-및-개인적인-생각",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#summary-및-개인적인-생각",
    "title": "Extra 00. tidydata",
    "section": "summary 및 개인적인 생각",
    "text": "summary 및 개인적인 생각\n1. 물론 데이터를 처음 로드하고 탐색하기 위해 matplotlib를 이용하여 untidy한 데이터를 살펴본다.\n2. 그러나 후에 분석을 하고 그 결과를 시각화할 때는 tidy하지 못한 데이터로 할때 예쁜 그래프를 그리기 어렵다…(이건 개인적인 생각)\n3. 그리고 데이터를 3,4개의 차원(기준)에 따라 그래프를 그릴 때 untidy한 데이터로 어떻게 그릴지 접근조차 어렵다.(물론 가능한 분도 계시겠지만 거기까지 생각하고 싶지 않다.)\n4. 또한, 모델링 수행 수행시 tidy한 데이터가 들어가기 때문에 melt,groupby 등을 이용한 전처리는 필수적으로 갖추어야될 역량임.\n5. 이 부분은 스터디원 분들이 너무 겁먹지 않으셨으면 좋겠다. 하기 싫어도 매번 해야되서 언젠가 몸이 기억하고 반응할 거다.",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#ex2-전북대학교의-합격률-to-남녀-불평등",
    "href": "posts/PBA2024/2024-04-28-Extra 00. tidydata.html#ex2-전북대학교의-합격률-to-남녀-불평등",
    "title": "Extra 00. tidydata",
    "section": "ex2) 전북대학교의 합격률 \\(\\to\\) 남녀 불평등?",
    "text": "ex2) 전북대학교의 합격률 \\(\\to\\) 남녀 불평등?\n\n시각화 1 : 남녀 전체 합격률 비율 시각화\n- 아래와 같은 대학합격률 데이터가 있다고 하자.\n\nDEP : 학과\nSTATE : 합격 여부\nGEN : 성별\nCOUNT : 빈도\n\n\n\nCode\nDEP=(['A1']*2+['A2']*2+['B1']*2+['B2']*2)*2 \nGEN=['M']*8+['F']*8\nSTATE=['PASS','FAIL']*8\nCOUNT=[1,9,2,8,80,20,85,15,5,5,5,5,9,1,9,1]\n\ndf=pd.DataFrame({'DEP':DEP,'STATE':STATE,'GEN':GEN,'COUNT':COUNT})\ndf\n\n\n\n\n\n\n\n\n\n\nDEP\nSTATE\nGEN\nCOUNT\n\n\n\n\n0\nA1\nPASS\nM\n1\n\n\n1\nA1\nFAIL\nM\n9\n\n\n2\nA2\nPASS\nM\n2\n\n\n3\nA2\nFAIL\nM\n8\n\n\n4\nB1\nPASS\nM\n80\n\n\n5\nB1\nFAIL\nM\n20\n\n\n6\nB2\nPASS\nM\n85\n\n\n7\nB2\nFAIL\nM\n15\n\n\n8\nA1\nPASS\nF\n5\n\n\n9\nA1\nFAIL\nF\n5\n\n\n10\nA2\nPASS\nF\n5\n\n\n11\nA2\nFAIL\nF\n5\n\n\n12\nB1\nPASS\nF\n9\n\n\n13\nB1\nFAIL\nF\n1\n\n\n14\nB2\nPASS\nF\n9\n\n\n15\nB2\nFAIL\nF\n1\n\n\n\n\n\n\n\n\n\n\nCode\nt_g_c = df.groupby([\"GEN\",\"STATE\"],as_index=False)[[\"COUNT\"]].sum().\\\n            rename(columns = {\"COUNY\" : \"sum\"})\ngs = t_g_c.groupby(\"GEN\")[\"COUNT\"].sum()\ngs = list(np.repeat(gs,2))\nt_g_c[\"prop\"] = t_g_c[\"COUNT\"]/gs\nt_g_c\n\n\n\n\n\n\n\n\n\n\nGEN\nSTATE\nCOUNT\nprop\n\n\n\n\n0\nF\nFAIL\n12\n0.300000\n\n\n1\nF\nPASS\n28\n0.700000\n\n\n2\nM\nFAIL\n52\n0.236364\n\n\n3\nM\nPASS\n168\n0.763636\n\n\n\n\n\n\n\n\n\n\nCode\nfig = t_g_c.loc[t_g_c.STATE == \"PASS\"].\\\n        plot(x= \"GEN\", y=\"prop\",kind=\"bar\",\n            title=\"남녀 전북대학교의 합격률\",backend= \"plotly\",color=\"GEN\",\n             color_discrete_sequence=[\"red\",\"blue\"],height=500,width=700)\nfig.update_yaxes(range=[0.6, 0.8])\nfig.show()\n\n\n                                                \n\n\n- 전북대학교는 남녀 불평등인 학교인가?\n\n\n시각화 2 : (학과별, 성별) 합격률 비교\n\n\nCode\nt = df.groupby([\"DEP\",\"GEN\"],\n               as_index=False)[[\"COUNT\"]].sum().\\\n                rename(columns = {\"COUNT\":\"SUM\"}).merge(df)\nt[\"prop\"] = t.COUNT/t.SUM\nt\n\n\n\n\n\n\n\n\n\n\nDEP\nGEN\nSUM\nSTATE\nCOUNT\nprop\n\n\n\n\n0\nA1\nF\n10\nPASS\n5\n0.50\n\n\n1\nA1\nF\n10\nFAIL\n5\n0.50\n\n\n2\nA1\nM\n10\nPASS\n1\n0.10\n\n\n3\nA1\nM\n10\nFAIL\n9\n0.90\n\n\n4\nA2\nF\n10\nPASS\n5\n0.50\n\n\n5\nA2\nF\n10\nFAIL\n5\n0.50\n\n\n6\nA2\nM\n10\nPASS\n2\n0.20\n\n\n7\nA2\nM\n10\nFAIL\n8\n0.80\n\n\n8\nB1\nF\n10\nPASS\n9\n0.90\n\n\n9\nB1\nF\n10\nFAIL\n1\n0.10\n\n\n10\nB1\nM\n100\nPASS\n80\n0.80\n\n\n11\nB1\nM\n100\nFAIL\n20\n0.20\n\n\n12\nB2\nF\n10\nPASS\n9\n0.90\n\n\n13\nB2\nF\n10\nFAIL\n1\n0.10\n\n\n14\nB2\nM\n100\nPASS\n85\n0.85\n\n\n15\nB2\nM\n100\nFAIL\n15\n0.15\n\n\n\n\n\n\n\n\n\nfig = t.loc[t.STATE == \"PASS\",:].plot(x=\"GEN\", y=\"prop\", kind = \"bar\",backend = \"plotly\",\n          color= \"GEN\",color_discrete_sequence = [\"red\",\"blue\"],\n        facet_col=\"DEP\",facet_col_wrap=2,height=500,width=700)\n\nfig.update_yaxes(matches=None)\nfig.show()\n\n                                                \n\n\n- 실상은 그렇지 않다!\n\n\nsummary\n- 어떤 현상에 대해서 설명할 때 단순히 이분법적인 사고로 바라보면 안된다.\n\n위 예제처럼 잘못된 오해를 불러올 수 있는 상황이 발생할 수 있지 않은가?\n실제로 학과별 남녀 합격률을 조사한 결과 여학생의 비율이 더 많은 것을 확인했다.\n이 같은 현상을 심슨의 역설 이라고 하며 모델링 과정에서도 굉장히 중요한 부분을 차지한다.",
    "crumbs": [
      "Posts",
      "PBA2024",
      "Extra 00. tidydata"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-12-02. stat & ml basic.html",
    "href": "posts/PBA2024/2024-05-12-02. stat & ml basic.html",
    "title": "02. stat & ML basic",
    "section": "",
    "text": "1 모집단(Population) : 연구 또는 관측 대상이 되는 전체\n\nex : 우리나라 30세이상 월 평균 소득 조사, 사실 이를 위해 해당 집단을 전수 조사하는 것은 거의 불가능에 가까움\n\n2 표본 : 모집단의 일부, 연구 또는 관측을 위해 일정 시점 또는 일정 집단에서 수집한 대상(데이터)\n\nex : 2020년 1월 1일시점에 조사한 30세 이상 국내 성인 남자 10명의 월평균 소득\n\n3 표본평균과 중앙값 구하기\n\n평균\n\n\nincome &lt;- c(100, 200, 300, 400, 500, 600, 700, 800, 1000 ,1000)\n\nsum(income)/length(income)\n\n560\n\n\n\n중앙값\n\n\n#sort(income);length(income)\n\nprint(paste(\"손계산 :\",(sort(income)[5]+sort(income)[6])/2))\n\nprint(paste(\"함수 사용 :\",median(income)))\n\n[1] \"손계산 : 550\"\n[1] \"함수 사용 : 550\"",
    "crumbs": [
      "Posts",
      "PBA2024",
      "02. stat & ML basic"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-12-02. stat & ml basic.html#모집단과-표본",
    "href": "posts/PBA2024/2024-05-12-02. stat & ml basic.html#모집단과-표본",
    "title": "02. stat & ML basic",
    "section": "",
    "text": "1 모집단(Population) : 연구 또는 관측 대상이 되는 전체\n\nex : 우리나라 30세이상 월 평균 소득 조사, 사실 이를 위해 해당 집단을 전수 조사하는 것은 거의 불가능에 가까움\n\n2 표본 : 모집단의 일부, 연구 또는 관측을 위해 일정 시점 또는 일정 집단에서 수집한 대상(데이터)\n\nex : 2020년 1월 1일시점에 조사한 30세 이상 국내 성인 남자 10명의 월평균 소득\n\n3 표본평균과 중앙값 구하기\n\n평균\n\n\nincome &lt;- c(100, 200, 300, 400, 500, 600, 700, 800, 1000 ,1000)\n\nsum(income)/length(income)\n\n560\n\n\n\n중앙값\n\n\n#sort(income);length(income)\n\nprint(paste(\"손계산 :\",(sort(income)[5]+sort(income)[6])/2))\n\nprint(paste(\"함수 사용 :\",median(income)))\n\n[1] \"손계산 : 550\"\n[1] \"함수 사용 : 550\"",
    "crumbs": [
      "Posts",
      "PBA2024",
      "02. stat & ML basic"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-12-02. stat & ml basic.html#평균과-분산",
    "href": "posts/PBA2024/2024-05-12-02. stat & ml basic.html#평균과-분산",
    "title": "02. stat & ML basic",
    "section": "평균과 분산",
    "text": "평균과 분산\n- 모집단의 분포(형태는) 일반적으로 수집한 표본의 모수인 평균과 분산에 의해 결정된다.\n\n평균(기댓값)\n1 이산형일 경우 : 베르누이, 이항, 포아송 분포 등등…\n\nex1 : 동전 던지기를 해서 앞면이 나올 평균확률은?\n\n\n\n\n\\(x\\)\n0\n1\n\n\n\n\n\\(f(x)\\)\n1/2\n1/2\n\n\n\n\\[E(X) = \\sum_{i=1}^{n} x_i f(x_i)\\]\n2 연속형일 경우 : 베타, 지수, 감마, 정규 등등…\n\nex2, 표준정규분포의 평균은?\n\n\\[X \\sim N(0, 1)\\]\n\\[f(x) = \\frac{1}{\\sqrt {2\\pi}} e^{- \\frac {x^2}{2}}\\]\n\n\n숙제 1. 아래의 공식을 이용하여 표준정규분포의 평균이 실제로 1이 나오는지 확인!\n\\[E(X) = \\int xf(x) \\,\\,dx\\]\n\n\n대수의 법칙\n- Law of large numbers\n- 샘플사이즈가 클수록 표본평균은 모집단의 평균으로 수렴한다는 법칙! (체비셰프 부등식을 이용하여 증명 가능!)\n\\[\\begin {align} P(|\\overline X - \\mu| &lt; \\varepsilon)  &=  P(|\\overline X - \\mu| &lt; \\varepsilon^2) \\\\ \\\\\n                                                       &= 1 -  \\frac{E(\\overline X_{n} - \\mu)^2}{\\varepsilon^2} \\\\ \\\\\n                                                       &= 1 - \\frac{\\sigma^2/n}{\\varepsilon^2} \\approx 1 \\end {align} \\]\n- R을 이용한 증명\n\n\n\n\\(x\\)\n100\n200\n\n\n\n\n\\(f(x)\\)\n1/2\n1/2\n\n\n\n\n이므로 모집단의 기대값은 $E(X) = xf(x) = 100 /2 + 200 /2 = 150 $\n\n\n### code_fold : true\n#install.packages(\"plotly\")\nlibrary(tidyverse)\nlibrary(plotly)\n\n\nx &lt;- tibble(income = c(100, 200))\n\ndata &lt;- tibble()\nfor (i in 1:500) \n    {\n        sample &lt;- x %&gt;% sample_n(i, replace = T, weight = c(1/2,1/2))\n        sample &lt;- sample %&gt;% mutate(sample_size = i)\n        data &lt;- bind_rows(data, sample)\n    }\n\n\n### code_fold : true\nsample_mean &lt;- data %&gt;% group_by(sample_size) %&gt;% \n                        summarize(sample_mean = mean(income))\n\nfig &lt;- plot_ly(data = sample_mean, \n               x = ~sample_size, y = ~sample_mean, mode = \"lines\", \n                type = \"scatter\", name = \"표본평균\", opacity = 0.5) %&gt;% \n                        add_trace(y = 150, name = \"모집단 평균\")\n\nfig %&gt;% \n    layout(title = \"Law of large numbers\") \n\n\n\n    \n        \n        \n\n\n\n\n\n\n\n\n    \n    \n        \n\n    \n\n\n\n\n\n분산\n- 수집한 표본들이 평균으로부터 떨어져 있는 정도를 측정\n1 이산형일 경우\n\nex1 : 동전 던지기를 해서 앞면이 나오는 실험의 분산은?\n\n\n\n\n\\(x\\)\n0\n1\n\n\n\n\n\\(f(x)\\)\n1/2\n1/2\n\n\n\n\\[\\sum_{i=0}^{n} = [x_i-E(X)]^2f(x_i)\\]\n2 연속형일 경우\n\\[Var(X) = \\int [x-E(x)]^2f(x) \\,\\,dx\\]",
    "crumbs": [
      "Posts",
      "PBA2024",
      "02. stat & ML basic"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-12-02. stat & ml basic.html#정규분포",
    "href": "posts/PBA2024/2024-05-12-02. stat & ml basic.html#정규분포",
    "title": "02. stat & ML basic",
    "section": "정규분포",
    "text": "정규분포\n1 분포 정의 \\(\\to X \\sim N(\\mu, \\sigma^2)\\)\n\\[ f(x) = \\frac{1}{\\sqrt(2\\pi\\sigma^2)}\\exp\\left( -\\frac12 \\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)\\]\n2 대게 종모양의 분포를 띔\n3 표준편차가 작을수록 곡선이 좀더 뾰족해지고 폭이 좁아진다? \\(\\to\\) 분산에 정의를 다시 한번 생각해보시면됩니다.\n\noptions(repr.plot.res=200, repr.plot.width = 8, repr.plot.height = 6)\nx1 &lt;- rnorm(10000, 0, 1)\nx2 &lt;- rnorm(10000, 0, 10)\nx3 &lt;- rnorm(10000, 0, 20)\nx4 &lt;- rnorm(10000, 0, 30)\n\npar(mfrow = c(2,2))\nhist(x1, main = \"평균 : 0, 표준편차 : 1\", breaks = 50, xlim = c(-100,100))\nhist(x2, main = \"평균 : 0, 표준편차 : 10\", breaks = 50, xlim = c(-100,100))\nhist(x3, main = \"평균 : 0, 표준편차 : 20\", breaks = 50, xlim = c(-100,100))\nhist(x4, main = \"평균 : 0, 표준편차 : 30\", breaks = 50, xlim = c(-100,100))\n\n\n\n\n\n\n\n\n4 정규분포의 기각영역\n\n우리가 수집한 어떤 집단의 평균이 0일 때의 가설검정\n\n\\[H_0 : \\mu = 0, \\quad H_1 : \\mu \\neq 0\\]\n\nx = rnorm(1000)\nnormal &lt;- tibble(x = x, fx = fx)\n\nnormal &lt;- normal %&gt;% mutate(p = ifelse(abs(x) &gt;= 2.58, \"유의수준 1%\", \n                             ifelse(abs(x) &gt;= 1.96, \"유의수준 5%\", \n                                    ifelse(abs(x) &gt;= 1.65, \"유의수준 10%\", \"기각 x\"))))\n\n\noptions(repr.plot.res = 200, repr.plot.width = 8, repr.plot.height = 4)\nnormal %&gt;% ggplot(`aes(x = x, fill = p)) +\n            geom_histogram(bins = 100)\n\n\n\n\n\n\n\n\n5 중심극한정리\n\n표본의 크기가 커지면, 표본평균의 분포는 정규분포로 수렴 \\(n \\to \\infty,\\, \\overline X_n \\sim N(\\mu, \\sigma^2/n)\\)\n이것을 가정하고 있기 때문에, 우리는 어떠한 가설설정 시 정규분포를 이용해서 통계적 유의성을 말할 수 있는거임\n\n\nexample1, 동전던지기를 통한 중심극한정리 실험\n\n\nstep1. 동전던지기 실험, 동전던지기 100회 \\(\\text{sample size} = 100\\)\n\n\nrbinom(100, 1, 0.5)\n\n\n0110110111111110011101100000100100011100101111101000000110111100000101000011100001011111010111000000\n\n\n\nstep2. 표본평균 구하기\n\n\nx_mean = mean(rbinom(100, 1, 0.5))\nx_mean\n\n0.6\n\n\n\nstep3. 표본평균의 분포 구하기\n\n\nsample_mean = c()\n\nfor (i in 1:1000) {\n    x_mean = mean(rbinom(1000, 1, 0.5))\n    sample_mean = append(sample_mean,x_mean)\n    }\n\n\nhist(sample_mean,main = \"n=1000\", breaks = 50)",
    "crumbs": [
      "Posts",
      "PBA2024",
      "02. stat & ML basic"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-12-02. stat & ml basic.html#ml-basic",
    "href": "posts/PBA2024/2024-05-12-02. stat & ml basic.html#ml-basic",
    "title": "02. stat & ML basic",
    "section": "ML basic",
    "text": "ML basic\n\n1. 선형회귀 평가지표\n- 결정계수 \\(R^2\\) : 우리가 적합시킨 모델이 얼마만큼의 설명력을 가지고 있는가\n\n지난시간 학습한 자료 꼭 한번 확인!\n\n\\[R^2 = \\frac{SSR}{SST} = \\frac{1-SSE}{SST}\\]\n\n\n2. Overfitting\n\n우리가 학습시킨 모델이, 훈련데이터에만 집중적으로 학습이되어서, 실제 검증용 데이터로 평가를 했을 때 모델 성능이 낮아지는 경우\n이거를 표현해보면…\n\n\\[\\begin {align} \\text {MSE} = E[(\\hat y - y)^2] &= Var(\\hat y) + \\text {bias}^2   \\\\\n                                                                &= E[(\\hat y - E(\\hat y ))^2] + E[(E(\\hat y) - y)^2] + Var(\\varepsilon) \\end {align}\\]\n- \\(Var(\\hat y) \\, \\to \\,\\) 훈련자료의 변화에 \\(\\hat y\\)가 얼마나 민감하게 반응하는가?(즉, 설명력을 의미한다.)\n- \\(\\text {bias}^2\\) : 실제자료를 모형으로 얼마나 가깝게 근사할 수 있는가? (만약, \\(E(\\hat y) = y\\) 일 경우 해당 추정량은 불편추정량이다.)\n\n\\(Var(\\hat y),\\,\\text {bias}^2\\) 는 reducible error로 어떤 모델을 선택하느냐에 따라 줄일 수 있는 오차이다.\n\n- \\(Var(\\varepsilon)\\) : 어떤 모델을 사용하건 줄일 수 없는 오차\n- Flexibility : 모델 복잡도 측도\n\nFlexibility가 높아지면 \\(Var(\\hat y)\\) 는 높아지고, \\(\\text {bias}\\)는 낮아진다. (trade-off)\n이를 바꿔말하면 더 유연한(복잡한) 모형은 더 큰 분산을 가지고 더 단순한 모형일수록 큰 편의를 가진다.\n훈련 데이터로 모델 적합시, bias를 낮추기 위해 Var를 높여 모델 복잡도를 지나치게 높이는 것은 좋은 선택이 아니다.\n아래의 예시는 모형의 복잡도가 너무 높으면 평가 데이터에 대한 MSE가 오히려 과대추정되는 “과적합 문제” 를 보여주고 있다.\n\n\n\n\n\nex1\n\n\n\n- 실제로 위의 원데이터는 일반적인 선형모형으로 적합해도 괜찮을 것 같다.\n- 왼쪽그림에서 지나치게 복잡하게 적합된 초록색 선을 살펴보자.\n\n훈련 MSE(회색선) 는 3개의 모델 중 가장 낮으나, 과적합 이슈로 평가 MSE(빨간색선) 가 과대추정된 것을 볼 수 있다.\n\n\n\nex2\n\n\n\n- 위 데이터는 ex1과 달리 일반적인 선형회귀모형으로 적합하면 안될것 같은 느낌이 든다.\n- 파란색선은 중간 정도로 적합된 복잡한 모형이고, 초록색선은 지나치게 적합된 모형이다.\n\n오른쪽 그림을 살펴보면 파란색선으로 적합된 모형의 Test MSE가 가장 낮음을 볼 수 있다.\n\n\n\n과적합 방지 1. 변수선택법\n- 변수선택법 : AIC, BIC, Mallows’ C_p 등등\n\n\n과적합 방지 2. Regularized Linear Regression\n- 정규화 선형회귀는 선형회귀 계수에 대한 제약 조건을 추가하여 모델이 과도하게 최적화되는 현상을 막는 방법!\n1 Ridge\n\n기존 OLS를 이용한 \\(\\beta_i\\) 추정할 떄 사용하는 정규방정식\n\n\\[RSS = \\sum_{i=1}^n(y_i - \\beta_0 -\\sum_{j=1}^p\\beta_jx_{ij})^2\\]\n\nRidge 추정량은 위와 매우 유사하나 shrinkage penalty를 추가로 고려하여 최소화한다.\n\n\\[\\sum_{i=1}^n(y_i - \\beta_0 -\\sum_{j=1}^p\\beta_jx_{ij})^2 + \\lambda \\sum_{i=1}^p \\beta_j^2\\]\n\\[\\lambda \\,:\\,tuning \\,\\,parameter\\]\n\n\\(\\lambda\\)가 0이되면 일반적인 선형회귀모형이되고, \\(\\lambda\\)가 커지면 정규화 정도가 커진다.(회귀계수들이 작아진다.) \\(\\to\\) bias는 증가 variance는 감소\n릿지회귀모형에서는 L2 penalty 를 사용하여 베타계수들이 0에 근사하도록 한다.\n\n2 Lasso\n\n라쏘는 릿지와 다르게 베타계수를 “0값으로 보낸다.”\n이것을 “L1 penalty” 라고 부른다.m\n\n\\[\\sum_{i=1}^n(y_i - \\beta_0 -\\sum_{j=1}^p\\beta_jx_{ij})^2 + \\lambda \\sum_{i=1}^p |\\beta_j|\\]\n\n\n실습\n\n#install.packages(\"mosaicData\")\n\n\nlibrary(tidyverse)\nlibrary(mosaicData)\nlibrary(glmnet) ## 리지 및 라쏘 회귀를 적합하기 위한 패키지\n\n1 데이터확인\n\nglimpse(RailTrail)\n\nRows: 90\nColumns: 11\n$ hightemp   &lt;int&gt; 83, 73, 74, 95, 44, 69, 66, 66, 80, 79, 78, 65, 41, 59, 50,~\n$ lowtemp    &lt;int&gt; 50, 49, 52, 61, 52, 54, 39, 38, 55, 45, 55, 48, 49, 35, 35,~\n$ avgtemp    &lt;dbl&gt; 66.5, 61.0, 63.0, 78.0, 48.0, 61.5, 52.5, 52.0, 67.5, 62.0,~\n$ spring     &lt;int&gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,~\n$ summer     &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,~\n$ fall       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,~\n$ cloudcover &lt;dbl&gt; 7.6, 6.3, 7.5, 2.6, 10.0, 6.6, 2.4, 0.0, 3.8, 4.1, 8.5, 7.2~\n$ precip     &lt;dbl&gt; 0.00, 0.29, 0.32, 0.00, 0.14, 0.02, 0.00, 0.00, 0.00, 0.00,~\n$ volume     &lt;int&gt; 501, 419, 397, 385, 200, 375, 417, 629, 533, 547, 432, 418,~\n$ weekday    &lt;lgl&gt; TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, TR~\n$ dayType    &lt;chr&gt; \"weekday\", \"weekday\", \"weekday\", \"weekend\", \"weekday\", \"wee~\n\n\n- model.matrix는 기본적으로 절편을 포함한 모형을 산정한다. 절편을 제외시킨 모형을 고려해보자\n\nx &lt;- model.matrix(volume~.-1 ,RailTrail) \ny &lt;- RailTrail$volume\n\n- 모형적합\n\nridge.fit &lt;- cv.glmnet(x,y,alpha=0) ## 모형적합 alpha=0 은 릿지를 말함\n\n\nbestlam &lt;- ridge.fit$lambda.min\nlog(bestlam)\n\n3.02271971457234\n\n\n- 각 변수들의 beta 계수구하기\n\ncoef(ridge.fit, s = \"lambda.min\")\n\n12 x 1 sparse Matrix of class \"dgCMatrix\"\n                        s1\n(Intercept)    111.4030636\nhightemp         3.6351375\nlowtemp         -0.9192012\navgtemp          1.9016337\nspring           9.6677301\nsummer           4.4014186\nfall           -27.7525473\ncloudcover      -8.1350693\nprecip         -88.4982099\nweekdayFALSE    13.8267092\nweekdayTRUE    -13.8900786\ndayTypeweekend  13.9990402\n\n\n- MSE가 최소가 되는 지점을 확인\n\noptions(repr.plot.res=200,repr.plot.height=4,repr.plot.width=10)\n\nplot(ridge.fit)\nabline(v=log(bestlam),lty=\"dashed\",lwd=2,col=\"blue\")\n\n\n\n\n\n\n\n\n- 산출된 best.lam값으로 모형을 다시 적합후 test_MSE를 구해보자\n\ntrain  &lt;- sample(1:nrow(x),nrow(x)*0.7)\ntest  &lt;- -train\ny.test &lt;- y[test]\ny.train &lt;- y[train]\n\n\nridge.fit &lt;- glmnet(x[train,], y.train, alpha=0, lambda=bestlam, family=\"gaussian\")\n\nridge.pred &lt;- predict(ridge.fit,s=bestlam,newx=x[test,]) \n\nridge.coef &lt;- predict(ridge.fit,s=bestlam,newx=x[test,],type=\"coefficients\") ## type=\"coefficients\"로 하면 예측된 베타계수를 보여줌\n\n- 여기선 MSE가 거의 1만에 가깝게 나왔는데 이는 지나치게 모형을 단순화한 것임 즉, best_lam값을 다시 찾는 과정을 반복해야한다….\n\nmean((ridge.pred - y.test)^2)\n\n8763.80834274373",
    "crumbs": [
      "Posts",
      "PBA2024",
      "02. stat & ML basic"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-26-04. advanced lm.html",
    "href": "posts/PBA2024/2024-05-26-04. advanced lm.html",
    "title": "04. advanced LM",
    "section": "",
    "text": "- 회귀분석은 설명변수 \\(\\boldsymbol{X} = [x_1,x_2, \\dots x_p]\\)를 이용하여 반응변수 \\(y\\)를 예측하기 위해 가장 널리 활용되는 통계적 모형이다.\n- 아래와 같은 형태의 함수를 찾는 것\n\\[y \\approx \\beta_1 x_1 + \\beta_0 \\]\n- OLS, MLE 기법들을 활용하여 \\(\\beta_1,\\beta_0\\)를 추정하는 방법을 학습함.\n\\[(\\hat {\\beta_0}, \\hat {\\beta_1}) = \\left (\\bar y - \\hat {\\beta}_1\\bar x. \\,\\, \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum (x-\\bar x)(y-\\bar y)}{\\sum (x-\\bar x)^2} = \\frac{\\sum (x-\\bar x)y}{S_{xx}}\\right )\\]\n- 오늘은 진도를 빼기 전에, 조금 더 이전에 학습한 것들을 자세히 학습하는 시간을 가질 것임",
    "crumbs": [
      "Posts",
      "PBA2024",
      "04. advanced LM"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-26-04. advanced lm.html#최소제곱추정량ols의-설징",
    "href": "posts/PBA2024/2024-05-26-04. advanced lm.html#최소제곱추정량ols의-설징",
    "title": "04. advanced LM",
    "section": "최소제곱추정량(OLS)의 설징",
    "text": "최소제곱추정량(OLS)의 설징\n- 용어정리\n\n\\(\\hat y_i\\) : \\(i\\)번째 관측치의 적합치(=fitted value)\n\\(\\beta_1\\) : 기울기(slope)\n\\(\\beta_0\\) : 절편(intercept)\n\\(\\epsilon_i = \\hat y_i - y_i\\) : \\(i\\)번째 관측치의 오차(=잔차)\n\n잔차는 후에 모형의 가정을 체크하는데 있어서 매우 중요한 역할을 한다.\n\n불편추정량 : 추정량의 기대값이 실제 모수와 같은 추정량\n\n\\[E(\\hat {\\beta_1}) = \\beta_1\\]\n\\(\\to\\) proof\n1 $(x-x) = 0 $\n2 \\(\\sum(x-\\bar x)^2 = \\sum(x-\\bar x)x = S_{xx}\\)\n3 \\(E(y) = \\beta_0 + \\beta_1x\\)\n\\[\\begin{align*} E(\\hat {\\beta_1}) &= \\frac{\\sum(x-\\bar x)}{S_{xx}}E(y)  \\\\ \\\\\n                                   &= \\frac{\\sum(x-\\bar x)(\\beta_0+\\beta_1x_1)}{S_xx} \\\\ \\\\\n                                   &= \\frac{\\sum(x-\\bar x)\\beta_0 + \\beta_1\\sum(x-\\bar x)x}{S_{xx}} \\\\ \\\\\n                                   &= \\frac{0 + \\beta_1S_{xx}}{S_{xx}} = \\beta_1\n                                    \\end{align*}\\]\n따라서 OLS를 이용하여 추정한 \\(\\hat {\\beta_1}\\)은 불편추정량이다!\n\\[\\begin {align*} E(\\hat {\\beta_0}) &=  E(\\bar y - \\hat \\beta_1\\bar x) \\\\ \\\\\n                                    &= \\bar y  - \\beta_1\\bar x \\\\ \\\\\n                                    &= (\\beta_1 x_1 + \\beta_0) - \\beta_1 x_1 = \\beta_0 \\end{align*}\\]\n\n이 불편추정량의 성질로 인해 이런 말로도 쓴다.\n\n회귀분석은 주어진 관측치를 이용하여 어떤 현상에 대한 평균반응량을 구하는 것과 같다.\n\n\n\n\n회귀계수의 분산\n1 \\(\\hat {\\beta_1}\\)의 분산\n\n\\(\\hat {\\beta_1} = \\sum\\frac{(x-\\bar x)y}{S_{xx}}, Var(y) = Var(\\epsilon) = \\sigma^2\\)\n\n\\[\\begin{align*} Var(\\hat {\\beta}_1) &= Var(\\sum\\frac{(x-\\bar x)y}{S_{xx}}) \\\\ \\\\\n                                     &= \\sum \\frac {(x-\\bar x)^2}{S_{xx}^2} Var(y) \\\\ \\\\\n                                     &=  \\frac{\\sigma^2}{S_{xx}}\\end {align*}\\]\n2 \\(\\hat \\beta_{0}\\)의 분산\n\n$_0 = y - _1 x $\n\n\\[\\begin{align*} Var(\\hat {\\beta}_0) &= Var(\\bar y) - (\\bar x)^2Var(\\hat {\\beta}_1) - 2\\bar x Cov(\\bar y, \\hat {\\hat {\\beta}_1}) \\\\ \\\\\n                                     &= \\frac {\\sigma^2}{n} -  (\\bar x)^2 \\frac{\\sigma^2}{S_{xx}}  + 0 \\\\ \\\\\n                                     &= \\sigma^2 \\left ( \\frac 1n + \\frac {(\\bar x)^2}{S_{xx}}\\right)\n                            \\end{align*}\\]\n\n\n몇 가지 이론적인 성질(참고)\n1 \\(\\sum \\epsilon_i = \\sum \\epsilon_i x_I = 0\\)\n2 \\(\\sum y_i = \\sum \\hat {y}_i\\)\n3 \\(\\sum \\hat {y}_i \\epsilon_i = 0\\)",
    "crumbs": [
      "Posts",
      "PBA2024",
      "04. advanced LM"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-05-26-04. advanced lm.html#오차항의-분산-추정",
    "href": "posts/PBA2024/2024-05-26-04. advanced lm.html#오차항의-분산-추정",
    "title": "04. advanced LM",
    "section": "오차항의 분산 추정",
    "text": "오차항의 분산 추정\n\\[Var(y) = Var(\\epsilon) = \\sigma^2\\]\n1 오차항은 회귀계수의 추정에서는 중요하지 않으나, 우리가 추정한 회기계수의 신뢰구간 구성, 가설 검정에서 필요함\n2 오차항의 추정치를 다음과 같이 정의한다.\n\\[\\hat {\\sigma}^2 = \\frac{\\sum{(y - \\hat y)^2}}{n-2} = \\frac{SSE}{n-2} = MSE\\]\n\n또한, 이렇게도 정의할 수 있음\n\n\\[MSE =  Var(\\hat y) + \\text{bias}^2 = Var(\\hat y) + (E(\\hat y) - y)^2 \\]\n3 n-2를 이용하여 오차제곱합의 평균인 MSE를 산출하는 이유?\n\n카이제곱분포의 가법성의 성질을 알아햐 하나 넘어간다.\n이렇게 생각하자. 만약에 어떤 모집단의 평균을 우리가 알고 있다고 하자. 그러면 우리는 n-1개의 정보만 있으면 모평균의 추정치를 구할 수 있다.\n\n\\(\\to\\) 예제 : 우리는 저기 비어있는 관측치가 무엇인지 몰라도 모평균을 알고 있기 때문에 3개의 정보만으로도 모평균을 짐작할 수 있음\n\\(E(X) = 2.5\\)\nx = [1,2,?,4]\n\n자, 다시 우리는 앞에서 이론적으로 \\(E(\\hat {\\beta}_0), E(\\hat {\\beta}_1)\\)이 불편추정량임을 증명하였음\n그러면, 우리는 어떤 추정량의 정보 2개를 이미 알고 있으니까 n-2개의 정보만 있으면 된다.\n정리 : SSE의 자유도는 n-(예측변수 개수 + 1)이다!\n\n4 회귀모형의 유의성 검정\n\\[H_0 : \\beta_1 = 0, \\quad H_1 = \\beta_1 \\neq 0  \\]\n\n검정통계량\n\n\\[t = \\frac{\\hat \\beta_1}{se(\\hat {\\beta_1})}\\]\n\n위 통계량의 절대값이 자유도 n-2의 t분포 분위수보다 크면 귀무가설을 기각, 즉 회귀모형이 유의하다고 할 수 있다.\n\n\\[|t| &gt; t_{\\alpha/2., n-2}\\]\n5 MSE는 모형의 잔차로부터 추정되므로 모형에 깊게 의존한다. 즉, 모형이 잘못 설정된 경우 유용성이 심각하게 저하됨\n6 또한, MSE는 불편추정량이다.(카이제곱 통계량을 알면 구하기 쉬움) : 교재에서 그냥 스쳐지나갔으니 넘어가겠습니다.(숙제?)\n\n\\(E(\\hat {\\sigma}^2) = \\sigma^2\\)",
    "crumbs": [
      "Posts",
      "PBA2024",
      "04. advanced LM"
    ]
  }
]