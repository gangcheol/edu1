[
  {
    "objectID": "posts/PBA2024/2024-04-10-03. MLE.html",
    "href": "posts/PBA2024/2024-04-10-03. MLE.html",
    "title": "03. MLE",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\\[L(\\theta)=f(\\mathbf{x}  | \\theta) =  f(x_1|\\theta)f(x_2|\\theta)\\dots f(x_n|\\theta)\\]\n- 해석 1(일반적인 정의 : 블로그, 위키피디아, 기타 포럼 등등)\n\nLikelihood function\n우도함수는 일반적으로 설명력 함수라고 알려져 있다.\n수식을 보았을 때, 주어진 랜덤샘플 \\(x_1,x_2 \\dots x_n\\)에 대한 결합확률밀도함수(그냥 다 곱한거)를 나타내고 있다.\n즉, 이는 모수(\\(p, \\theta, \\mu, \\sigma...\\))가 주어졌을 때 주어진 데이터(샘플)가 어떤 분포(or 가정, 현상)를 얼만큼 설명하는지 직관적인 형태를 수식으로 나타냈다고 이해하자. (\\(\\star\\star\\star\\))\n\n- 해석 2 (조금 더.. 전문적인 지식이 들어간 해석)\n\n주어진 모수 (\\(p, \\theta, \\mu, \\sigma...\\))가 주어졌을 때 \\(\\bf {x}\\)를 관측할 확률(or 확률밀도)를 나타낸다.(\\(\\star\\star\\star\\))\n즉, 특정 모수가 주어졌을 때 어떤 샘플들이 얻어질 확률이지 모수(확률, 평균)가 어떤 상수 값 (ex \\(p = 0.2\\))과 일치할 확률을 의미하지 않는다.(\\(\\star\\star\\star\\))\n\n- 해석 2에 대한 좀더 직관적인 접근으로 아래와 같은 실험에서 우도함수를 최대화하는 \\(\\hat{p}_{mle}\\) 구해보자.\n\n동전 던지기 실험 \\(\\to X_i \\overset{iid}\\sim Ber(p), \\quad f(x) = p^{x}(1-p)^{1-x}, \\quad  x \\in [0,1]\\)\n\n- 일반적으로 우리가 알고 있는 사실 \\(\\to p = 1/2, \\,\\, f(x) = 1/2\\)\n\nx = [0,1,0,1]\nx\n\n[0, 1, 0, 1]\n\n\n- 우도함수 계산\n\n(1/2)**4\n\n0.0625\n\n\n- 만약 \\(p = 1/3\\)이라면? \\(\\to f(x) = (1/3)^{x}(2/3)^{1-x}\\)\n- 우도함수 계산\n\n(2/3)*(1/3)*(2/3)*(1/3)\n\n0.04938271604938271\n\n\n- 음? \\(p=1/2\\)일 때가, 더 우도함수 값이 크다.\n\n해석 1의 관점 : 오, \\(p=1/2\\)일 때, 설명력이 더 좋네?\n해석 2의 관점 : 오, \\(p=1/2\\)일 때, 샘플 (0,1,0,1)이 얻어질 확률이 더 크네?\n우리는 앞으로 해석 1의 관점을 일차적으로 말하고, 그 다음에 디테일하게 해석 2를 말할 줄 알아야 한다!\n\n\n\n\n1 가능도함수 = 우도함수 = likelihood function\n2 최대가능도함수 = 최대우도함수 = Maximum likelihood function = 모수 \\(p\\)의 MLE\n\n\n\n(조건은 생략할게여…)\nstep 1. 우도함수 구하기\n\\[L(\\theta)=f(\\mathbf{x}  | \\theta) =  f(x_1|\\theta)f(x_2|\\theta)\\dots f(x_n|\\theta)\\]\nstep 2. \\(\\log L(\\theta) = l(\\theta)\\) 미분\n\\[\\hat {\\theta}_{mle} = \\frac {d}{d\\,\\theta} \\log L(\\theta) = \\frac {d}{d\\,\\theta} l(\\theta)\\]\nwhy? 미분하여 최대값?\n- 중학교 수학 : 로그함수는 일반적으로 증가함수이며, 증가함수는 미분하여 0이되는 지점에서 최대값을 갖는다.\n- 다시 예제\n\n동전 던저기 실험 \\(\\to X_i \\overset{iid}\\sim Ber(p), \\quad f(x) = p^{x}(1-p)^{1-x}, \\quad  x \\in [0,1]\\)\n\n\n일반적으로 알려진 사실 \\(p = 1/2\\)\n\\(L(p) = p^{\\sum x}(1-p)^{n-\\sum x}\\)\n\\(l(p) = \\sum x \\log p + (n-\\sum x)\\log (1-p)\\)\n\n\\[\\begin{align*} \\frac{d}{d\\,p} l(p) &= \\frac{\\sum x}{p} - \\frac{n-\\sum x}{1-p} \\\\ \\\\\n                    &= (1-p)\\sum x - p(n-\\sum x)  \\\\ \\\\\n                    &= \\sum x  - pn = 0\\end{align*}\\]\n\\[\\divideontimes \\,\\,\\hat {p}_{mle} = \\frac {\\sum x}{ n} \\]\n\n확인\n\n\nx = [0,1,0,1]\n\nsum(x)/4\n\n0.5\n\n\n\n\n\n\n\n\\[y = \\beta_0 + \\beta_1x + \\varepsilon,\\quad \\varepsilon \\overset{iid}\\sim N(0,\\sigma^2)\\]\n1 복습. OLS(정규방정식)\n\n아래의 식을 만족하는 \\(\\beta_0, \\beta_1\\)은 OLS, 즉 최소제곱법을 이용하여 구한 최적의 해다.\n\n\\[\\sum ({y_i - \\beta_0 -\\beta_1x})^2 = 0\\]\n2 MLE\n\n오차항은 정규성을 가정하였으므로. 다음과 같이 쓸 수 있다.\n\n\\[f(\\varepsilon) = \\frac {1}{\\sqrt {2\\pi}}e^{-\\frac{1}{2}\\varepsilon^2}\\]\n\n가능도 함수 해석\n\n\\(\\to\\) 1. 설명력 함수를 생각하면 오차항에 대한 우도함수를 구하고, 그것을 미분하여 우도함수를 최대화 시키는 \\(\\beta_0, \\beta_1\\)을 구하면 되지 않을까?\n\\(\\to\\) 우도함수 구하기\n\\[l(\\beta_0,\\beta_1) = -\\frac{\\sum \\varepsilon^2}{2\\sqrt{2\\pi}} = -\\frac{\\sum(y -  \\beta_0- \\beta_1x)^2}{2\\sqrt{2\\pi}} \\]\n\\(\\to\\) 우도함수 미분\n\\[\\frac {d}{d (\\beta_0, \\beta_1)}l(\\beta_0,\\beta_1)  = -\\frac {d}{d (\\beta_0, \\beta_1)}\\frac{\\sum(y -  \\beta_0- \\beta_1x)^2}{2\\sqrt{2\\pi}} = 0\\text{ 을 만족하는} (\\beta_1, \\beta_0) \\]\n\\(\\to\\) 차피 상수항은 넘어가니까…\n\\[\\frac {d}{d (\\beta_0, \\beta_1)}l(\\beta_0,\\beta_1)  = \\frac {d}{d (\\beta_0, \\beta_1)}\\sum(y -  \\beta_0- \\beta_1x)^2 = 0\\text{ 을 만족하는} (\\beta_1, \\beta_0) \\]\n\\(\\to\\) 어? 근데 이거 OLS랑 똑같네??\n\nOLS : 아래의 식을 만족하는 \\(\\beta_0, \\beta_1\\)은 OLS, 즉 최소제곱법을 이용하여 구한 최적의 해다.\n\n\\[\\sum ({y_i - \\beta_0 -\\beta_1x})^2 = 0\\]\n\n아하! 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 \\(\\beta_0, \\beta_1\\)을 구하면 된다!\n\n\n\n\n1 잠깐 위에 동전던저기 예제\n\n\\(L(p) = p^{\\sum x}(1-p)^{n-\\sum x}\\)\n\\(l(p) = \\sum x \\log p + (n-\\sum x)\\log (1-p)\\)\n\n\\[\\begin{align*} \\frac{d}{d\\,p} l(p) &= \\frac{\\sum x}{p} - \\frac{n-\\sum x}{1-p} \\\\ \\\\\n                    &= (1-p)\\sum x - p(n-\\sum x)  \\\\ \\\\\n                    &= \\sum x  - pn = 0\\end{align*}\\]\n\\[\\divideontimes \\,\\,\\hat {p}_{mle} = \\frac {\\sum x}{ n} \\]\n2 로지스틱 복습\n\n지난 시간 로지스틱은 어떤 범주의 속할 확률(동전이 앞면이 나올지, 내가 여자인지 남자인지)을 모형화하는 것이라고 했다…\n확률을 모형화…모형화..어?? 위에 하고 똑같자나…\n\n\n동전 던지기 실험 \\(\\to Y_i \\overset{iid}\\sim Ber(p), \\quad f(y) = p^{y}(1-p)^{1-y}, \\quad  y \\in [0,1]\\)\n\n\n단순히 \\(p\\)가 \\(\\beta_0 + \\beta_1x\\)로 바뀐거죠?\n다시 쓰면?\n\n\n동전 던지기 실험 \\(\\to Y_i \\overset{iid}\\sim Ber(p), \\quad f(y) = (\\beta_0+\\beta_1x)^{y}(1-p)^{1-y}, \\quad  y \\in [0,1]\\)\n\n\n이거에 대한 우도함수는?\n\n\\(\\to\\) \\(L(\\beta_0, \\beta_1) = (\\beta_0+\\beta_1x)^{\\sum y}[1-(\\beta_0+\\beta_1x)]^{n-\\sum y}\\)\n\n이거를 교재 4장, 17페이지에서 아래와 같이 좀 문과생? 들이 보기 어려운 수식을 써서 표현한거에요…\n\n\\[\\prod_{i=1}^{n}\\left (\\beta_0 + \\beta_1x_1^{y_i} \\right )\\left (1-(\\beta_0 + \\beta_1x_1)^{y_i} \\right )\\]\n\\(\\to\\) 다시, \\(l(\\beta_0\\,\\beta_1)\\) 구하기. 이제 그냥 편의상 \\(p\\)라고 쓸게요.\n\\[\\begin{align*} l(p) &= \\sum y \\log p - (n-\\sum y)\\log (1-p)  \\\\ \\\\\n                      &= \\sum y \\log p - (1-y)\\log (1-p)  \\\\ \\\\\n                      &= \\sum y \\log (\\beta_0 + \\beta_1x) - (1-y)\\log (1-(\\beta_0 + \\beta_1x)) \\end{align*}\\]\n\\(\\to \\sum y \\log (\\beta_0 + \\beta_1x) - (1-y)\\log \\left (1-(\\beta_0 + \\beta_1x)\\right )\\) 와 같은 형태를 BCEloss 라고합니다.(뒤에서 배울거에요.)\n\\(\\to\\) 즉, 분류범주가 2개인 경우 BCEloss를 이용하며 모형을 학습해 최적의 모형을 산출하는데, 그게 결국 MLE를 통해 최적의 모수를 구하는 것과 같다!\n\n\n\n\n\n1 우도함수는 설명력 함수이다.\n2 우도함수는 주어진 모수 (\\(p, \\theta, \\mu, \\sigma...\\))가 주어졌을 때 \\(\\bf {x}\\)를 관측할 확률(or 확률밀도)를 나타낸다.\n3 어떤 분포, 모형에 관한 최적의 MLE를 구하기 위해서는 우도함수의 로그를 취하고 그것을 미분하여 0을 만족하는 경우를 MLE라고 한다.\n4 회귀모형에서 최적의 회귀계수(\\(\\beta_0,\\beta_1\\))을 구하는 방법은 MLE를 이용하여 구하는 방법과 동일하다.\n\n\n\n1 예측 성능 지표\n2 고급회귀분석\n3 모수 모델 vs 비모수 모델\n4 cost function(손실함수, 비용함수)",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-10-03. MLE.html#우도함수",
    "href": "posts/PBA2024/2024-04-10-03. MLE.html#우도함수",
    "title": "03. MLE",
    "section": "",
    "text": "\\[L(\\theta)=f(\\mathbf{x}  | \\theta) =  f(x_1|\\theta)f(x_2|\\theta)\\dots f(x_n|\\theta)\\]\n- 해석 1(일반적인 정의 : 블로그, 위키피디아, 기타 포럼 등등)\n\nLikelihood function\n우도함수는 일반적으로 설명력 함수라고 알려져 있다.\n수식을 보았을 때, 주어진 랜덤샘플 \\(x_1,x_2 \\dots x_n\\)에 대한 결합확률밀도함수(그냥 다 곱한거)를 나타내고 있다.\n즉, 이는 모수(\\(p, \\theta, \\mu, \\sigma...\\))가 주어졌을 때 주어진 데이터(샘플)가 어떤 분포(or 가정, 현상)를 얼만큼 설명하는지 직관적인 형태를 수식으로 나타냈다고 이해하자. (\\(\\star\\star\\star\\))\n\n- 해석 2 (조금 더.. 전문적인 지식이 들어간 해석)\n\n주어진 모수 (\\(p, \\theta, \\mu, \\sigma...\\))가 주어졌을 때 \\(\\bf {x}\\)를 관측할 확률(or 확률밀도)를 나타낸다.(\\(\\star\\star\\star\\))\n즉, 특정 모수가 주어졌을 때 어떤 샘플들이 얻어질 확률이지 모수(확률, 평균)가 어떤 상수 값 (ex \\(p = 0.2\\))과 일치할 확률을 의미하지 않는다.(\\(\\star\\star\\star\\))\n\n- 해석 2에 대한 좀더 직관적인 접근으로 아래와 같은 실험에서 우도함수를 최대화하는 \\(\\hat{p}_{mle}\\) 구해보자.\n\n동전 던지기 실험 \\(\\to X_i \\overset{iid}\\sim Ber(p), \\quad f(x) = p^{x}(1-p)^{1-x}, \\quad  x \\in [0,1]\\)\n\n- 일반적으로 우리가 알고 있는 사실 \\(\\to p = 1/2, \\,\\, f(x) = 1/2\\)\n\nx = [0,1,0,1]\nx\n\n[0, 1, 0, 1]\n\n\n- 우도함수 계산\n\n(1/2)**4\n\n0.0625\n\n\n- 만약 \\(p = 1/3\\)이라면? \\(\\to f(x) = (1/3)^{x}(2/3)^{1-x}\\)\n- 우도함수 계산\n\n(2/3)*(1/3)*(2/3)*(1/3)\n\n0.04938271604938271\n\n\n- 음? \\(p=1/2\\)일 때가, 더 우도함수 값이 크다.\n\n해석 1의 관점 : 오, \\(p=1/2\\)일 때, 설명력이 더 좋네?\n해석 2의 관점 : 오, \\(p=1/2\\)일 때, 샘플 (0,1,0,1)이 얻어질 확률이 더 크네?\n우리는 앞으로 해석 1의 관점을 일차적으로 말하고, 그 다음에 디테일하게 해석 2를 말할 줄 알아야 한다!",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-10-03. MLE.html#용어정리",
    "href": "posts/PBA2024/2024-04-10-03. MLE.html#용어정리",
    "title": "03. MLE",
    "section": "",
    "text": "1 가능도함수 = 우도함수 = likelihood function\n2 최대가능도함수 = 최대우도함수 = Maximum likelihood function = 모수 \\(p\\)의 MLE",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-10-03. MLE.html#mle를-구하는-방법",
    "href": "posts/PBA2024/2024-04-10-03. MLE.html#mle를-구하는-방법",
    "title": "03. MLE",
    "section": "",
    "text": "(조건은 생략할게여…)\nstep 1. 우도함수 구하기\n\\[L(\\theta)=f(\\mathbf{x}  | \\theta) =  f(x_1|\\theta)f(x_2|\\theta)\\dots f(x_n|\\theta)\\]\nstep 2. \\(\\log L(\\theta) = l(\\theta)\\) 미분\n\\[\\hat {\\theta}_{mle} = \\frac {d}{d\\,\\theta} \\log L(\\theta) = \\frac {d}{d\\,\\theta} l(\\theta)\\]\nwhy? 미분하여 최대값?\n- 중학교 수학 : 로그함수는 일반적으로 증가함수이며, 증가함수는 미분하여 0이되는 지점에서 최대값을 갖는다.\n- 다시 예제\n\n동전 던저기 실험 \\(\\to X_i \\overset{iid}\\sim Ber(p), \\quad f(x) = p^{x}(1-p)^{1-x}, \\quad  x \\in [0,1]\\)\n\n\n일반적으로 알려진 사실 \\(p = 1/2\\)\n\\(L(p) = p^{\\sum x}(1-p)^{n-\\sum x}\\)\n\\(l(p) = \\sum x \\log p + (n-\\sum x)\\log (1-p)\\)\n\n\\[\\begin{align*} \\frac{d}{d\\,p} l(p) &= \\frac{\\sum x}{p} - \\frac{n-\\sum x}{1-p} \\\\ \\\\\n                    &= (1-p)\\sum x - p(n-\\sum x)  \\\\ \\\\\n                    &= \\sum x  - pn = 0\\end{align*}\\]\n\\[\\divideontimes \\,\\,\\hat {p}_{mle} = \\frac {\\sum x}{ n} \\]\n\n확인\n\n\nx = [0,1,0,1]\n\nsum(x)/4\n\n0.5",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-10-03. MLE.html#회귀모형에서의-mle",
    "href": "posts/PBA2024/2024-04-10-03. MLE.html#회귀모형에서의-mle",
    "title": "03. MLE",
    "section": "",
    "text": "\\[y = \\beta_0 + \\beta_1x + \\varepsilon,\\quad \\varepsilon \\overset{iid}\\sim N(0,\\sigma^2)\\]\n1 복습. OLS(정규방정식)\n\n아래의 식을 만족하는 \\(\\beta_0, \\beta_1\\)은 OLS, 즉 최소제곱법을 이용하여 구한 최적의 해다.\n\n\\[\\sum ({y_i - \\beta_0 -\\beta_1x})^2 = 0\\]\n2 MLE\n\n오차항은 정규성을 가정하였으므로. 다음과 같이 쓸 수 있다.\n\n\\[f(\\varepsilon) = \\frac {1}{\\sqrt {2\\pi}}e^{-\\frac{1}{2}\\varepsilon^2}\\]\n\n가능도 함수 해석\n\n\\(\\to\\) 1. 설명력 함수를 생각하면 오차항에 대한 우도함수를 구하고, 그것을 미분하여 우도함수를 최대화 시키는 \\(\\beta_0, \\beta_1\\)을 구하면 되지 않을까?\n\\(\\to\\) 우도함수 구하기\n\\[l(\\beta_0,\\beta_1) = -\\frac{\\sum \\varepsilon^2}{2\\sqrt{2\\pi}} = -\\frac{\\sum(y -  \\beta_0- \\beta_1x)^2}{2\\sqrt{2\\pi}} \\]\n\\(\\to\\) 우도함수 미분\n\\[\\frac {d}{d (\\beta_0, \\beta_1)}l(\\beta_0,\\beta_1)  = -\\frac {d}{d (\\beta_0, \\beta_1)}\\frac{\\sum(y -  \\beta_0- \\beta_1x)^2}{2\\sqrt{2\\pi}} = 0\\text{ 을 만족하는} (\\beta_1, \\beta_0) \\]\n\\(\\to\\) 차피 상수항은 넘어가니까…\n\\[\\frac {d}{d (\\beta_0, \\beta_1)}l(\\beta_0,\\beta_1)  = \\frac {d}{d (\\beta_0, \\beta_1)}\\sum(y -  \\beta_0- \\beta_1x)^2 = 0\\text{ 을 만족하는} (\\beta_1, \\beta_0) \\]\n\\(\\to\\) 어? 근데 이거 OLS랑 똑같네??\n\nOLS : 아래의 식을 만족하는 \\(\\beta_0, \\beta_1\\)은 OLS, 즉 최소제곱법을 이용하여 구한 최적의 해다.\n\n\\[\\sum ({y_i - \\beta_0 -\\beta_1x})^2 = 0\\]\n\n아하! 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 \\(\\beta_0, \\beta_1\\)을 구하면 된다!\n\n\n\n\n1 잠깐 위에 동전던저기 예제\n\n\\(L(p) = p^{\\sum x}(1-p)^{n-\\sum x}\\)\n\\(l(p) = \\sum x \\log p + (n-\\sum x)\\log (1-p)\\)\n\n\\[\\begin{align*} \\frac{d}{d\\,p} l(p) &= \\frac{\\sum x}{p} - \\frac{n-\\sum x}{1-p} \\\\ \\\\\n                    &= (1-p)\\sum x - p(n-\\sum x)  \\\\ \\\\\n                    &= \\sum x  - pn = 0\\end{align*}\\]\n\\[\\divideontimes \\,\\,\\hat {p}_{mle} = \\frac {\\sum x}{ n} \\]\n2 로지스틱 복습\n\n지난 시간 로지스틱은 어떤 범주의 속할 확률(동전이 앞면이 나올지, 내가 여자인지 남자인지)을 모형화하는 것이라고 했다…\n확률을 모형화…모형화..어?? 위에 하고 똑같자나…\n\n\n동전 던지기 실험 \\(\\to Y_i \\overset{iid}\\sim Ber(p), \\quad f(y) = p^{y}(1-p)^{1-y}, \\quad  y \\in [0,1]\\)\n\n\n단순히 \\(p\\)가 \\(\\beta_0 + \\beta_1x\\)로 바뀐거죠?\n다시 쓰면?\n\n\n동전 던지기 실험 \\(\\to Y_i \\overset{iid}\\sim Ber(p), \\quad f(y) = (\\beta_0+\\beta_1x)^{y}(1-p)^{1-y}, \\quad  y \\in [0,1]\\)\n\n\n이거에 대한 우도함수는?\n\n\\(\\to\\) \\(L(\\beta_0, \\beta_1) = (\\beta_0+\\beta_1x)^{\\sum y}[1-(\\beta_0+\\beta_1x)]^{n-\\sum y}\\)\n\n이거를 교재 4장, 17페이지에서 아래와 같이 좀 문과생? 들이 보기 어려운 수식을 써서 표현한거에요…\n\n\\[\\prod_{i=1}^{n}\\left (\\beta_0 + \\beta_1x_1^{y_i} \\right )\\left (1-(\\beta_0 + \\beta_1x_1)^{y_i} \\right )\\]\n\\(\\to\\) 다시, \\(l(\\beta_0\\,\\beta_1)\\) 구하기. 이제 그냥 편의상 \\(p\\)라고 쓸게요.\n\\[\\begin{align*} l(p) &= \\sum y \\log p - (n-\\sum y)\\log (1-p)  \\\\ \\\\\n                      &= \\sum y \\log p - (1-y)\\log (1-p)  \\\\ \\\\\n                      &= \\sum y \\log (\\beta_0 + \\beta_1x) - (1-y)\\log (1-(\\beta_0 + \\beta_1x)) \\end{align*}\\]\n\\(\\to \\sum y \\log (\\beta_0 + \\beta_1x) - (1-y)\\log \\left (1-(\\beta_0 + \\beta_1x)\\right )\\) 와 같은 형태를 BCEloss 라고합니다.(뒤에서 배울거에요.)\n\\(\\to\\) 즉, 분류범주가 2개인 경우 BCEloss를 이용하며 모형을 학습해 최적의 모형을 산출하는데, 그게 결국 MLE를 통해 최적의 모수를 구하는 것과 같다!",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-10-03. MLE.html#summary",
    "href": "posts/PBA2024/2024-04-10-03. MLE.html#summary",
    "title": "03. MLE",
    "section": "",
    "text": "1 우도함수는 설명력 함수이다.\n2 우도함수는 주어진 모수 (\\(p, \\theta, \\mu, \\sigma...\\))가 주어졌을 때 \\(\\bf {x}\\)를 관측할 확률(or 확률밀도)를 나타낸다.\n3 어떤 분포, 모형에 관한 최적의 MLE를 구하기 위해서는 우도함수의 로그를 취하고 그것을 미분하여 0을 만족하는 경우를 MLE라고 한다.\n4 회귀모형에서 최적의 회귀계수(\\(\\beta_0,\\beta_1\\))을 구하는 방법은 MLE를 이용하여 구하는 방법과 동일하다.",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-10-03. MLE.html#to-do",
    "href": "posts/PBA2024/2024-04-10-03. MLE.html#to-do",
    "title": "03. MLE",
    "section": "",
    "text": "1 예측 성능 지표\n2 고급회귀분석\n3 모수 모델 vs 비모수 모델\n4 cost function(손실함수, 비용함수)",
    "crumbs": [
      "Posts",
      "PBA2024",
      "03. MLE"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-03-28-01. intro.html",
    "href": "posts/PBA2024/2024-03-28-01. intro.html",
    "title": "01. intro",
    "section": "",
    "text": "- 음…\n\n모집단, 표본, 가설 검정, 기계학습… 중심극한정리???\n그래서 이런 것들에 대해 간략하게 설명 후, 방향성 선택!\n일단, 해당 수업의 목적을 제가 잘 모르겟어서… R, python을 활용하여 통계분석을 한다는 건지, 아니면 예측모형을 주로 공부한다는 건지…(두 가지 다인가…???)",
    "crumbs": [
      "Posts",
      "PBA2024",
      "01. intro"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-03-28-01. intro.html#supervised-learning",
    "href": "posts/PBA2024/2024-03-28-01. intro.html#supervised-learning",
    "title": "01. intro",
    "section": "Supervised Learning",
    "text": "Supervised Learning\n1 설명변수 \\(X = (x_1,x_2,\\dots x_p)\\)로 부터 target변수인 \\(Y\\)를 예측하는 것!\n\\[y \\approx \\beta_1x + \\beta_0\\]\n2 가장 중요한 개념!\n\\[\\text{총 제곱합} = \\text{회귀제곱합} + \\text{잔차제곱합}\\]\n\\[\\sum{(y-\\bar y)}^2 = \\sum{(\\hat y - \\bar y)}^2 + \\sum{( y - \\hat y)}^2 \\]\n\\[\\text{SST} = \\text{SSR} + \\text{SSE}\\]\n3 일반적으로 \\(\\text{SSR}\\) 회귀식으로 설명할 수 있는 변동이라고 하며, 회귀분석 시 가장 관심있는 부분이다.\n4 총 제곱합 \\(\\text{SST}\\)는 변하지 않는 값이므로, SSR이 커지면 SSE즉, 예측값과 실제값의 차이는 줄어들게 된다.\n5 이는, 주어진 데이터로부터 우리가 적합한 회귀직선이 어떤 현상을 잘 설명하고 있다고 볼 수 있다.\n6 그러면, 회귀모형 적합 후, 우리는 무엇을 검토해야 하나?\n\n모형 내의 개별 회귀계수에 대한 검정(t통계량, p-value확인)\n\n\\[H_0 : \\beta_1 =0 \\quad\\text{ vs } \\quad H_1  : \\text{not } H_0\\]\n\n모형에 설명력인 결정계수 값 \\(R^2\\)값 확인, 설명변수의 수가 많아질 경우 \\(adj-R^2\\)값을 확인\n\n\\[R^2 = \\frac{SSR}{SST} = \\frac{1-SSE}{SST}\\]\n\\[R^2_{adj} = \\frac{SSR\\,/\\,(n-(p+1))}{SST\\,/\\,(n-1)} \\]\n\n회귀모형이 통계적을 유의한가?(F-통계량, p-value확인)\n\n\\[H_0 : \\beta_1 = \\beta_2 = \\beta_3 = \\dots 0 \\quad\\text{ vs } \\quad H_1  : \\text{not } H_0\\]\n\n잔차 plot을 통한 모형 진단\n\n\nex1. cars(Simple Linear regression)\n\nlibrary(tidyverse)\n\n-- Attaching core tidyverse packages ---------------------------------------------------------------- tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.2     v tibble    3.2.1\nv lubridate 1.9.3     v tidyr     1.3.1\nv purrr     1.0.2     \n-- Conflicts ---------------------------------------------------------------------------------- tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nhead(cars)\n\n\n\nA data.frame: 6 × 2\n\n\n\nspeed\ndist\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n\n\n\n\n1\n4\n2\n\n\n2\n4\n10\n\n\n3\n7\n4\n\n\n4\n7\n22\n\n\n5\n8\n16\n\n\n6\n9\n10\n\n\n\n\n\n\n- 우리는 주어진 데이터로부터 아래왜 같은 현상을 예측하고 싶음\n\\[\\text{dist} = \\beta_1\\times\\text{speed} + \\beta_0\\]\n- 즉, 속력에 따른 거리를 예측하기 위한 \\(\\beta_1, \\beta_0\\)를 추정! \\(\\to\\) \\((\\hat{\\beta_1}, \\hat{\\beta_0})\\)\n- 일반적으로, 알려진 단순선형회귀분석에서의 \\((\\hat{\\beta_1}, \\hat{\\beta_0})\\)을 구하는 방법!\n\\[\\hat {\\beta_0} = \\bar y - \\hat {\\beta_1}\\bar x\\]\n\\[\\hat {\\beta_1} = \\frac{S_{xy}}{S_{xx}} = \\frac{\\sum(x-\\bar x)(y-\\bar y)}{\\sum(x-\\bar x)^2}\\]\n- 공식유도법(나중에 시간되면 한번 풀어보세요!)\n\n\n\n- 실제로 저렇게 계산한 값과 R에서 회귀분석을 적합한 값이 일치한지 확인해보자\n\nx = cars$speed\ny = cars$dist\n\n\nbar_y = mean(y)\nbar_x = mean(x)\n\nbeta_1 = sum((x-bar_x)*(y-bar_y))/(sum((x-bar_x)^2))\n\nbeta_0 = bar_y - beta_1*bar_x\n\n\nbeta_1\n\n3.93240875912409\n\n\n\nbeta_0\n\n-17.579094890511\n\n\n- 즉, 이론상으로 속도에 따른 거리는 다음과 같이 설명할 수 있다\n\\[\\text{dist} \\approx  3.9324 \\times \\text{spped} -17.579... \\]\n\nlm1 = lm(y~x)\n\n\nlm1\n\n\nCall:\nlm(formula = y ~ x)\n\nCoefficients:\n(Intercept)            x  \n    -17.579        3.932  \n\n\n- 결과해석\n\n9.464^2\n\n89.567296\n\n\n\nsummary(lm1)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nx             3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n\nResiduals : 실제값과 예측값의 차이\n\n\\[\\varepsilon_i = \\hat{y_i} - y_i, \\quad \\varepsilon_i \\sim N(0, \\sigma^2)\\]\n\n잔차 제곱합은 다음과 같이 표현한다.\n\n\\[SSE =  \\sum_{i=1}^{n} \\varepsilon^2\\]\n\noptions(repr.plot.width = 8, repr.plot.height = 5)\nplot(y - lm1$fitted.values,main = \"잔차 plot\")\n\n\n\n\n\n\n\n\n\nplot(lm1,1)\n\n\n\n\n\n\n\n\n\nextra. subplots 그리는법\n\n\noptions(repr.plot.width = 12, repr.plot.height = 4)\npar(mfrow = c(1,2))\nplot(y - lm1$fitted.values,main = \"잔차 plot\")\nplot(lm1,1)\n\n\n\n\n\n\n\n\n\np-value? 검정통계량의 근거하여 적합한 모형의 유의미성을 검증하는 척도\n\n\nsummary(lm1)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nx             3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n\n결과해석\n1 추정된 회귀계수 \\(\\hat{\\beta_1},\\hat{\\beta_0}\\)는 t-검정통계량의 근거한 p-value값을 보았을 때 0.05보다 작아 통계적으로 유의하다.\n2 결정계수값을 살펴본 결과 적합한 모형은 65% 정도의 설명력을 가지고 있다.\n3 또한, F-통계량의 근거한 p-value값을 보아도 생성된 모델은 통계적으로 유의하다.\n4 잔차 plot을 그려본 결과 오차항의 정규성과 독립성 가정에 위배되지 않았음\n\npar(mfrow=c(1,2))\nplot(lm1,1);plot(lm1,2)\n\n\n\n\n\n\n\n\n5 따라서, 우리가 적합한 모델은 속도에 따른 자동차의 주행거리를 설명하기에 적합한 모형이라고 할 수 있다.\n\n\n\n\nex2. adult(logistic)\n- 로지스틱 모형은 target 변수인 \\(y\\)에 대한 직접적 모형화가 아닌 \\(y\\)가 특정 범주에 포함될 확률을 모형화한다.\n- 임계치(threshole)를 정하고 어떤 범주에 포함될 확률이 임계치보다 높으면 0 or 1로 예측하는 분류 모형이다. (이진분류에서!)\n\n모형 유도\nstep1 : 0과 1사이의 값으로 예측해 주는 모형 설계\n\\[P(X) = \\frac{exp(\\beta_0+\\beta_1X)}{1+exp(\\beta_0+\\beta_1X)}, \\quad \\in (0,1)\\]\nstep2. odds, 배팅을 하는 분야에서 확률 대신 많이 쓰이는 측도\n\\[\\text{odds} = \\frac{P(X)}{1-P(x)}= {exp(\\beta_0 + \\beta_1X)}, \\quad \\in(0, \\infty)\\]\nstep3. logit, 배팅을하는 분야에서 확률 대신 많이 쓰이는 측도\n\\[\\text{logit} = \\log{\\frac{P(X)}{1-P(x)}}  = \\beta_0 + \\beta_1 x, \\quad \\in (-\\infty, \\infty)\\]\n즉, 우리는 로짓에서 보이는 \\(\\beta_0, \\beta_1\\)을 추정하는 것임\n\n\n실습, 주식 데이터\n\n#install.packages(\"ISLR\")\nlibrary(ISLR)\nlibrary(tidyverse)\n#names(Smarket)\n\n\nhead(Smarket)\n\n\n\nA data.frame: 6 × 9\n\n\n\nYear\nLag1\nLag2\nLag3\nLag4\nLag5\nVolume\nToday\nDirection\n\n\n\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;fct&gt;\n\n\n\n\n1\n2001\n0.381\n-0.192\n-2.624\n-1.055\n5.010\n1.1913\n0.959\nUp\n\n\n2\n2001\n0.959\n0.381\n-0.192\n-2.624\n-1.055\n1.2965\n1.032\nUp\n\n\n3\n2001\n1.032\n0.959\n0.381\n-0.192\n-2.624\n1.4112\n-0.623\nDown\n\n\n4\n2001\n-0.623\n1.032\n0.959\n0.381\n-0.192\n1.2760\n0.614\nUp\n\n\n5\n2001\n0.614\n-0.623\n1.032\n0.959\n0.381\n1.2057\n0.213\nUp\n\n\n6\n2001\n0.213\n0.614\n-0.623\n1.032\n0.959\n1.3491\n1.392\nUp\n\n\n\n\n\n\n-데이터 설명\n\nlag\\(_i\\) : i번째 전 날의 smarket 주식 종가\ntoday : 오늘 주식 종가\nDirection : 주식이 올라갔는지, 떨어졌는지\nVolume : 거래량\n\n1 데이터셋 분할 7:3으로 분할\n\ntrain &lt;- Smarket %&gt;% sample_frac(0.7)\ntest &lt;- Smarket %&gt;% setdiff(train)\n\n\nglimpse(test)\n\nRows: 375\nColumns: 9\n$ Year      &lt;dbl&gt; 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, 2001, ~\n$ Lag1      &lt;dbl&gt; 0.381, 0.959, 1.392, -0.403, 1.303, -0.189, -0.562, 0.359, -~\n$ Lag2      &lt;dbl&gt; -0.192, 0.381, 0.213, 1.392, 0.027, -0.498, 0.701, -1.747, -~\n$ Lag3      &lt;dbl&gt; -2.624, -0.192, 0.614, 0.213, -0.403, 0.287, 0.680, 0.546, 0~\n$ Lag4      &lt;dbl&gt; -1.055, -2.624, -0.623, 0.614, 1.392, 1.303, -0.189, -0.562,~\n$ Lag5      &lt;dbl&gt; 5.010, -1.055, 1.032, -0.623, 0.213, 0.027, -0.498, 0.701, 0~\n$ Volume    &lt;dbl&gt; 1.19130, 1.29650, 1.44500, 1.40780, 1.23260, 1.09800, 1.2953~\n$ Today     &lt;dbl&gt; 0.959, 1.032, -0.403, 0.027, 0.287, 0.680, 0.546, -0.151, -0~\n$ Direction &lt;fct&gt; Up, Up, Down, Up, Up, Up, Up, Down, Down, Down, Down, Down, ~\n\n\n2 모형 적합\n\nlogit_fit &lt;-glm(Direction ~., data = train, \n                family = binomial(link = \"probit\")) ## 이진분류이므로 \"binomial\"이라고 기입\n\nWarning message:\n\"glm.fit: algorithm did not converge\"\nWarning message:\n\"glm.fit: fitted probabilities numerically 0 or 1 occurred\"\n\n\n3 예측\n\nlibrary(caret)\n\n필요한 패키지를 로딩중입니다: lattice\n\n\n다음의 패키지를 부착합니다: 'caret'\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\n\n\n\npred &lt;-  ifelse(predict(logit_fit,test, type = \"response\") &gt; 0.5, \"Up\", \"Down\") %&gt;% as_factor()\n\n\nconfusionMatrix(factor(pred), factor(test$Direction))\n\nWarning message in confusionMatrix.default(factor(pred), factor(test$Direction)):\n\"Levels are not in the same order for reference and data. Refactoring data to match.\"\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Down  Up\n      Down  185   0\n      Up      1 189\n                                          \n               Accuracy : 0.9973          \n                 95% CI : (0.9852, 0.9999)\n    No Information Rate : 0.504           \n    P-Value [Acc &gt; NIR] : &lt;2e-16          \n                                          \n                  Kappa : 0.9947          \n                                          \n Mcnemar's Test P-Value : 1               \n                                          \n            Sensitivity : 0.9946          \n            Specificity : 1.0000          \n         Pos Pred Value : 1.0000          \n         Neg Pred Value : 0.9947          \n             Prevalence : 0.4960          \n         Detection Rate : 0.4933          \n   Detection Prevalence : 0.4933          \n      Balanced Accuracy : 0.9973          \n                                          \n       'Positive' Class : Down            \n                                          \n\n\n\n\n정밀도, 재현율, 정확도\n- 위 네 가지는, 분류 모델의 성능 평가시 사용되는 척도이며 처음 접하는 사람들이 많이 헷갈려한다.\n- 위 예제는 4가지 지표에서 뛰어난 성능을 보이나 그렇지 않은 경우가 존재한다면?\n\nas.table(matrix(c(9644,23,252,81), nrow = 2))\n\n     A    B\nA 9644  252\nB   23   81\n\n\n- 정확도(accuracy)를 구해보자\n\n(9644+81)/(9644+23+252+81)\n\n0.9725\n\n\n- 오, 괜찮은 것 같다?\n\n그러나 여기에는 함정이 존재한다.(재현율)\n\n\n(81)/(252+81)\n\n0.243243243243243\n\n\n\n어라? 전체 B중에 예측모델이 B라고 예측한 것은 0.24..밖에 안되네?\n기업의 입장에서 보았을 때, 정확도보단 재현율이 더 중요한 지표임\n즉, 실제 참인 것 중에 예측모델이 참이라고 예측한 비율이 낮은 경우가 발생할 수 있음\n\n- 정밀도 : 모델이 참이라고 예측한 것 중, 실제 참인 것의 비율\n\n81/(23+81)\n\n0.778846153846154\n\n\n- 즉, 우리가 분류모델을 설계하고, 에측 성능을 판단할 때 단순히 정확도만 보고 판단하면 안 된다는 것!",
    "crumbs": [
      "Posts",
      "PBA2024",
      "01. intro"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-03-28-01. intro.html#unsupervised-learning",
    "href": "posts/PBA2024/2024-03-28-01. intro.html#unsupervised-learning",
    "title": "01. intro",
    "section": "Unsupervised Learning",
    "text": "Unsupervised Learning\n- 설명변수 \\(X=(x_1, x_2, \\dots)\\)로 부터 비슷한 성질을 갖는 녀석들끼리 한 집단으로 묶는 것\n- 대표적인 알고리즘 : kmeans, dbscan, hdbscan 등등\n\nk-means\n1주어진 데이터를 k개의 클러스터로 묶는 알고리즘, 각 클러스터와 거리 차이의 분산을 최소화 하는 방식으로 동작\n2 허나 군집의 수, 가중치와 거리 정의가 어려우며, 사전에 주어진 목적이 없으므로 결과 해석이 어려움.\n3 또한 잡음이나 이상값의 영향을 받으며 초기 군집 수를 결정해야 한다는 단점이 있다.\n\n#install.packages(\"patchwork\")\n\n\n\nCode\noptions(repr.plot.res = 200)\n\np1 = ggplot(iris, \n       aes(Petal.Length, Petal.Width, color = Species)) + \n            geom_point()\n\np2 = ggplot(iris, \n       aes(Petal.Length, Petal.Width)) + \n            geom_point()\n\nlibrary(patchwork)\n\np1 + p2\n\n\n\n\n\n\n\n\n\n- 만약, 우리가 생성한 군집이 잘 생성되었다면? 완쪽과 같은 분포로 군집이 형성되어야 한다.\n\n\nCode\nic &lt;- kmeans(iris[,3:4], centers = 3)\n\niris$cluster &lt;- as.factor(ic$cluster)\n\np1 = ggplot(iris, \n       aes(Petal.Length, Petal.Width, color = Species)) + \n            geom_point()\n\np2 = ggplot(iris, \n       aes(Petal.Length, Petal.Width, color = cluster)) + \n            geom_point()\n\np1 + p2",
    "crumbs": [
      "Posts",
      "PBA2024",
      "01. intro"
    ]
  },
  {
    "objectID": "posts/2023-08-19-00. Plotly test.html",
    "href": "posts/2023-08-19-00. Plotly test.html",
    "title": "00. Plotly test",
    "section": "",
    "text": "import\n\nimport plotly.express as ex\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/kalilurrahman/datasets/main/mobilephonemktshare2020.csv')\ndf.head()\n\n\n\n\n\n\n\n\n\nDate\nSamsung\nApple\nHuawei\nXiaomi\nOppo\nMobicel\nMotorola\nLG\nOthers\nRealme\nGoogle\nNokia\nLenovo\nOnePlus\nSony\nAsus\n\n\n\n\n0\n2019-10\n31.49\n22.09\n10.02\n7.79\n4.10\n3.15\n2.41\n2.40\n9.51\n0.54\n2.35\n0.95\n0.96\n0.70\n0.84\n0.74\n\n\n1\n2019-11\n31.36\n22.90\n10.18\n8.16\n4.42\n3.41\n2.40\n2.40\n9.10\n0.78\n0.66\n0.97\n0.97\n0.73\n0.83\n0.75\n\n\n2\n2019-12\n31.37\n24.79\n9.95\n7.73\n4.23\n3.19\n2.50\n2.54\n8.13\n0.84\n0.75\n0.90\n0.87\n0.74\n0.77\n0.70\n\n\n3\n2020-01\n31.29\n24.76\n10.61\n8.10\n4.25\n3.02\n2.42\n2.40\n7.55\n0.88\n0.69\n0.88\n0.86\n0.79\n0.80\n0.69\n\n\n4\n2020-02\n30.91\n25.89\n10.98\n7.80\n4.31\n2.89\n2.36\n2.34\n7.06\n0.89\n0.70\n0.81\n0.77\n0.78\n0.80\n0.69\n\n\n\n\n\n\n\n\n\ndf.set_index(\"Date\").diff().\\\n  dropna().boxplot(backend = \"plotly\")",
    "crumbs": [
      "Posts",
      "00. Plotly test"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "edu1",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 10, 2024\n\n\n03. MLE\n\n\ngc \n\n\n\n\nApr 3, 2024\n\n\n02. stat & ML basic\n\n\nGC \n\n\n\n\nMar 28, 2024\n\n\n00. jupyter lab 단축키 추가\n\n\nGC \n\n\n\n\nMar 26, 2024\n\n\n01. intro\n\n\nGC \n\n\n\n\nAug 19, 2023\n\n\n00. Plotly test\n\n\nGC \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-03-26-00. jupyter 단축키 추가법.html",
    "href": "posts/2024-03-26-00. jupyter 단축키 추가법.html",
    "title": "00. jupyter lab 단축키 추가",
    "section": "",
    "text": "intro\n- 예전엔 그냥 주피터에서 R을 쓸 때 %&gt;% 같은 파이프 연산자와 &lt;- 같은 변수 정의를 위한 단축키가 잘 작동되었는데 버전 문제인가? 이제는 적용되지 않더라….\n- 이것 때문에 하루를 날리고 끝내 방법을 찾음\n- 이제 단축키를 내 마음대로 추가할 수 있음 (너무 좋앙)\n\n\n단축키 추가법\n1 settings Editor -&gt; keyboard shortcuts 클릭!\n2 그럼 상단에 Json Setting Editor라고 눈 동그랗게 뜨고 보면 보임\n\n\n\n3 이제 저기로 들어가서 아래와 같은 json 코드를 삽입하자\n\n\n\n4 후. 이것 때문에 하루 다 날린 걸 생각하면 열이 받지만, 다시금 다양한 언어를 잘 활용할 줄 아는 것의 중요성을 느꼈다.\n5 ref\n\n\ntest\n\nlibrary(tidyverse)\n\n\nmpg %&gt;% ggplot(aes(x = hwy, y = cty, color = cyl)) +\n            geom_point(alpha = 0.5, size = 2) +\n            scale_color_viridis_c() +\n            theme_minimal()",
    "crumbs": [
      "Posts",
      "00. jupyter lab 단축키 추가"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-03-02. stat & ml basic.html",
    "href": "posts/PBA2024/2024-04-03-02. stat & ml basic.html",
    "title": "02. stat & ML basic",
    "section": "",
    "text": "1 모집단(Population) : 연구 또는 관측 대상이 되는 전체\n\nex : 우리나라 30세이상 월 평균 소득 조사, 사실 이를 위해 해당 집단을 전수 조사하는 것은 거의 불가능에 가까움\n\n2 표본 : 모집단의 일부, 연구 또는 관측을 위해 일정 시점 또는 일정 집단에서 수집한 대상(데이터)\n\nex : 2020년 1월 1일시점에 조사한 30세 이상 국내 성인 남자 10명의 월평균 소득\n\n3 표본평균과 중앙값 구하기\n\n평균\n\n\nincome &lt;- c(100, 200, 300, 400, 500, 600, 700, 800, 1000 ,1000)\n\nsum(income)/length(income)\n\n560\n\n\n\n중앙값\n\n\n#sort(income);length(income)\n\nprint(paste(\"손계산 :\",(sort(income)[5]+sort(income)[6])/2))\n\nprint(paste(\"함수 사용 :\",median(income)))\n\n[1] \"손계산 : 550\"\n[1] \"함수 사용 : 550\"",
    "crumbs": [
      "Posts",
      "PBA2024",
      "02. stat & ML basic"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-03-02. stat & ml basic.html#모집단과-표본",
    "href": "posts/PBA2024/2024-04-03-02. stat & ml basic.html#모집단과-표본",
    "title": "02. stat & ML basic",
    "section": "",
    "text": "1 모집단(Population) : 연구 또는 관측 대상이 되는 전체\n\nex : 우리나라 30세이상 월 평균 소득 조사, 사실 이를 위해 해당 집단을 전수 조사하는 것은 거의 불가능에 가까움\n\n2 표본 : 모집단의 일부, 연구 또는 관측을 위해 일정 시점 또는 일정 집단에서 수집한 대상(데이터)\n\nex : 2020년 1월 1일시점에 조사한 30세 이상 국내 성인 남자 10명의 월평균 소득\n\n3 표본평균과 중앙값 구하기\n\n평균\n\n\nincome &lt;- c(100, 200, 300, 400, 500, 600, 700, 800, 1000 ,1000)\n\nsum(income)/length(income)\n\n560\n\n\n\n중앙값\n\n\n#sort(income);length(income)\n\nprint(paste(\"손계산 :\",(sort(income)[5]+sort(income)[6])/2))\n\nprint(paste(\"함수 사용 :\",median(income)))\n\n[1] \"손계산 : 550\"\n[1] \"함수 사용 : 550\"",
    "crumbs": [
      "Posts",
      "PBA2024",
      "02. stat & ML basic"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-03-02. stat & ml basic.html#평균과-분산",
    "href": "posts/PBA2024/2024-04-03-02. stat & ml basic.html#평균과-분산",
    "title": "02. stat & ML basic",
    "section": "평균과 분산",
    "text": "평균과 분산\n- 모집단의 분포(형태는) 일반적으로 수집한 표본의 모수인 평균과 분산에 의해 결정된다.\n\n평균(기댓값)\n1 이산형일 경우 : 베르누이, 이항, 포아송 분포 등등…\n\nex1 : 동전 던지기를 해서 앞면이 나올 평균확률은?\n\n\n\n\n\\(x\\)\n0\n1\n\n\n\n\n\\(f(x)\\)\n1/2\n1/2\n\n\n\n\\[E(X) = \\sum_{i=1}^{n} x_i f(x_i)\\]\n2 연속형일 경우 : 베타, 지수, 감마, 정규 등등…\n\nex2, 표준정규분포의 평균은?\n\n\\[X \\sim N(0, 1)\\]\n\\[f(x) = \\frac{1}{\\sqrt {2\\pi}} e^{- \\frac {x^2}{2}}\\]\n\n\n숙제 1. 아래의 공식을 이용하여 표준정규분포의 평균이 실제로 1이 나오는지 확인!\n\\[E(X) = \\int xf(x) \\,\\,dx\\]\n\n\n대수의 법칙\n- Law of large numbers\n- 샘플사이즈가 클수록 표본평균은 모집단의 평균으로 수렴한다는 법칙! (체비셰프 부등식을 이용하여 증명 가능!)\n\\[\\begin {align} P(|\\overline X - \\mu| &lt; \\varepsilon)  &=  P(|\\overline X - \\mu| &lt; \\varepsilon^2) \\\\ \\\\\n                                                       &= 1 -  \\frac{E(\\overline X_{n} - \\mu)^2}{\\varepsilon^2} \\\\ \\\\\n                                                       &= 1 - \\frac{\\sigma^2/n}{\\varepsilon^2} \\approx 1 \\end {align} \\]\n- R을 이용한 증명\n\n\n\n\\(x\\)\n100\n200\n\n\n\n\n\\(f(x)\\)\n1/2\n1/2\n\n\n\n\n이므로 모집단의 기대값은 $E(X) = xf(x) = 100 /2 + 200 /2 = 150 $\n\n\n### code_fold : true\n#install.packages(\"plotly\")\nlibrary(tidyverse)\nlibrary(plotly)\n\n\nx &lt;- tibble(income = c(100, 200))\n\ndata &lt;- tibble()\nfor (i in 1:500) \n    {\n        sample &lt;- x %&gt;% sample_n(i, replace = T, weight = c(1/2,1/2))\n        sample &lt;- sample %&gt;% mutate(sample_size = i)\n        data &lt;- bind_rows(data, sample)\n    }\n\n\n### code_fold : true\nsample_mean &lt;- data %&gt;% group_by(sample_size) %&gt;% \n                        summarize(sample_mean = mean(income))\n\nfig &lt;- plot_ly(data = sample_mean, \n               x = ~sample_size, y = ~sample_mean, mode = \"lines\", \n                type = \"scatter\", name = \"표본평균\", opacity = 0.5) %&gt;% \n                        add_trace(y = 150, name = \"모집단 평균\")\n\nfig %&gt;% \n    layout(title = \"Law of large numbers\") \n\n\n\n    \n        \n        \n\n\n\n\n\n\n\n\n    \n    \n        \n\n    \n\n\n\n\n\n분산\n- 수집한 표본들이 평균으로부터 떨어져 있는 정도를 측정\n1 이산형일 경우\n\nex1 : 동전 던지기를 해서 앞면이 나오는 실험의 분산은?\n\n\n\n\n\\(x\\)\n0\n1\n\n\n\n\n\\(f(x)\\)\n1/2\n1/2\n\n\n\n\\[\\sum_{i=0}^{n} = [x_i-E(X)]^2f(x_i)\\]\n2 연속형일 경우\n\\[Var(X) = \\int [x-E(x)]^2f(x) \\,\\,dx\\]",
    "crumbs": [
      "Posts",
      "PBA2024",
      "02. stat & ML basic"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-03-02. stat & ml basic.html#정규분포",
    "href": "posts/PBA2024/2024-04-03-02. stat & ml basic.html#정규분포",
    "title": "02. stat & ML basic",
    "section": "정규분포",
    "text": "정규분포\n1 분포 정의 \\(\\to X \\sim N(\\mu, \\sigma^2)\\)\n\\[ f(x) = \\frac{1}{\\sqrt(2\\pi\\sigma^2)}\\exp\\left( -\\frac12 \\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)\\]\n2 대게 종모양의 분포를 띔\n3 표준편차가 작을수록 곡선이 좀더 뾰족해지고 폭이 좁아진다? \\(\\to\\) 분산에 정의를 다시 한번 생각해보시면됩니다.\n\noptions(repr.plot.res=200, repr.plot.width = 8, repr.plot.height = 6)\nx1 &lt;- rnorm(10000, 0, 1)\nx2 &lt;- rnorm(10000, 0, 10)\nx3 &lt;- rnorm(10000, 0, 20)\nx4 &lt;- rnorm(10000, 0, 30)\n\npar(mfrow = c(2,2))\nhist(x1, main = \"평균 : 0, 표준편차 : 1\", breaks = 50, xlim = c(-100,100))\nhist(x2, main = \"평균 : 0, 표준편차 : 10\", breaks = 50, xlim = c(-100,100))\nhist(x3, main = \"평균 : 0, 표준편차 : 20\", breaks = 50, xlim = c(-100,100))\nhist(x4, main = \"평균 : 0, 표준편차 : 30\", breaks = 50, xlim = c(-100,100))\n\n\n\n\n\n\n\n\n4 정규분포의 기각영역\n\n우리가 수집한 어떤 집단의 평균이 0일 때의 가설검정\n\n\\[H_0 : \\mu = 0, \\quad H_1 : \\mu \\neq 0\\]\n\nx = rnorm(1000)\nnormal &lt;- tibble(x = x, fx = fx)\n\nnormal &lt;- normal %&gt;% mutate(p = ifelse(abs(x) &gt;= 2.58, \"유의수준 1%\", \n                             ifelse(abs(x) &gt;= 1.96, \"유의수준 5%\", \n                                    ifelse(abs(x) &gt;= 1.65, \"유의수준 10%\", \"기각 x\"))))\n\n\noptions(repr.plot.res = 200, repr.plot.width = 8, repr.plot.height = 4)\nnormal %&gt;% ggplot(`aes(x = x, fill = p)) +\n            geom_histogram(bins = 100)\n\n\n\n\n\n\n\n\n5 중심극한정리\n\n표본의 크기가 커지면, 표본평균의 분포는 정규분포로 수렴 \\(n \\to \\infty,\\, \\overline X_n \\sim N(\\mu, \\sigma^2)\\)\n이것을 가정하고 있기 때문에, 우리는 어떠한 가설설정 시 정규분포를 이용해서 통계적 유의성을 말할 수 있는거임\n\n\nexample1, 동전던지기를 통한 중심극한정리 실험\n\n\nstep1. 동전던지기 실험, 동전던지기 100회 \\(\\text{sample size} = 100\\)\n\n\nrbinom(100, 1, 0.5)\n\n\n0110110111111110011101100000100100011100101111101000000110111100000101000011100001011111010111000000\n\n\n\nstep2. 표본평균 구하기\n\n\nx_mean = mean(rbinom(100, 1, 0.5))\nx_mean\n\n0.6\n\n\n\nstep3. 표본평균의 분포 구하기\n\n\nsample_mean = c()\n\nfor (i in 1:1000) {\n    x_mean = mean(rbinom(1000, 1, 0.5))\n    sample_mean = append(sample_mean,x_mean)\n    }\n\n\nhist(sample_mean,main = \"n=1000\", breaks = 50)",
    "crumbs": [
      "Posts",
      "PBA2024",
      "02. stat & ML basic"
    ]
  },
  {
    "objectID": "posts/PBA2024/2024-04-03-02. stat & ml basic.html#ml-basic",
    "href": "posts/PBA2024/2024-04-03-02. stat & ml basic.html#ml-basic",
    "title": "02. stat & ML basic",
    "section": "ML basic",
    "text": "ML basic\n\n1. 선형회귀 평가지표\n- 결정계수 \\(R^2\\) : 우리가 적합시킨 모델이 얼마만큼의 설명력을 가지고 있는가\n\n지난시간 학습한 자료 꼭 한번 확인!\n\n\\[R^2 = \\frac{SSR}{SST} = \\frac{1-SSE}{SST}\\]\n\n\n2. Overfitting\n\n우리가 학습시킨 모델이, 훈련데이터에만 집중적으로 학습이되어서, 실제 검증용 데이터로 평가를 했을 때 모델 성능이 낮아지는 경우\n이거를 표현해보면…\n\n\\[\\begin {align} \\text {MSE} = E[(\\hat y - y)^2] &= Var(\\hat y) + \\text {bias}^2   \\\\\n                                                                &= E[(\\hat y - E(\\hat y ))^2] + E[(E(\\hat y) - y)^2] + Var(\\varepsilon) \\end {align}\\]\n- \\(Var(\\hat y) \\, \\to \\,\\) 훈련자료의 변화에 \\(\\hat y\\)가 얼마나 민감하게 반응하는가?(즉, 설명력을 의미한다.)\n- \\(\\text {bias}^2\\) : 실제자료를 모형으로 얼마나 가깝게 근사할 수 있는가? (만약, \\(E(\\hat y) = y\\) 일 경우 해당 추정량은 불편추정량이다.)\n\n\\(Var(\\hat y),\\,\\text {bias}^2\\) 는 reducible error로 어떤 모델을 선택하느냐에 따라 줄일 수 있는 오차이다.\n\n- \\(Var(\\varepsilon)\\) : 어떤 모델을 사용하건 줄일 수 없는 오차\n- Flexibility : 모델 복잡도 측도\n\nFlexibility가 높아지면 \\(Var(\\hat y)\\) 는 높아지고, \\(\\text {bias}\\)는 낮아진다. (trade-off)\n이를 바꿔말하면 더 유연한(복잡한) 모형은 더 큰 분산을 가지고 더 단순한 모형일수록 큰 편의를 가진다.\n훈련 데이터로 모델 적합시, bias를 낮추기 위해 Var를 높여 모델 복잡도를 지나치게 높이는 것은 좋은 선택이 아니다.\n아래의 예시는 모형의 복잡도가 너무 높으면 평가 데이터에 대한 MSE가 오히려 과대추정되는 “과적합 문제” 를 보여주고 있다.\n\n\n\n\n\nex1\n\n\n\n- 실제로 위의 원데이터는 일반적인 선형모형으로 적합해도 괜찮을 것 같다.\n- 왼쪽그림에서 지나치게 복잡하게 적합된 초록색 선을 살펴보자.\n\n훈련 MSE(회색선) 는 3개의 모델 중 가장 낮으나, 과적합 이슈로 평가 MSE(빨간색선) 가 과대추정된 것을 볼 수 있다.\n\n\n\nex2\n\n\n\n- 위 데이터는 ex1과 달리 일반적인 선형회귀모형으로 적합하면 안될것 같은 느낌이 든다.\n- 파란색선은 중간 정도로 적합된 복잡한 모형이고, 초록색선은 지나치게 적합된 모형이다.\n\n오른쪽 그림을 살펴보면 파란색선으로 적합된 모형의 Test MSE가 가장 낮음을 볼 수 있다.\n\n\n\n과적합 방지 1. 변수선택법\n- 변수선택법 : AIC, BIC, Mallows’ C_p 등등\n\n\n과적합 방지 2. Regularized Linear Regression\n- 정규화 선형회귀는 선형회귀 계수에 대한 제약 조건을 추가하여 모델이 과도하게 최적화되는 현상을 막는 방법!\n1 Ridge\n\n기존 OLS를 이용한 \\(\\beta_i\\) 추정할 떄 사용하는 정규방정식\n\n\\[RSS = \\sum_{i=1}^n(y_i - \\beta_0 -\\sum_{j=1}^p\\beta_jx_{ij})^2\\]\n\nRidge 추정량은 위와 매우 유사하나 shrinkage penalty를 추가로 고려하여 최소화한다.\n\n\\[\\sum_{i=1}^n(y_i - \\beta_0 -\\sum_{j=1}^p\\beta_jx_{ij})^2 + \\lambda \\sum_{i=1}^p \\beta_j^2\\]\n\\[\\lambda \\,:\\,tuning \\,\\,parameter\\]\n\n\\(\\lambda\\)가 0이되면 일반적인 선형회귀모형이되고, \\(\\lambda\\)가 커지면 정규화 정도가 커진다.(회귀계수들이 작아진다.) \\(\\to\\) bias는 증가 variance는 감소\n릿지회귀모형에서는 L2 penalty 를 사용하여 베타계수들이 0에 근사하도록 한다.\n\n2 Lasso\n\n라쏘는 릿지와 다르게 베타계수를 “0값으로 보낸다.”\n이것을 “L1 penalty” 라고 부른다.m\n\n\\[\\sum_{i=1}^n(y_i - \\beta_0 -\\sum_{j=1}^p\\beta_jx_{ij})^2 + \\lambda \\sum_{i=1}^p |\\beta_j|\\]\n\n\n실습\n\n#install.packages(\"mosaicData\")\n\n\nlibrary(tidyverse)\nlibrary(mosaicData)\nlibrary(glmnet) ## 리지 및 라쏘 회귀를 적합하기 위한 패키지\n\n1 데이터확인\n\nglimpse(RailTrail)\n\nRows: 90\nColumns: 11\n$ hightemp   &lt;int&gt; 83, 73, 74, 95, 44, 69, 66, 66, 80, 79, 78, 65, 41, 59, 50,~\n$ lowtemp    &lt;int&gt; 50, 49, 52, 61, 52, 54, 39, 38, 55, 45, 55, 48, 49, 35, 35,~\n$ avgtemp    &lt;dbl&gt; 66.5, 61.0, 63.0, 78.0, 48.0, 61.5, 52.5, 52.0, 67.5, 62.0,~\n$ spring     &lt;int&gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,~\n$ summer     &lt;int&gt; 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,~\n$ fall       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1,~\n$ cloudcover &lt;dbl&gt; 7.6, 6.3, 7.5, 2.6, 10.0, 6.6, 2.4, 0.0, 3.8, 4.1, 8.5, 7.2~\n$ precip     &lt;dbl&gt; 0.00, 0.29, 0.32, 0.00, 0.14, 0.02, 0.00, 0.00, 0.00, 0.00,~\n$ volume     &lt;int&gt; 501, 419, 397, 385, 200, 375, 417, 629, 533, 547, 432, 418,~\n$ weekday    &lt;lgl&gt; TRUE, TRUE, TRUE, FALSE, TRUE, TRUE, TRUE, FALSE, FALSE, TR~\n$ dayType    &lt;chr&gt; \"weekday\", \"weekday\", \"weekday\", \"weekend\", \"weekday\", \"wee~\n\n\n- model.matrix는 기본적으로 절편을 포함한 모형을 산정한다. 절편을 제외시킨 모형을 고려해보자\n\nx &lt;- model.matrix(volume~.-1 ,RailTrail) \ny &lt;- RailTrail$volume\n\n- 모형적합\n\nridge.fit &lt;- cv.glmnet(x,y,alpha=0) ## 모형적합 alpha=0 은 릿지를 말함\n\n\nbestlam &lt;- ridge.fit$lambda.min\nlog(bestlam)\n\n3.02271971457234\n\n\n- 각 변수들의 beta 계수구하기\n\ncoef(ridge.fit, s = \"lambda.min\")\n\n12 x 1 sparse Matrix of class \"dgCMatrix\"\n                        s1\n(Intercept)    111.4030636\nhightemp         3.6351375\nlowtemp         -0.9192012\navgtemp          1.9016337\nspring           9.6677301\nsummer           4.4014186\nfall           -27.7525473\ncloudcover      -8.1350693\nprecip         -88.4982099\nweekdayFALSE    13.8267092\nweekdayTRUE    -13.8900786\ndayTypeweekend  13.9990402\n\n\n- MSE가 최소가 되는 지점을 확인\n\noptions(repr.plot.res=200,repr.plot.height=4,repr.plot.width=10)\n\nplot(ridge.fit)\nabline(v=log(bestlam),lty=\"dashed\",lwd=2,col=\"blue\")\n\n\n\n\n\n\n\n\n- 산출된 best.lam값으로 모형을 다시 적합후 test_MSE를 구해보자\n\ntrain  &lt;- sample(1:nrow(x),nrow(x)*0.7)\ntest  &lt;- -train\ny.test &lt;- y[test]\ny.train &lt;- y[train]\n\n\nridge.fit &lt;- glmnet(x[train,], y.train, alpha=0, lambda=bestlam, family=\"gaussian\")\n\nridge.pred &lt;- predict(ridge.fit,s=bestlam,newx=x[test,]) \n\nridge.coef &lt;- predict(ridge.fit,s=bestlam,newx=x[test,],type=\"coefficients\") ## type=\"coefficients\"로 하면 예측된 베타계수를 보여줌\n\n- 여기선 MSE가 거의 1만에 가깝게 나왔는데 이는 지나치게 모형을 단순화한 것임 즉, best_lam값을 다시 찾는 과정을 반복해야한다….\n\nmean((ridge.pred - y.test)^2)\n\n8763.80834274373",
    "crumbs": [
      "Posts",
      "PBA2024",
      "02. stat & ML basic"
    ]
  }
]