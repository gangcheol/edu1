{"title":"02. stat & ML basic","markdown":{"yaml":{"title":"02. stat & ML basic","author":"GC","date":"04/23/24"},"headingText":"모집단과 표본","containsRefs":false,"markdown":"\n\n\n`1` 모집단(Population) : 연구 또는 관측 대상이 되는 전체\n\n* ex : 우리나라 30세이상 월 평균 소득 조사, 사실 이를 위해 해당 집단을 전수 조사하는 것은 거의 `불가능`에 가까움\n\n`2` 표본 : 모집단의 일부, 연구 또는 관측을 위해 일정 시점 또는 일정 집단에서 수집한 대상(데이터)\n\n* ex : 2020년 1월 1일시점에 조사한 30세 이상 `국내 성인 남자 10명의 월평균 소득`\n\n`3` 표본평균과 중앙값 구하기\n\n* 평균\n\n* 중앙값\n\n***\n\n## 평균과 분산\n\n`-` 모집단의 분포(형태는) 일반적으로 수집한 표본의 모수인 평균과 분산에 의해 결정된다.\n\n### 평균(기댓값)\n\n`1` 이산형일 경우 : 베르누이, 이항, 포아송 분포 등등...\n\n> ex1 : 동전 던지기를 해서 앞면이 나올 평균확률은?\n\n\n|$x$|0|1|\n|---|---|---|\n|$f(x)$|1/2|1/2|\n\n$$E(X) = \\sum_{i=1}^{n} x_i f(x_i)$$\n\n`2` 연속형일 경우 : 베타, 지수, 감마, 정규 등등...\n\n> ex2, 표준정규분포의 평균은?\n\n\n$$X \\sim N(0, 1)$$\n\n$$f(x) = \\frac{1}{\\sqrt {2\\pi}} e^{- \\frac {x^2}{2}}$$\n\n### 숙제 1. 아래의 공식을 이용하여 표준정규분포의 평균이 실제로 1이 나오는지 확인!\n\n$$E(X) = \\int xf(x) \\,\\,dx$$\n\n### 대수의 법칙\n\n`-` Law of large numbers \n\n`-` 샘플사이즈가 클수록 표본평균은 모집단의 평균으로 수렴한다는 법칙! (`체비셰프 부등식`을 이용하여 증명 가능!)\n\n$$\\begin {align} P(|\\overline X - \\mu| < \\varepsilon)  &=  P(|\\overline X - \\mu| < \\varepsilon^2) \\\\ \\\\ \n                                                       &= 1 -  \\frac{E(\\overline X_{n} - \\mu)^2}{\\varepsilon^2} \\\\ \\\\\n                                                       &= 1 - \\frac{\\sigma^2/n}{\\varepsilon^2} \\approx 1 \\end {align} $$\n\n`-` R을 이용한 증명\n\n|$x$|100|200|\n|---|---|---|\n|$f(x)$|1/2|1/2|\n\n* 이므로 모집단의 기대값은 $E(X) = \\sum xf(x) = 100 \\times 1/2 + 200 \\times 1/2 = 150 $\n\n### 분산\n\n`-` 수집한 표본들이 평균으로부터 떨어져 있는 정도를 측정\n\n`1` 이산형일 경우\n\n> ex1 : 동전 던지기를 해서 앞면이 나오는 실험의 분산은?\n\n\n|$x$|0|1|\n|---|---|---|\n|$f(x)$|1/2|1/2|\n\n$$\\sum_{i=0}^{n} = [x_i-E(X)]^2f(x_i)$$\n\n`2` 연속형일 경우\n\n$$Var(X) = \\int [x-E(x)]^2f(x) \\,\\,dx$$\n\n***\n\n## 정규분포\n\n`1` 분포 정의 $\\to X \\sim N(\\mu, \\sigma^2)$ \n\n$$ f(x) = \\frac{1}{\\sqrt(2\\pi\\sigma^2)}\\exp\\left( -\\frac12 \\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)$$\n\n`2` 대게 종모양의 분포를 띔\n\n`3` 표준편차가 작을수록 곡선이 좀더 뾰족해지고 폭이 좁아진다? $\\to$ 분산에 정의를 다시 한번 생각해보시면됩니다.\n\n`4` 정규분포의 기각영역\n\n* 우리가 수집한 어떤 집단의 평균이 0일 때의 가설검정\n\n$$H_0 : \\mu = 0, \\quad H_1 : \\mu \\neq 0$$\n\n`5` 중심극한정리\n\n* 표본의 크기가 커지면, 표본평균의 분포는 정규분포로 수렴 $n \\to \\infty,\\, \\overline X_n \\sim N(\\mu, \\sigma^2)$\n\n\n* 이것을 가정하고 있기 때문에, 우리는 어떠한 가설설정 시 정규분포를 이용해서 통계적 유의성을 말할 수 있는거임\n\n> example1, 동전던지기를 통한 중심극한정리 실험\n\n* step1. 동전던지기 실험, 동전던지기 100회 $\\text{sample size} = 100$\n\n* step2. 표본평균 구하기\n\n* step3. 표본평균의 분포 구하기\n\n***\n\n## ML basic\n\n### 1. 선형회귀 평가지표\n\n`-` 결정계수 $R^2$ : 우리가 적합시킨 모델이 얼마만큼의 설명력을 가지고 있는가\n\n* 지난시간 학습한 자료 꼭 한번 확인!\n\n$$R^2 = \\frac{SSR}{SST} = \\frac{1-SSE}{SST}$$\n\n### 2. Overfitting\n\n* 우리가 학습시킨 모델이, 훈련데이터에만 집중적으로 학습이되어서, 실제 검증용 데이터로 평가를 했을 때 모델 성능이 낮아지는 경우\n\n* 이거를 표현해보면...\n\n$$\\begin {align} \\text {MSE} = E[(\\hat y - y)^2] &= Var(\\hat y) + \\text {bias}^2   \\\\ \n                                                                &= E[(\\hat y - E(\\hat y ))^2] + E[(E(\\hat y) - y)^2] + Var(\\varepsilon) \\end {align}$$\n\n`-` $Var(\\hat y) \\, \\to \\,$  훈련자료의 변화에 $\\hat y$가 얼마나 민감하게 반응하는가?(즉, 설명력을 의미한다.)\n\n`-` $\\text {bias}^2$  :  실제자료를 모형으로 얼마나 가깝게 근사할 수 있는가? (만약, $E(\\hat y) = y$ 일 경우 해당 추정량은 불편추정량이다.)\n\n* $Var(\\hat y),\\,\\text {bias}^2$ 는 reducible error로 어떤 모델을 선택하느냐에 따라 줄일 수 있는 오차이다.\n\n`-` $Var(\\varepsilon)$ : 어떤 모델을 사용하건 줄일 수 없는 오차\n\n`-` Flexibility :  모델 복잡도 측도\n\n* Flexibility가 높아지면 $Var(\\hat y)$ 는 높아지고, $\\text {bias}$는 낮아진다. (trade-off)\n \n* 이를 바꿔말하면 더 유연한(복잡한) 모형은 더 큰 분산을 가지고 더 단순한 모형일수록 큰 편의를 가진다.\n\n* 훈련 데이터로 모델 적합시, `bias`를 낮추기 위해 `Var`를 높여 모델 복잡도를 지나치게 높이는 것은 좋은 선택이 아니다.\n\n* 아래의 예시는 모형의 복잡도가 너무 높으면 **평가 데이터에 대한 MSE**가 오히려 과대추정되는 **\"과적합 문제\"** 를 보여주고 있다.\n\n<center><img src = \"2-1.png\" width = 600></center>\n\n#### ex1\n\n<center><img src = \"2-3.png\" width = 600></center>\n\n`-` 실제로 위의 원데이터는 일반적인 선형모형으로 적합해도 괜찮을 것 같다.\n\n`-` 왼쪽그림에서 지나치게 복잡하게 적합된 초록색 선을 살펴보자.\n\n* **훈련 MSE(회색선)** 는 3개의 모델 중 가장 낮으나, 과적합 이슈로 **평가 MSE(빨간색선)** 가 과대추정된 것을 볼 수 있다.\n\n#### ex2\n\n<center><img src = \"2-2.png\" width = 600></center>\n\n`-` 위 데이터는 `ex1`과 달리 일반적인 선형회귀모형으로 적합하면 안될것 같은 느낌이 든다.\n\n`-` 파란색선은 중간 정도로 적합된 복잡한 모형이고, 초록색선은 지나치게 적합된 모형이다.\n\n* 오른쪽 그림을 살펴보면 파란색선으로 적합된 모형의 `Test MSE`가 가장 낮음을 볼 수 있다.\n\n#### 과적합 방지 1. 변수선택법\n\n`-` 변수선택법 : AIC, BIC, Mallows' C_p 등등\n\n#### 과적합 방지 2. Regularized Linear Regression\n\n`-` 정규화 선형회귀는 선형회귀 계수에 대한 제약 조건을 추가하여 모델이 과도하게 최적화되는 현상을 막는 방법!\n\n`1` Ridge\n\n* 기존 OLS를 이용한 $\\beta_i$ 추정할 떄 사용하는 정규방정식\n\n$$RSS = \\sum_{i=1}^n(y_i - \\beta_0 -\\sum_{j=1}^p\\beta_jx_{ij})^2$$\n\n*  Ridge 추정량은 위와 매우 유사하나 `shrinkage penalty`를 추가로 고려하여 최소화한다.\n\n$$\\sum_{i=1}^n(y_i - \\beta_0 -\\sum_{j=1}^p\\beta_jx_{ij})^2 + \\lambda \\sum_{i=1}^p \\beta_j^2$$\n\n$$\\lambda \\,:\\,tuning \\,\\,parameter$$\n\n* $\\lambda$가 0이되면 일반적인 선형회귀모형이되고, $\\lambda$가 커지면 **정규화 정도가 커진다.(회귀계수들이 작아진다.)** $\\to$ `bias`는 증가 `variance`는 감소\n\n* 릿지회귀모형에서는 **L2 penalty** 를 사용하여 베타계수들이 0에 근사하도록 한다.\n\n`2` Lasso\n\n* 라쏘는 릿지와 다르게 베타계수를 **\"0값으로 보낸다.\"**\n\n* 이것을 **\"L1 penalty\"** 라고 부른다.m\n\n$$\\sum_{i=1}^n(y_i - \\beta_0 -\\sum_{j=1}^p\\beta_jx_{ij})^2 + \\lambda \\sum_{i=1}^p |\\beta_j|$$\n\n#### 실습\n\n`1` 데이터확인\n\n`-` model.matrix는 기본적으로 절편을 포함한 모형을 산정한다. 절편을 제외시킨 모형을 고려해보자\n\n`-` 모형적합\n\n`-` 각 변수들의 beta 계수구하기\n\n`-` `MSE`가 최소가 되는 지점을 확인\n\n`-` 산출된 `best.lam`값으로 모형을 다시 적합후 `test_MSE`를 구해보자\n\n`-` 여기선 MSE가 거의 1만에 가깝게 나왔는데 이는 지나치게 모형을 단순화한 것임 즉, best_lam값을 다시 찾는 과정을 반복해야한다....\n\n****\n","srcMarkdownNoYaml":"\n\n## 모집단과 표본\n\n`1` 모집단(Population) : 연구 또는 관측 대상이 되는 전체\n\n* ex : 우리나라 30세이상 월 평균 소득 조사, 사실 이를 위해 해당 집단을 전수 조사하는 것은 거의 `불가능`에 가까움\n\n`2` 표본 : 모집단의 일부, 연구 또는 관측을 위해 일정 시점 또는 일정 집단에서 수집한 대상(데이터)\n\n* ex : 2020년 1월 1일시점에 조사한 30세 이상 `국내 성인 남자 10명의 월평균 소득`\n\n`3` 표본평균과 중앙값 구하기\n\n* 평균\n\n* 중앙값\n\n***\n\n## 평균과 분산\n\n`-` 모집단의 분포(형태는) 일반적으로 수집한 표본의 모수인 평균과 분산에 의해 결정된다.\n\n### 평균(기댓값)\n\n`1` 이산형일 경우 : 베르누이, 이항, 포아송 분포 등등...\n\n> ex1 : 동전 던지기를 해서 앞면이 나올 평균확률은?\n\n\n|$x$|0|1|\n|---|---|---|\n|$f(x)$|1/2|1/2|\n\n$$E(X) = \\sum_{i=1}^{n} x_i f(x_i)$$\n\n`2` 연속형일 경우 : 베타, 지수, 감마, 정규 등등...\n\n> ex2, 표준정규분포의 평균은?\n\n\n$$X \\sim N(0, 1)$$\n\n$$f(x) = \\frac{1}{\\sqrt {2\\pi}} e^{- \\frac {x^2}{2}}$$\n\n### 숙제 1. 아래의 공식을 이용하여 표준정규분포의 평균이 실제로 1이 나오는지 확인!\n\n$$E(X) = \\int xf(x) \\,\\,dx$$\n\n### 대수의 법칙\n\n`-` Law of large numbers \n\n`-` 샘플사이즈가 클수록 표본평균은 모집단의 평균으로 수렴한다는 법칙! (`체비셰프 부등식`을 이용하여 증명 가능!)\n\n$$\\begin {align} P(|\\overline X - \\mu| < \\varepsilon)  &=  P(|\\overline X - \\mu| < \\varepsilon^2) \\\\ \\\\ \n                                                       &= 1 -  \\frac{E(\\overline X_{n} - \\mu)^2}{\\varepsilon^2} \\\\ \\\\\n                                                       &= 1 - \\frac{\\sigma^2/n}{\\varepsilon^2} \\approx 1 \\end {align} $$\n\n`-` R을 이용한 증명\n\n|$x$|100|200|\n|---|---|---|\n|$f(x)$|1/2|1/2|\n\n* 이므로 모집단의 기대값은 $E(X) = \\sum xf(x) = 100 \\times 1/2 + 200 \\times 1/2 = 150 $\n\n### 분산\n\n`-` 수집한 표본들이 평균으로부터 떨어져 있는 정도를 측정\n\n`1` 이산형일 경우\n\n> ex1 : 동전 던지기를 해서 앞면이 나오는 실험의 분산은?\n\n\n|$x$|0|1|\n|---|---|---|\n|$f(x)$|1/2|1/2|\n\n$$\\sum_{i=0}^{n} = [x_i-E(X)]^2f(x_i)$$\n\n`2` 연속형일 경우\n\n$$Var(X) = \\int [x-E(x)]^2f(x) \\,\\,dx$$\n\n***\n\n## 정규분포\n\n`1` 분포 정의 $\\to X \\sim N(\\mu, \\sigma^2)$ \n\n$$ f(x) = \\frac{1}{\\sqrt(2\\pi\\sigma^2)}\\exp\\left( -\\frac12 \\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right)$$\n\n`2` 대게 종모양의 분포를 띔\n\n`3` 표준편차가 작을수록 곡선이 좀더 뾰족해지고 폭이 좁아진다? $\\to$ 분산에 정의를 다시 한번 생각해보시면됩니다.\n\n`4` 정규분포의 기각영역\n\n* 우리가 수집한 어떤 집단의 평균이 0일 때의 가설검정\n\n$$H_0 : \\mu = 0, \\quad H_1 : \\mu \\neq 0$$\n\n`5` 중심극한정리\n\n* 표본의 크기가 커지면, 표본평균의 분포는 정규분포로 수렴 $n \\to \\infty,\\, \\overline X_n \\sim N(\\mu, \\sigma^2)$\n\n\n* 이것을 가정하고 있기 때문에, 우리는 어떠한 가설설정 시 정규분포를 이용해서 통계적 유의성을 말할 수 있는거임\n\n> example1, 동전던지기를 통한 중심극한정리 실험\n\n* step1. 동전던지기 실험, 동전던지기 100회 $\\text{sample size} = 100$\n\n* step2. 표본평균 구하기\n\n* step3. 표본평균의 분포 구하기\n\n***\n\n## ML basic\n\n### 1. 선형회귀 평가지표\n\n`-` 결정계수 $R^2$ : 우리가 적합시킨 모델이 얼마만큼의 설명력을 가지고 있는가\n\n* 지난시간 학습한 자료 꼭 한번 확인!\n\n$$R^2 = \\frac{SSR}{SST} = \\frac{1-SSE}{SST}$$\n\n### 2. Overfitting\n\n* 우리가 학습시킨 모델이, 훈련데이터에만 집중적으로 학습이되어서, 실제 검증용 데이터로 평가를 했을 때 모델 성능이 낮아지는 경우\n\n* 이거를 표현해보면...\n\n$$\\begin {align} \\text {MSE} = E[(\\hat y - y)^2] &= Var(\\hat y) + \\text {bias}^2   \\\\ \n                                                                &= E[(\\hat y - E(\\hat y ))^2] + E[(E(\\hat y) - y)^2] + Var(\\varepsilon) \\end {align}$$\n\n`-` $Var(\\hat y) \\, \\to \\,$  훈련자료의 변화에 $\\hat y$가 얼마나 민감하게 반응하는가?(즉, 설명력을 의미한다.)\n\n`-` $\\text {bias}^2$  :  실제자료를 모형으로 얼마나 가깝게 근사할 수 있는가? (만약, $E(\\hat y) = y$ 일 경우 해당 추정량은 불편추정량이다.)\n\n* $Var(\\hat y),\\,\\text {bias}^2$ 는 reducible error로 어떤 모델을 선택하느냐에 따라 줄일 수 있는 오차이다.\n\n`-` $Var(\\varepsilon)$ : 어떤 모델을 사용하건 줄일 수 없는 오차\n\n`-` Flexibility :  모델 복잡도 측도\n\n* Flexibility가 높아지면 $Var(\\hat y)$ 는 높아지고, $\\text {bias}$는 낮아진다. (trade-off)\n \n* 이를 바꿔말하면 더 유연한(복잡한) 모형은 더 큰 분산을 가지고 더 단순한 모형일수록 큰 편의를 가진다.\n\n* 훈련 데이터로 모델 적합시, `bias`를 낮추기 위해 `Var`를 높여 모델 복잡도를 지나치게 높이는 것은 좋은 선택이 아니다.\n\n* 아래의 예시는 모형의 복잡도가 너무 높으면 **평가 데이터에 대한 MSE**가 오히려 과대추정되는 **\"과적합 문제\"** 를 보여주고 있다.\n\n<center><img src = \"2-1.png\" width = 600></center>\n\n#### ex1\n\n<center><img src = \"2-3.png\" width = 600></center>\n\n`-` 실제로 위의 원데이터는 일반적인 선형모형으로 적합해도 괜찮을 것 같다.\n\n`-` 왼쪽그림에서 지나치게 복잡하게 적합된 초록색 선을 살펴보자.\n\n* **훈련 MSE(회색선)** 는 3개의 모델 중 가장 낮으나, 과적합 이슈로 **평가 MSE(빨간색선)** 가 과대추정된 것을 볼 수 있다.\n\n#### ex2\n\n<center><img src = \"2-2.png\" width = 600></center>\n\n`-` 위 데이터는 `ex1`과 달리 일반적인 선형회귀모형으로 적합하면 안될것 같은 느낌이 든다.\n\n`-` 파란색선은 중간 정도로 적합된 복잡한 모형이고, 초록색선은 지나치게 적합된 모형이다.\n\n* 오른쪽 그림을 살펴보면 파란색선으로 적합된 모형의 `Test MSE`가 가장 낮음을 볼 수 있다.\n\n#### 과적합 방지 1. 변수선택법\n\n`-` 변수선택법 : AIC, BIC, Mallows' C_p 등등\n\n#### 과적합 방지 2. Regularized Linear Regression\n\n`-` 정규화 선형회귀는 선형회귀 계수에 대한 제약 조건을 추가하여 모델이 과도하게 최적화되는 현상을 막는 방법!\n\n`1` Ridge\n\n* 기존 OLS를 이용한 $\\beta_i$ 추정할 떄 사용하는 정규방정식\n\n$$RSS = \\sum_{i=1}^n(y_i - \\beta_0 -\\sum_{j=1}^p\\beta_jx_{ij})^2$$\n\n*  Ridge 추정량은 위와 매우 유사하나 `shrinkage penalty`를 추가로 고려하여 최소화한다.\n\n$$\\sum_{i=1}^n(y_i - \\beta_0 -\\sum_{j=1}^p\\beta_jx_{ij})^2 + \\lambda \\sum_{i=1}^p \\beta_j^2$$\n\n$$\\lambda \\,:\\,tuning \\,\\,parameter$$\n\n* $\\lambda$가 0이되면 일반적인 선형회귀모형이되고, $\\lambda$가 커지면 **정규화 정도가 커진다.(회귀계수들이 작아진다.)** $\\to$ `bias`는 증가 `variance`는 감소\n\n* 릿지회귀모형에서는 **L2 penalty** 를 사용하여 베타계수들이 0에 근사하도록 한다.\n\n`2` Lasso\n\n* 라쏘는 릿지와 다르게 베타계수를 **\"0값으로 보낸다.\"**\n\n* 이것을 **\"L1 penalty\"** 라고 부른다.m\n\n$$\\sum_{i=1}^n(y_i - \\beta_0 -\\sum_{j=1}^p\\beta_jx_{ij})^2 + \\lambda \\sum_{i=1}^p |\\beta_j|$$\n\n#### 실습\n\n`1` 데이터확인\n\n`-` model.matrix는 기본적으로 절편을 포함한 모형을 산정한다. 절편을 제외시킨 모형을 고려해보자\n\n`-` 모형적합\n\n`-` 각 변수들의 beta 계수구하기\n\n`-` `MSE`가 최소가 되는 지점을 확인\n\n`-` 산출된 `best.lam`값으로 모형을 다시 적합후 `test_MSE`를 구해보자\n\n`-` 여기선 MSE가 거의 1만에 가깝게 나왔는데 이는 지나치게 모형을 단순화한 것임 즉, best_lam값을 다시 찾는 과정을 반복해야한다....\n\n****\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"2024-04-03-stat & ml basic.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","theme":"minty","editor":"visual","code-copy":true,"title-block-banner":true,"title":"02. stat & ML basic","author":"GC","date":"04/23/24"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}