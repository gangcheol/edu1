{"title":"03. MLE","markdown":{"yaml":{"title":"03. MLE","author":"gc","date":"04/10/24"},"headingText":"MLE","containsRefs":false,"markdown":"\n\n\n## 우도함수\n\n$$L(\\theta)=f(\\mathbf{x}  | \\theta) =  f(x_1|\\theta)f(x_2|\\theta)\\dots f(x_n|\\theta)$$\n\n`-` 해석 1(일반적인 정의 : 블로그, 위키피디아, 기타 포럼 등등)\n\n* Likelihood function\n\n* 우도함수는 일반적으로 설명력 함수라고 알려져 있다.\n\n* 수식을 보았을 때, 주어진 랜덤샘플 $x_1,x_2 \\dots x_n$에 대한 결합확률밀도함수(그냥 다 곱한거)를 나타내고 있다.\n\n* **즉, 이는 모수($p, \\theta, \\mu, \\sigma...$)가 주어졌을 때 주어진 데이터(샘플)가 어떤 분포(or 가정, 현상)를 얼만큼 설명하는지 직관적인 형태를 수식으로 나타냈다고 이해하자.** ($\\star\\star\\star$)\n\n`-` 해석 2 (조금 더.. 전문적인 지식이 들어간 해석)\n\n* 주어진 모수 ($p, \\theta, \\mu, \\sigma...$)가 주어졌을 때 $\\bf {x}$를 관측할 `확률`(or 확률밀도)를 나타낸다.($\\star\\star\\star$)\n\n* 즉, 특정 모수가 주어졌을 때 어떤 `샘플들이 얻어질 확률`이지 **모수(확률, 평균)가 어떤 상수 값 (`ex` $p = 0.2$)과 일치할 확률을 의미하지 않는다.**($\\star\\star\\star$)\n\n`-` 해석 2에 대한 좀더 직관적인 접근으로 아래와 같은 실험에서 우도함수를 최대화하는 $\\hat{p}_{mle}$ 구해보자.\n\n> 동전 던지기 실험 $\\to X_i \\overset{iid}\\sim Ber(p), \\quad f(x) = p^{x}(1-p)^{1-x}, \\quad  x \\in [0,1]$\n\n`-` 일반적으로 우리가 알고 있는 사실 $\\to p = 1/2, \\,\\, f(x) = 1/2$ \n\n`-` 우도함수 계산\n\n`-` 만약 $p = 1/3$이라면? $\\to f(x) = (1/3)^{x}(2/3)^{1-x}$ \n\n`-` 우도함수 계산\n\n`-` 음? $p=1/2$일 때가, 더 우도함수 값이 크다.\n\n* 해석 1의 관점 : 오, $p=1/2$일 때, 설명력이 더 좋네?\n\n* 해석 2의 관점 : 오, $p=1/2$일 때, 샘플 `(0,1,0,1)`이 얻어질 확률이 더 크네?\n\n* 우리는 앞으로 해석 1의 관점을 일차적으로 말하고, 그 다음에 디테일하게 해석 2를 말할 줄 알아야 한다!\n\n\n## 용어정리\n\n`1` 가능도함수 = 우도함수 = likelihood function\n\n`2` 최대가능도함수 = 최대우도함수 = Maximum likelihood function = 모수 $p$의 MLE\n\n## MLE를 구하는 방법\n\n(조건은 생략할게여...)\n\n`step 1`. 우도함수 구하기\n\n$$L(\\theta)=f(\\mathbf{x}  | \\theta) =  f(x_1|\\theta)f(x_2|\\theta)\\dots f(x_n|\\theta)$$\n\n`step 2`. $\\log L(\\theta) = l(\\theta)$ 미분\n\n$$\\hat {\\theta}_{mle} = \\frac {d}{d\\,\\theta} \\log L(\\theta) = \\frac {d}{d\\,\\theta} l(\\theta)$$\n\nwhy? 미분하여 최대값?\n\n`-` 중학교 수학 : 로그함수는 일반적으로 증가함수이며, 증가함수는 일반적으로 미분하여 0이되는 지점에서 최대값을 갖는다.\n\n`-` 다시 예제\n\n> 동전 던저기 실험 $\\to X_i \\overset{iid}\\sim Ber(p), \\quad f(x) = p^{x}(1-p)^{1-x}, \\quad  x \\in [0,1]$\n\n* 일반적으로 알려진 사실 $p = 1/2$\n\n* $L(p) = p^{\\sum x}(1-p)^{n-\\sum x}$\n\n*  $l(p) = \\sum x \\log p + (n-\\sum x)\\log (1-p)$\n\n$$\\begin{align*} \\frac{d}{d\\,p} l(p) &= \\frac{\\sum x}{p} - \\frac{n-\\sum x}{1-p} \\\\ \\\\\n                    &= (1-p)\\sum x - p(n-\\sum x)  \\\\ \\\\ \n                    &= \\sum x  - pn = 0\\end{align*}$$\n\n$$\\divideontimes \\,\\,\\hat {p}_{mle} = \\frac {\\sum x}{ n} $$\n\n* 확인\n\n## 회귀모형에서의 MLE\n\n### 단순선형회귀분석\n\n$$y = \\beta_0 + \\beta_1x + \\varepsilon,\\quad \\varepsilon \\overset{iid}\\sim N(0,\\sigma^2)$$\n\n`1` 복습. OLS(정규방정식)\n\n* 아래의 식을 만족하는 $\\beta_0, \\beta_1$은 OLS, 즉 최소제곱법을 이용하여 구한 최적의 해다.\n\n$$\\sum ({y_i - \\beta_0 -\\beta_1x})^2 = 0$$\n\n`2` MLE\n\n* 오차항은 정규성을 가정하였으므로. 다음과 같이 쓸 수 있다.\n\n$$f(\\varepsilon) = \\frac {1}{\\sqrt {2\\pi}}e^{-\\frac{1}{2}\\varepsilon^2}$$\n\n* 가능도 함수 해석\n\n$\\to$ `1. 설명력 함수`를 생각하면 오차항에 대한 우도함수를 구하고, 그것을 미분하여 우도함수를 최대화 시키는 $\\beta_0, \\beta_1$을 구하면 되지 않을까?\n\n$\\to$ 우도함수 구하기\n\n$$l(\\beta_0,\\beta_1) = -\\frac{\\sum \\varepsilon^2}{2\\sqrt{2\\pi}} = -\\frac{\\sum(y -  \\beta_0- \\beta_1x)^2}{2\\sqrt{2\\pi}} $$\n\n$\\to$ 우도함수 미분\n\n$$\\frac {d}{d (\\beta_0, \\beta_1)}l(\\beta_0,\\beta_1)  = -\\frac {d}{d (\\beta_0, \\beta_1)}\\frac{\\sum(y -  \\beta_0- \\beta_1x)^2}{2\\sqrt{2\\pi}} = 0\\text{ 을 만족하는} (\\beta_1, \\beta_0) $$\n\n$\\to$ 차피 상수항은 넘어가니까...\n\n$$\\frac {d}{d (\\beta_0, \\beta_1)}l(\\beta_0,\\beta_1)  = \\frac {d}{d (\\beta_0, \\beta_1)}\\sum(y -  \\beta_0- \\beta_1x)^2 = 0\\text{ 을 만족하는} (\\beta_1, \\beta_0) $$\n\n$\\to$ 어? 근데 이거 OLS랑 똑같네??\n\n* OLS : 아래의 식을 만족하는 $\\beta_0, \\beta_1$은 OLS, 즉 최소제곱법을 이용하여 구한 최적의 해다.\n\n$$\\sum ({y_i - \\beta_0 -\\beta_1x})^2 = 0$$\n\n* 아하! 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 $\\beta_0, \\beta_1$을 구하면 된다!\n\n### 로지스틱\n\n`1` 잠깐 위에 동전던저기 예제\n\n* $L(p) = p^{\\sum x}(1-p)^{n-\\sum x}$\n\n*  $l(p) = \\sum x \\log p + (n-\\sum x)\\log (1-p)$\n\n$$\\begin{align*} \\frac{d}{d\\,p} l(p) &= \\frac{\\sum x}{p} - \\frac{n-\\sum x}{1-p} \\\\ \\\\\n                    &= (1-p)\\sum x - p(n-\\sum x)  \\\\ \\\\ \n                    &= \\sum x  - pn = 0\\end{align*}$$\n\n$$\\divideontimes \\,\\,\\hat {p}_{mle} = \\frac {\\sum x}{ n} $$\n\n`2` 로지스틱 복습\n\n* 지난 시간 로지스틱은 어떤 `범주의 속할 확률`(동전이 앞면이 나올지, 내가 여자인지 남자인지)을 모형화하는 것이라고 했다...\n\n* 확률을 모형화...모형화..어?? 위에 하고 똑같자나...\n\n> 동전 던지기 실험 $\\to Y_i \\overset{iid}\\sim Ber(p), \\quad f(y) = p^{y}(1-p)^{1-y}, \\quad  y \\in [0,1]$\n\n* 단순히 $p$가 $\\beta_0 + \\beta_1x$로 바뀐거죠?\n\n* 다시 쓰면?\n\n> 동전 던지기 실험 $\\to Y_i \\overset{iid}\\sim Ber(p), \\quad f(y) = (\\beta_0+\\beta_1x)^{y}(1-p)^{1-y}, \\quad  y \\in [0,1]$\n\n* 이거에 대한 우도함수는?\n\n$\\to$ $L(\\beta_0, \\beta_1) = (\\beta_0+\\beta_1x)^{\\sum y}[1-(\\beta_0+\\beta_1x)]^{n-\\sum y}$\n\n* 이거를 교재 4장, 17페이지에서 아래와 같이 좀 문과생? 들이 보기 어려운 수식을 써서 표현한거에요...\n\n$$\\prod_{i=1}^{n}\\left (\\beta_0 + \\beta_1x_1^{y_i} \\right )\\left (1-(\\beta_0 + \\beta_1x_1)^{y_i} \\right )$$\n\n$\\to$ 다시, $l(\\beta_0\\,\\beta_1)$ 구하기. 이제 그냥 편의상 $p$라고 쓸게요.\n\n$$\\begin{align*} l(p) &= \\sum y \\log p - (n-\\sum y)\\log (1-p)  \\\\ \\\\ \n                      &= \\sum y \\log p - (1-y)\\log (1-p)  \\\\ \\\\ \n                      &= \\sum y \\log (\\beta_0 + \\beta_1x) - (1-y)\\log (1-(\\beta_0 + \\beta_1x)) \\end{align*}$$\n\n\n$\\to \\sum y \\log (\\beta_0 + \\beta_1x) - (1-y)\\log \\left (1-(\\beta_0 + \\beta_1x)\\right )$ 와 같은 형태를 `BCEloss` 라고합니다.(뒤에서 배울거에요.)\n\n$\\to$ 즉, 분류범주가 2개인 경우 `BCEloss`를 이용하며 모형을 학습해 최적의 모형을 산출하는데, 그게 결국 `MLE`를 통해 최적의 모수를 구하는 것과 같다!\n\n***\n\n## summary\n\n`1` 우도함수는 설명력 함수이다.\n\n`2` 우도함수는 주어진 모수 ($p, \\theta, \\mu, \\sigma...$)가 주어졌을 때 $\\bf {x}$를 관측할 `확률`(or 확률밀도)를 나타낸다.\n\n`3` 어떤 분포, 모형에 관한 최적의 MLE를 구하기 위해서는 우도함수의 로그를 취하고 그것을 미분하여 0을 만족하는 경우를 `MLE`라고 한다.\n\n`4` 회귀모형에서 최적의 회귀계수($\\beta_0,\\beta_1$)을 구하는 방법은 `MLE`를 이용하여 구하는 방법과 동일하다.\n\n## to do\n\n`1` 예측 성능 지표\n\n`2` 고급회귀분석\n\n`3` 모수 모델 vs 비모수 모델\n\n`4` cost function(손실함수, 비용함수)\n","srcMarkdownNoYaml":"\n\n# MLE\n\n## 우도함수\n\n$$L(\\theta)=f(\\mathbf{x}  | \\theta) =  f(x_1|\\theta)f(x_2|\\theta)\\dots f(x_n|\\theta)$$\n\n`-` 해석 1(일반적인 정의 : 블로그, 위키피디아, 기타 포럼 등등)\n\n* Likelihood function\n\n* 우도함수는 일반적으로 설명력 함수라고 알려져 있다.\n\n* 수식을 보았을 때, 주어진 랜덤샘플 $x_1,x_2 \\dots x_n$에 대한 결합확률밀도함수(그냥 다 곱한거)를 나타내고 있다.\n\n* **즉, 이는 모수($p, \\theta, \\mu, \\sigma...$)가 주어졌을 때 주어진 데이터(샘플)가 어떤 분포(or 가정, 현상)를 얼만큼 설명하는지 직관적인 형태를 수식으로 나타냈다고 이해하자.** ($\\star\\star\\star$)\n\n`-` 해석 2 (조금 더.. 전문적인 지식이 들어간 해석)\n\n* 주어진 모수 ($p, \\theta, \\mu, \\sigma...$)가 주어졌을 때 $\\bf {x}$를 관측할 `확률`(or 확률밀도)를 나타낸다.($\\star\\star\\star$)\n\n* 즉, 특정 모수가 주어졌을 때 어떤 `샘플들이 얻어질 확률`이지 **모수(확률, 평균)가 어떤 상수 값 (`ex` $p = 0.2$)과 일치할 확률을 의미하지 않는다.**($\\star\\star\\star$)\n\n`-` 해석 2에 대한 좀더 직관적인 접근으로 아래와 같은 실험에서 우도함수를 최대화하는 $\\hat{p}_{mle}$ 구해보자.\n\n> 동전 던지기 실험 $\\to X_i \\overset{iid}\\sim Ber(p), \\quad f(x) = p^{x}(1-p)^{1-x}, \\quad  x \\in [0,1]$\n\n`-` 일반적으로 우리가 알고 있는 사실 $\\to p = 1/2, \\,\\, f(x) = 1/2$ \n\n`-` 우도함수 계산\n\n`-` 만약 $p = 1/3$이라면? $\\to f(x) = (1/3)^{x}(2/3)^{1-x}$ \n\n`-` 우도함수 계산\n\n`-` 음? $p=1/2$일 때가, 더 우도함수 값이 크다.\n\n* 해석 1의 관점 : 오, $p=1/2$일 때, 설명력이 더 좋네?\n\n* 해석 2의 관점 : 오, $p=1/2$일 때, 샘플 `(0,1,0,1)`이 얻어질 확률이 더 크네?\n\n* 우리는 앞으로 해석 1의 관점을 일차적으로 말하고, 그 다음에 디테일하게 해석 2를 말할 줄 알아야 한다!\n\n\n## 용어정리\n\n`1` 가능도함수 = 우도함수 = likelihood function\n\n`2` 최대가능도함수 = 최대우도함수 = Maximum likelihood function = 모수 $p$의 MLE\n\n## MLE를 구하는 방법\n\n(조건은 생략할게여...)\n\n`step 1`. 우도함수 구하기\n\n$$L(\\theta)=f(\\mathbf{x}  | \\theta) =  f(x_1|\\theta)f(x_2|\\theta)\\dots f(x_n|\\theta)$$\n\n`step 2`. $\\log L(\\theta) = l(\\theta)$ 미분\n\n$$\\hat {\\theta}_{mle} = \\frac {d}{d\\,\\theta} \\log L(\\theta) = \\frac {d}{d\\,\\theta} l(\\theta)$$\n\nwhy? 미분하여 최대값?\n\n`-` 중학교 수학 : 로그함수는 일반적으로 증가함수이며, 증가함수는 일반적으로 미분하여 0이되는 지점에서 최대값을 갖는다.\n\n`-` 다시 예제\n\n> 동전 던저기 실험 $\\to X_i \\overset{iid}\\sim Ber(p), \\quad f(x) = p^{x}(1-p)^{1-x}, \\quad  x \\in [0,1]$\n\n* 일반적으로 알려진 사실 $p = 1/2$\n\n* $L(p) = p^{\\sum x}(1-p)^{n-\\sum x}$\n\n*  $l(p) = \\sum x \\log p + (n-\\sum x)\\log (1-p)$\n\n$$\\begin{align*} \\frac{d}{d\\,p} l(p) &= \\frac{\\sum x}{p} - \\frac{n-\\sum x}{1-p} \\\\ \\\\\n                    &= (1-p)\\sum x - p(n-\\sum x)  \\\\ \\\\ \n                    &= \\sum x  - pn = 0\\end{align*}$$\n\n$$\\divideontimes \\,\\,\\hat {p}_{mle} = \\frac {\\sum x}{ n} $$\n\n* 확인\n\n## 회귀모형에서의 MLE\n\n### 단순선형회귀분석\n\n$$y = \\beta_0 + \\beta_1x + \\varepsilon,\\quad \\varepsilon \\overset{iid}\\sim N(0,\\sigma^2)$$\n\n`1` 복습. OLS(정규방정식)\n\n* 아래의 식을 만족하는 $\\beta_0, \\beta_1$은 OLS, 즉 최소제곱법을 이용하여 구한 최적의 해다.\n\n$$\\sum ({y_i - \\beta_0 -\\beta_1x})^2 = 0$$\n\n`2` MLE\n\n* 오차항은 정규성을 가정하였으므로. 다음과 같이 쓸 수 있다.\n\n$$f(\\varepsilon) = \\frac {1}{\\sqrt {2\\pi}}e^{-\\frac{1}{2}\\varepsilon^2}$$\n\n* 가능도 함수 해석\n\n$\\to$ `1. 설명력 함수`를 생각하면 오차항에 대한 우도함수를 구하고, 그것을 미분하여 우도함수를 최대화 시키는 $\\beta_0, \\beta_1$을 구하면 되지 않을까?\n\n$\\to$ 우도함수 구하기\n\n$$l(\\beta_0,\\beta_1) = -\\frac{\\sum \\varepsilon^2}{2\\sqrt{2\\pi}} = -\\frac{\\sum(y -  \\beta_0- \\beta_1x)^2}{2\\sqrt{2\\pi}} $$\n\n$\\to$ 우도함수 미분\n\n$$\\frac {d}{d (\\beta_0, \\beta_1)}l(\\beta_0,\\beta_1)  = -\\frac {d}{d (\\beta_0, \\beta_1)}\\frac{\\sum(y -  \\beta_0- \\beta_1x)^2}{2\\sqrt{2\\pi}} = 0\\text{ 을 만족하는} (\\beta_1, \\beta_0) $$\n\n$\\to$ 차피 상수항은 넘어가니까...\n\n$$\\frac {d}{d (\\beta_0, \\beta_1)}l(\\beta_0,\\beta_1)  = \\frac {d}{d (\\beta_0, \\beta_1)}\\sum(y -  \\beta_0- \\beta_1x)^2 = 0\\text{ 을 만족하는} (\\beta_1, \\beta_0) $$\n\n$\\to$ 어? 근데 이거 OLS랑 똑같네??\n\n* OLS : 아래의 식을 만족하는 $\\beta_0, \\beta_1$은 OLS, 즉 최소제곱법을 이용하여 구한 최적의 해다.\n\n$$\\sum ({y_i - \\beta_0 -\\beta_1x})^2 = 0$$\n\n* 아하! 결국 오차항이 정규분포를 따르는 회귀모형의 MLE는 MSE를 최소화하는 $\\beta_0, \\beta_1$을 구하면 된다!\n\n### 로지스틱\n\n`1` 잠깐 위에 동전던저기 예제\n\n* $L(p) = p^{\\sum x}(1-p)^{n-\\sum x}$\n\n*  $l(p) = \\sum x \\log p + (n-\\sum x)\\log (1-p)$\n\n$$\\begin{align*} \\frac{d}{d\\,p} l(p) &= \\frac{\\sum x}{p} - \\frac{n-\\sum x}{1-p} \\\\ \\\\\n                    &= (1-p)\\sum x - p(n-\\sum x)  \\\\ \\\\ \n                    &= \\sum x  - pn = 0\\end{align*}$$\n\n$$\\divideontimes \\,\\,\\hat {p}_{mle} = \\frac {\\sum x}{ n} $$\n\n`2` 로지스틱 복습\n\n* 지난 시간 로지스틱은 어떤 `범주의 속할 확률`(동전이 앞면이 나올지, 내가 여자인지 남자인지)을 모형화하는 것이라고 했다...\n\n* 확률을 모형화...모형화..어?? 위에 하고 똑같자나...\n\n> 동전 던지기 실험 $\\to Y_i \\overset{iid}\\sim Ber(p), \\quad f(y) = p^{y}(1-p)^{1-y}, \\quad  y \\in [0,1]$\n\n* 단순히 $p$가 $\\beta_0 + \\beta_1x$로 바뀐거죠?\n\n* 다시 쓰면?\n\n> 동전 던지기 실험 $\\to Y_i \\overset{iid}\\sim Ber(p), \\quad f(y) = (\\beta_0+\\beta_1x)^{y}(1-p)^{1-y}, \\quad  y \\in [0,1]$\n\n* 이거에 대한 우도함수는?\n\n$\\to$ $L(\\beta_0, \\beta_1) = (\\beta_0+\\beta_1x)^{\\sum y}[1-(\\beta_0+\\beta_1x)]^{n-\\sum y}$\n\n* 이거를 교재 4장, 17페이지에서 아래와 같이 좀 문과생? 들이 보기 어려운 수식을 써서 표현한거에요...\n\n$$\\prod_{i=1}^{n}\\left (\\beta_0 + \\beta_1x_1^{y_i} \\right )\\left (1-(\\beta_0 + \\beta_1x_1)^{y_i} \\right )$$\n\n$\\to$ 다시, $l(\\beta_0\\,\\beta_1)$ 구하기. 이제 그냥 편의상 $p$라고 쓸게요.\n\n$$\\begin{align*} l(p) &= \\sum y \\log p - (n-\\sum y)\\log (1-p)  \\\\ \\\\ \n                      &= \\sum y \\log p - (1-y)\\log (1-p)  \\\\ \\\\ \n                      &= \\sum y \\log (\\beta_0 + \\beta_1x) - (1-y)\\log (1-(\\beta_0 + \\beta_1x)) \\end{align*}$$\n\n\n$\\to \\sum y \\log (\\beta_0 + \\beta_1x) - (1-y)\\log \\left (1-(\\beta_0 + \\beta_1x)\\right )$ 와 같은 형태를 `BCEloss` 라고합니다.(뒤에서 배울거에요.)\n\n$\\to$ 즉, 분류범주가 2개인 경우 `BCEloss`를 이용하며 모형을 학습해 최적의 모형을 산출하는데, 그게 결국 `MLE`를 통해 최적의 모수를 구하는 것과 같다!\n\n***\n\n## summary\n\n`1` 우도함수는 설명력 함수이다.\n\n`2` 우도함수는 주어진 모수 ($p, \\theta, \\mu, \\sigma...$)가 주어졌을 때 $\\bf {x}$를 관측할 `확률`(or 확률밀도)를 나타낸다.\n\n`3` 어떤 분포, 모형에 관한 최적의 MLE를 구하기 위해서는 우도함수의 로그를 취하고 그것을 미분하여 0을 만족하는 경우를 `MLE`라고 한다.\n\n`4` 회귀모형에서 최적의 회귀계수($\\beta_0,\\beta_1$)을 구하는 방법은 `MLE`를 이용하여 구하는 방법과 동일하다.\n\n## to do\n\n`1` 예측 성능 지표\n\n`2` 고급회귀분석\n\n`3` 모수 모델 vs 비모수 모델\n\n`4` cost function(손실함수, 비용함수)\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"2024-04-10-03. MLE.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","theme":"minty","editor":"visual","code-copy":true,"title-block-banner":true,"title":"03. MLE","author":"gc","date":"04/10/24"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}